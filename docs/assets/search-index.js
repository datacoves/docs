{"documentCount":70,"nextId":70,"documentIds":{"0":"docs_output/client-docs/ccs/cluster-requirements-azure.html","1":"docs_output/client-docs/index.html","2":"docs_output/client-docs/jnj/1-cluster-requirements.html","3":"docs_output/client-docs/jnj/2-configuration.html","4":"docs_output/client-docs/jnj/3-configure-bastion-ec2-instance.html","5":"docs_output/client-docs/jnj/4-configure-bitbucket-and-jenkins.html","6":"docs_output/client-docs/jnj/5-deployment.html","7":"docs_output/client-docs/jnj/6-access.html","8":"docs_output/client-docs/jnj/7-configure-sa-docker-in-kubernates.html","9":"docs_output/client-docs/jnj/8-summary-requirements-new-cluster.html","10":"docs_output/client-docs/jnj/index.html","11":"docs_output/client-docs/kenvue/how-to-setup-helm-chart.html","12":"docs_output/client-docs/orrum/index.html","13":"docs_output/dev-logs/2021-06-create-er-diagram.html","14":"docs_output/dev-logs/2021-09-eks-setup.html","15":"docs_output/dev-logs/2021-12-jnj-ensembledev-deployment.html","16":"docs_output/dev-logs/2022-04-jnj-artemisdev-configuration.html","17":"docs_output/dev-logs/2022-04-jnj-ensembletest-deployment.html","18":"docs_output/dev-logs/2022-05-setup-aks-postgres-flexible-server.html","19":"docs_output/dev-logs/index.html","20":"docs_output/how-tos/administrate-east-us-a-aks-cluster.html","21":"docs_output/how-tos/airflow-configuration.html","22":"docs_output/how-tos/billing-system.html","23":"docs_output/how-tos/celery-monitoring.html","24":"docs_output/how-tos/choose-ec2-nodes.html","25":"docs_output/how-tos/codeserver-images.html","26":"docs_output/how-tos/connect-to-kenvue-cluster-using-a-bastion.html","27":"docs_output/how-tos/custom-dns.html","28":"docs_output/how-tos/datacoves-versioning.html","29":"docs_output/how-tos/debug-airflow-workers.html","30":"docs_output/how-tos/debug-dbt-errors-when-return-code-is-not-zero.html","31":"docs_output/how-tos/grafana-grant-permisions.html","32":"docs_output/how-tos/grafana-loki-storage-config-providers.html","33":"docs_output/how-tos/grafana-loki-storage-config.html","34":"docs_output/how-tos/hotfix.html","35":"docs_output/how-tos/how-to-create-a-ssl-certificate.html","36":"docs_output/how-tos/index.html","37":"docs_output/how-tos/install-python-reqs-on-jnj-bastion.html","38":"docs_output/how-tos/list-code-server-pods-processes.html","39":"docs_output/how-tos/make-and-install-a-release.html","40":"docs_output/how-tos/manage-profiles-and-image-sets.html","41":"docs_output/how-tos/move-a-gpg-secret-key.html","42":"docs_output/how-tos/onboard-a-new-project-on-datacoves.html","43":"docs_output/how-tos/prometheus-queries.html","44":"docs_output/how-tos/q-and-a.html","45":"docs_output/how-tos/recover-disk-on-aks.html","46":"docs_output/how-tos/register-github-self-hosted-runner.html","47":"docs_output/how-tos/release-notes.html","48":"docs_output/how-tos/request-access-to-a-cloud-pc-on-kenvue.html","49":"docs_output/how-tos/reset-datahub.html","50":"docs_output/how-tos/security-vulnerabilities-fix.html","51":"docs_output/how-tos/set-maintenance-mode.html","52":"docs_output/how-tos/setup-oauth-on-azure.html","53":"docs_output/how-tos/setup-s3-for-dbt-api.html","54":"docs_output/how-tos/testing-alerts.html","55":"docs_output/how-tos/trigger-cloudx-pipeline-on-kenvue-cluster.html","56":"docs_output/how-tos/update-kubernetes-and-datacoves.html","57":"docs_output/how-tos/update-ssl-certificates.html","58":"docs_output/how-tos/upgrade-dbt-or-related-tools.html","59":"docs_output/how-tos/work-on-a-pre-release-locally.html","60":"docs_output/implementation/index.html","61":"docs_output/implementation/operator.html","62":"docs_output/index.html","63":"docs_output/issues-resolutions/airflow-corrupted-dag-logs.html","64":"docs_output/issues-resolutions/dbt-core-debugging.html","65":"docs_output/issues-resolutions/docker-image-debugging.html","66":"docs_output/issues-resolutions/docker-push-stopped-working.html","67":"docs_output/issues-resolutions/helm-chart.html","68":"docs_output/issues-resolutions/pomerium-not-allowing-access.html","69":"index.template.html"},"fieldIds":{"text":0},"fieldLength":{"0":[153],"1":[90],"2":[608],"3":[254],"4":[229],"5":[141],"6":[131],"7":[178],"8":[126],"9":[138],"10":[59],"11":[77],"12":[172],"13":[48],"14":[143],"15":[279],"16":[82],"17":[120],"18":[119],"19":[3],"20":[105],"21":[55],"22":[380],"23":[143],"24":[103],"25":[175],"26":[129],"27":[95],"28":[142],"29":[127],"30":[83],"31":[25],"32":[137],"33":[83],"34":[201],"35":[72],"36":[3],"37":[39],"38":[41],"39":[141],"40":[169],"41":[74],"42":[118],"43":[80],"44":[98],"45":[192],"46":[162],"47":[164],"48":[18],"49":[134],"50":[33],"51":[34],"52":[144],"53":[84],"54":[95],"55":[40],"56":[700],"57":[558],"58":[55],"59":[95],"60":[3],"61":[416],"62":[127],"63":[170],"64":[83],"65":[71],"66":[58],"67":[96],"68":[155],"69":[4]},"averageFieldLength":[137.98571428571435],"storedFields":{"0":{"url":"/docs_output/client-docs/ccs/cluster-requirements-azure.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      Summary for the requirements of a new Cluster.\n     \n     \n     \n     \n      Database (Azure Database for PostgreSQL - Flexible Server)\n     \n     \n     \n     \n      Minimum requirements\n     \n     \n      \n       Version: 14 or later\n      \n      \n       Workload Type: Production\n      \n      \n       Compute+Storage: General Purpose, D4ds_v5\n      \n      \n       Geo-Redundancy and High Availability optional but recommended.\n      \n      \n       Admin user/password required and must be provided to Datacoves.\n      \n      \n       Storage Type: Premium SSD\n      \n      \n       Storage Size: 128 GiB\n      \n      \n       Performance Tier: P10\n      \n      \n       Storage auto growth enabled optional but recommended.\n      \n     \n     \n     \n     \n      Kubernetes Services\n     \n     \n     \n     \n      Configuration\n     \n     \n      \n       Kubernetes version: 1.30.6 or later\n      \n     \n     \n     \n     \n      Node pools\n     \n     \n      \n       general\n      \n      \n       volumed\n      \n      \n       workers - Standard_D4s_v3 node, 128 gig OS disk size\n      \n     \n     \n     \n     \n      Worker groups\n     \n     \n      \n       General\n      \n      \n       Volumed\n      \n      \n       Workers\n      \n     \n     \n     \n     \n      General\n     \n     \n      \n       Standard_D4s_v3\n      \n      \n       min_nodes: 1\n      \n      \n       max_nodes: 4\n      \n      \n       root_volume_size: 128\n      \n      \n       labels:\n      \n     \n     labels:\n    ...\n    - key: k8s.datacoves.com/nodegroup-kind\n    value: general\n\n     \n     \n     \n      Volumed\n     \n     \n      \n       Standard_D16s_v5\n      \n      \n       min_nodes: 1\n      \n      \n       max_nodes: 4\n      \n      \n       root_volume_size: 512\n      \n      \n       labels:\n      \n     \n     labels:\n    ...\n    - key: k8s.datacoves.com/nodegroup-kind\n    value: volumed\n\n     \n     \n     \n      Workers\n     \n     \n      \n       min_nodes: 1\n      \n      \n       max_nodes: 4\n      \n      \n       root_volume_size: 128\n      \n      \n       labels:\n      \n     \n     labels:\n    ...\n    - key: k8s.datacoves.com/workers\n    value: enabled\n\n     \n     \n     \n      Other configuration.\n     \n     \n     \n     \n      SSL Certificate\n     \n     \n      We recommend using a wildcard certificate, however we can also use cert manager for free certificates if that is the preference.\n     \n     \n      Certificates must be issued for:\n     \n     \n      \n       \n        *.domain.com\n       \n      \n      \n       \n        domain.com\n       \n      \n     \n     \n      Where 'domain.com' is whatever base domain you wish to use.  We recommend using \"datacoves.YOUR_DOMAIN.YOUR_TLD\", such as 'datacoves.mycompany.com'.  In such a case, you would need certificates for:\n     \n     \n      \n       \n        *.datacoves.mycompany.com\n       \n      \n      \n       \n        datacoves.mycompany.com\n       \n      \n     \n     \n     \n     \n      DNS Configuration\n     \n     \n      Either DNS must be configured to support the same wildcard and base domain, or the cluster must be allowed to create DNS entries via kubernetes' external-dns annotation."},"1":{"url":"/docs_output/client-docs/","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      Customer clusters\n     \n     \n     \n     \n      jnj\n     \n     \n      \n       Artemis\n      \n      \n       Artemis dev\n      \n      \n       Ensemble\n      \n      \n       Ensemble test\n      \n      \n       RND\n      \n     \n     \n     \n     \n      Requirements:\n     \n     \n      \n       Access to jnj workspace (worspace is provided by jnj and is personal)\n       \n        \n         Request access: Onboarding (Noel)\n        \n        \n         Check access: https://jnjitod.service-now.com/myworkspaces\n        \n       \n      \n      \n       Repo with all configurations https://sourcecode.jnj.com/projects/asx-ahrx/repos/datacoves_deployment/browse  (READ.me with all configurations). There will be a specific repo for each clusters (onboarding Noel).\n       \n        \n         Request access: https://confluence.jnj.com/pages/viewpage.action?spaceKey=AHRX&amp;title=How+to+request+access+to+Bitbucket+-+How+to+request+access+-+How+to+guides\n        \n       \n      \n      \n       Access to Bastion\n      \n     \n     \n     \n     \n      kenvue\n     \n     \n      \n       Chap dev\n      \n      \n       Chap production\n      \n     \n     \n     \n     \n      Requirements:\n     \n     \n      \n       Access to kenvue microsoft remote desktop (provided by jnj and is personal)\n       \n        \n         Request access: Onboarding (Noel)\n        \n        \n         Check access: https://kenvue.sharepoint.com/\n        \n       \n      \n      \n       Repo is the same as jnj\n      \n      \n       Access to Bastion\n      \n     \n     \n     \n     \n      orrum\n     \n     \n      \n       old\n      \n      \n       new\n      \n     \n     \n     \n     \n      Requirements:\n     \n     \n      \n       Download VPN fron Azure (see client-docs instructions)\n      \n      \n       Credentials in 1 password"},"2":{"url":"/docs_output/client-docs/jnj/1-cluster-requirements.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      Datacoves cluster requirements\n     \n     \n      \n       Summary for the requirements of a new Cluster.\n      \n     \n     \n     \n     \n      EKS cluster\n     \n     \n      The clusters are created through CloudX pipelines, from\n      \n       cluster.yaml\n      \n      files (\n      \n       docs\n      \n      ).\nFor every cluster there's a git repository with the cluster definition. If your\nteam create one of this repositories, please either grant access to datacoves staff so\nwe can make changes if required or ask us to check your\n      \n       cluster.yaml\n      \n      .\n     \n     \n      An example repository of this kind is\n      \n       itx-ank/ensemble\n      \n      .\n     \n     \n      Important configuration to take into consideration:\n     \n     \n      \n       Kubernetes version: latest confirmed working version.  This is either -1 or -2 releases from current based on the time of year.\n      \n      \n       Addons versions\n      \n      \n       Worker groups: general, volumed, and workers.\n      \n     \n     \n     \n     \n      Cluster configuration files\n     \n     \n      \n       \n        \n         Cluster\n        \n        \n         Repository\n        \n        \n         Branch\n        \n       \n      \n      \n       \n        \n         Ensemble test\n        \n        \n         https://sourcecode.jnj.com/projects/ITX-ANK/repos/ensemble/browse/_scm_cluster\n        \n        \n         test\n        \n       \n       \n        \n         Ensemble\n        \n        \n         https://sourcecode.jnj.com/projects/ITX-CCC/repos/ensemble/browse/_scm_cluster\n        \n        \n         production\n        \n       \n       \n        \n         R&amp;D\n        \n        \n         https://sourcecode.jnj.com/projects/ITX-BHE/repos/integrationscluster/browse/_scm_cluster\n        \n        \n         test\n        \n       \n       \n        \n         Artemis Dev\n        \n        \n         https://sourcecode.jnj.com/projects/ITX-ADW/repos/artemiseks/browse/_scm_cluster\n        \n        \n         development\n        \n       \n       \n        \n         Artemis\n        \n        \n         https://sourcecode.jnj.com/projects/ITX-ADW/repos/artemiseks/browse/_scm_cluster\n        \n        \n         production\n        \n       \n       \n        \n         Chap development\n        \n        \n         https://sourcecode.jnj.com/projects/ITX-WCR/repos/datacove/browse/_scm_cluster\n        \n        \n         development\n        \n       \n       \n        \n         Chap production\n        \n        \n         https://sourcecode.jnj.com/projects/ITX-WCR/repos/datacove/browse/_scm_cluster\n        \n        \n         production\n        \n       \n      \n     \n     \n      Once the cluster was provisioned, you'll receive an e-mail containing the details to configure\n      \n       kubectl\n      \n      . Please forward to the datacoves team.\n     \n     \n      The installer will need kubectl access to the cluster\n      \n       docs\n      \n      .\n     \n     \n     \n     \n      Opt out from EFS CSI driver\n     \n     \n      The EFS CSI driver installed by cloudx is usually outdated (v1.0.0) so we need to opt out from the cloudx managed service.\n     \n     \n      To opt out from EFS CSI managed driver, create a pull request on this repo, similar to this\n      \n       one\n      \n      .\n     \n     \n     \n     \n      External DNS\n     \n     \n      In the cluster.yaml configuration there is a key\n      \n       external_dns\n      \n      . This key deploys the service\n      \n       External DNS\n      \n      to the cluster, managed by CloudX.\nThis service might not be available in some clusters yet, so a manual configuration might be needed on Route53 or any other DNS service, typically a CNAME record pointing to the cluster's load balancer hostname.\n     \n     \n     \n     \n      Getting load balancer's hostname\n     \n     kubectl -n ingress-nginx get svc ingress-nginx-controller -o=jsonpath='{.status.loadBalancer.ingress[0].hostname}'\n\n     \n     \n     \n      SSL Certificates Manager\n     \n     \n      CloudX will install\n      \n       Cert Manager\n      \n      if the cluster supports it.\n     \n     \n      If Cert Manager is not installed, 2 SSL certificates need to be issued manually:\n- wildcard certificate: *.[SUBDOMAIN].[DOMAIN]\n- root certificate: [SUBDOMAIN].[DOMAIN]\n     \n     \n      A certificate chain file and a Private key are required for each certificate, please send the 4 files to Datacoves staff.\n     \n     \n     \n     \n      Git repositories\n     \n     \n     \n     \n      Config repo\n     \n     \n      Each datacoves installation requires a configuration repo where Datacoves staff will store configuration details.\n     \n     \n      Please create one repo per kubernetes cluster and grant access to Datacoves staff.\n     \n     \n     \n     \n      Dbt analytics repo\n     \n     \n      This is the repo where your analytics (dbt) project resides, along with airflow dags, db security roles, documentation, etc.\n     \n     \n     \n     \n      Git Service Account\n     \n     \n      Please create a Service Account with read access to the analytics repo, since that service account will be configured on services like Airflow and dbt-docs to read files from the repo.\n     \n     \n      To do so, submit a PR to have Cloudx stop managing the currently installed driver here: https://sourcecode.jnj.com/projects/ITX-AED/repos/cloudx_container_pipelines_configs/browse/argocd/config.yaml#19\n     \n     \n      This account will be also used by Jenkins to download images from artifactory (taqy-docker namespace), so please request access to\n      \n       taqy-docker\n      \n      on that account via AppDevTools.\n     \n     \n     \n     \n      Database\n     \n     \n      Some services require Postgres databases, as described below. These databases can share an RDS instance or aurora cluster. You will need to create this database cluster/instance and ensure it can be accessed from the EKS cluster.\n     \n     \n     \n     \n      Minimum requirements\n     \n     \n      \n       Engine: Postgres\n      \n      \n       Version: 14.9\n      \n      \n       Multi-AZ: \"Single DB Instance\" for sandbox clusters, \"Multi-AZ DB Cluster\" if not.\n      \n      \n       Master user: postgres\n      \n      \n       Master password:\n       \n       \n      \n      \n       Instance class: db.r5.large\n      \n      \n       Storage type: Aurora Standard or gp2\n      \n      \n       Allocated_storage: 100GB\n      \n      \n       Enable storage autoscaling\n      \n      \n       Maximum storage threshold: 1TB\n      \n      \n       Authentication: password\n      \n     \n     \n      Keep in mind that JNJ cycles the master password every 24 hours so you need to run any setup command using this password before that happens.\n     \n     \n     \n     \n      Initial database and user\n     \n     \n      You'll need to create a master Postgres user and the datacoves database:\n     \n     CREATE USER datacoves PASSWORD insert_generated_random_password_without_special_characters;\nALTER USER datacoves CREATEDB CREATEROLE;\nGRANT datacoves TO postgres;\nCREATE DATABASE datacoves OWNER datacoves;\nREVOKE connect ON DATABASE datacoves FROM PUBLIC;\nGRANT connect ON DATABASE datacoves TO datacoves;\nGRANT connect ON DATABASE datacoves TO postgres;\n\n     \n      A way to generate passwords:\n      \n       python -c 'import secrets; print(secrets.token_urlsafe())'\n      \n      .\nAvoid special characters, they cause issues with some services, such as airflow.\n     \n     \n      Please share this password with the Datacoves team.\n     \n     \n     \n     \n      Active Directory groups\n     \n     \n      Roles/groups required for datacoves users:\n     \n     JNJ-APP-{division}-DATACOVES-ADMIN\nJNJ-APP-{division}-DATACOVES-DEVELOPER\nJNJ-APP-{division}-DATACOVES-VIEWER\nJNJ-APP-{division}-DATACOVES-KTLO\n\n     \n      Substitute your\n      \n       {division}\n      \n      , e.g.\n      \n       PCE\n      \n      ,\n      \n       HMD\n      \n      ,\n      \n       CHAP\n      \n      , etc.\n     \n     \n     \n     \n      Ping identity account\n     \n     \n      Submit a ticket to\n      \n       Web Single Sign-On - SAML Federation\n      \n      to create a ping account.\n     \n     \n     \n     \n      IRIS Request\n     \n     \n     \n     \n      Short Description\n     \n     \n      This is a request to enable SSO for\n      \n       cluster.\n      \n     \n     \n     \n     \n      Description\n     \n     \n      Need to add PingID to application.\n     \n     \n     \n     \n      Groups\n     \n     \n      Need groups only filtered to ones that have the following pattern JNJ-APP-\n      \n       -DATACOVES-*\n      \n     \n     \n     \n     \n      Type\n     \n     \n      Choose: OAuth/OpenID Connect\n     \n     \n     \n     \n      Client id\n     \n     \n      It should be any name for your cluster (e.g.\n      \n       chapsbx\n      \n      ,\n      \n       emea_ensemble_test\n      \n      ,\n      \n       emea_artemis_dev\n      \n      , etc.).\n     \n     \n     \n     \n      Redirect urls\n     \n     \n      \n       https://api.{cluster_domain}/complete/ping_federate\n      \n     \n     \n     \n     \n      Additional fields\n     \n     \n      Requires interactive electronic signatures using SSO: No\nAttributes: groups, openid, profile, email\n     \n     \n      When the Iris request is fulfilled, you will receive an email with:\n     \n     \n      \n       Client ID (verify this is the one that was requested)\n      \n      \n       Client Secret\n      \n      \n       A list of OAuth endpoints\n      \n     \n     \n      Please share this information with the Datacoves team.\n     \n     \n     \n     \n      Airflow\n     \n     \n     \n     \n      EFS file system for airflow logs\n     \n     \n      Follow the instructions to \"Create EFS in AWS Account\" from\n      \n       this confluence page\n      \n      . Don't follow the other sections of the page.\n     \n     \n      As a name use datacoves-[cluster id]-[environment slug]-airflow-logs.\n     \n     \n      It's important to attach the right the EKS security group so the EKS cluster has access to the EFS filesystem. You can find the security group id in the EKS cluster admin page, Networking tab, under\n      \n       Additional security groups\n      \n      .\n     \n     \n     \n     \n      S3 bucket for Airflow dags\n     \n     \n      Due to bitbucket scheduled downtimes we recommend using S3 as the DAGs store to mimimize disruptions.\n     \n     \n      \n       Create an S3 bucket per environment, i.e. datacoves-[cluster id]-[environment slug]-airflow-dags (datacoves-ensemble-pro001-airflow-dags)\n      \n      \n       Create an IAM policy that grants read/write access to the new S3 bucket created, use the same name convention used for the S3 bucket.\n      \n      \n       Follow\n       \n        this instructions\n       \n       to create an IAM Role, up to \"Create IAM Role For K8s Service Account\", attach the policy you created on step 2. Name the IAM role using the same convention you used for the S3 bucket\n      \n      \n       Do not associate the IAM role to a K8s Service Account, that part is managed by Datacoves.\n      \n      \n       Create a IAM user for jenkins to upload the dbt project and dags to S3. Use the same naming convention. Attach the same policy you created on step 2.\n      \n     \n     \n     \n     \n      Trusted policy example:\n     \n     {\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Principal\": {\n                \"Federated\": \"arn:aws:iam::327112934799:oidc-provider/oidc.eks.us-east-1.amazonaws.com/id/125EA29C302DF7DBB900ED84AA85F0BB\"\n            },\n            \"Action\": \"sts:AssumeRoleWithWebIdentity\",\n            \"Condition\": {\n                \"StringLike\": {\n                    \"oidc.eks.us-east-1.amazonaws.com/id/125EA29C302DF7DBB900ED84AA85F0BB:sub\": \"system:serviceaccount:dcw-dev123:dev123-airflow-*\",\n                    \"oidc.eks.us-east-1.amazonaws.com/id/125EA29C302DF7DBB900ED84AA85F0BB:aud\": \"sts.amazonaws.com\"\n                }\n            }\n        }\n    ]\n}\n\n     \n     \n     \n      DBT API\n     \n     \n      \n       Create an S3 bucket.\n      \n      \n       Choose a bucket name, we suggest using\n       \n        _dbt_api where\n        \n         could be\n         \n          ensemble\n         \n         ,\n         \n          ensembletest\n         \n         , etc.\n        \n       \n      \n      \n       Create an IAM user with a policy to access the bucket, like the one below,\n  replacing\n       \n        {your_bucket_name}\n       \n       with your bucket's name.\n      \n      \n       Create an access key for the user. Share it with the Datacoves team.\n      \n     \n     {\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:PutObject\",\n        \"s3:GetObject\",\n        \"s3:GetObjectVersion\",\n        \"s3:DeleteObject\",\n        \"s3:DeleteObjectVersion\"\n      ],\n      \"Resource\": \"arn:aws:s3:::{your_bucket_name}/*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:ListBucket\",\n        \"s3:GetBucketLocation\"\n      ],\n      \"Resource\": \"arn:aws:s3:::{your_bucket_name}\"\n    }\n  ]\n}\n\n     \n     \n     \n      Grafana\n     \n     \n      Grafana requires an S3 bucket with lifecycle management enabled.\nFollow\n      \n       this guide\n      \n      to configure it accordingly.\n     \n     \n     \n     \n      Airbyte\n     \n     \n      \n       S3 bucket for airbyte logs, an IAM user with a policy to access it, and an\n  access key for the user.\n      \n     \n     \n     \n     \n      S3 bucket for airbyte logs\n     \n     \n      \n       Create an S3 bucket.\n      \n      \n       Create an IAM user with a policy to access the bucket, like the one below,\n  replacing\n       \n        {your_bucket_name}\n       \n       with your bucket's name.\n      \n      \n       Create an access key for the user. Share it with the Datacoves team.\n      \n     \n     {\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:PutObject\",\n        \"s3:GetObject\",\n        \"s3:GetObjectVersion\",\n        \"s3:DeleteObject\",\n        \"s3:DeleteObjectVersion\"\n      ],\n      \"Resource\": \"arn:aws:s3:::{your_bucket_name}/*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:ListBucket\",\n        \"s3:GetBucketLocation\"\n      ],\n      \"Resource\": \"arn:aws:s3:::{your_bucket_name}\"\n    }\n  ]\n}\n\n     \n     \n     \n      Data warehouse connection templates\n     \n     \n      Please define how your data warehouse architecture will look and define the connection templates for both Analytics Engineers and Services, I.e. on a Snowflake database you’ll need to specify fields such as account, warehouse, database, role.\n     \n     \n     \n     \n      Terraform\n     \n     \n      Some work has been done (repo:\n      \n       itx-azt/iac\n      \n      ) to automate the creation of\nthese cluster requirements using terraform. However, because of authorization\nrestrictions imposed on terraform in jnj, it still requires manual\nintervention. At the moment it is probably faster overall to do everything\nmanually."},"3":{"url":"/docs_output/client-docs/jnj/2-configuration.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      Configuring datacoves\n     \n     \n      Configuration for each cluster is kept in a separate repository. They are\nmounted as git submodules under\n      \n       config/{cluster_domain}\n      \n      .\n     \n     \n      You will need to create this git repository if there isn't one already for your\ncluster. Grant access to datacoves staff to this repo so we can initialize the\nconfiguration files and add the people that will do configuration or deployment\nto the git secret keyring.\n     \n     \n      Clone this configuration to make changes to it. Alternatively, if you will run\nthe datacoves deployment from the same machine you can clone the datacoves_deployment\nrepository which has the configuration repos as\n      \n       git submodules\n      \n      .\n     \n     git clone https://sourcecode.jnj.com/scm/asx-ahrx/datacoves_deployment.git\ncd datacoves_deployment\ngit submodule init\ngit submodule update config/$cluster_domain  # Specify the path to the submodule to update.\ncd config/$cluster_domain # Config repo cloned as submodule in here.\n\n     \n      After the initial setup, the workflow to update configuration is as follows:\n     \n     # From within the cluster configuration repo.\n\n# 1. Fetch the latest configuration.\ngit checkout main\ngit pull\ngit secret reveal -f\n\n# 2. Make your changes (see below what's required).\n\n# 3. Commit and push your changes.\ngit secret hide\ngit diff # Review your changes, all sensitive data should be encrypted.\ngit add .\ngit commit -m 'Updated secrets/configuration.'\ngit push\n\n     \n     \n     \n      What values are required?\n     \n     \n      Initially the configuration files will contain\n      \n       TODO\n      \n      comments to mark the\nplaces where values need to be filled in. Run\n      \n       grep -r . TODO\n      \n      to see what's\npending. Remove the\n      \n       TODO\n      \n      comments when you add a value. Most values are used\nto configure the external services that were created during\n      \n       cluster setup\n      \n      .\n     \n     \n      The configuration variable names should give you an indication of what's needed.\nIf in doubt, ask.\n     \n     \n      The requirements for each datacoves service follow. The list may be a useful\nguide but it could be out of date. Please rely on the\n      \n       TODO\n      \n      marks, not on the\nlist, as authoritative information.\n     \n     \n     \n     \n      Datacoves core\n     \n     \n      \n       Datacoves api DB host (\n       \n        DB_HOST\n       \n       ) and password (\n       \n        DB_PASS\n       \n       ) in\n       \n        secrets/core-api.env\n       \n      \n      \n       PING_CLIENT_ID and PING_CLIENT_SECRET in\n       \n        secrets/core-api.env\n       \n      \n      \n       Ping group names in\n       \n        cluster-params.yaml\n       \n       , under\n       \n        project\n       \n       .\n      \n      \n       Postgres DB Provisioner for services such as airbyte/airfow/superset in\n       \n        cluster-params.secret.yaml\n       \n       under\n       \n        postgres_db_provisioner\n       \n       .\n      \n     \n     \n     \n     \n      DBT Docs\n     \n     \n      \n       Deploy credentials in\n       \n        cluster-params.secret.yaml\n       \n       under\n       \n        deploy_credentials\n       \n       .\n      \n     \n     \n     \n     \n      Airbyte\n     \n     \n      Not yet documented.\n     \n     \n     \n     \n      Airflow\n     \n     \n      The EFS CSI driver installed by cloudx is usually outdated (v1.0.0) so we need to opt out from the cloudx managed service.\n     \n     \n      To do so, submit a PR to have Cloudx stop managing the currently installed driver here: https://sourcecode.jnj.com/projects/ITX-AED/repos/cloudx_container_pipelines_configs/browse/argocd/config.yaml#19\n     \n     \n      \n       Airflow EFS volume_handle (fs id) in:\n       \n        environments/dev123/airflow.secret.yaml\n       \n      \n     \n     \n     \n     \n      Superset\n     \n     \n      Not yet documented."},"4":{"url":"/docs_output/client-docs/jnj/3-configure-bastion-ec2-instance.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      Configure Bastion EC2 instance\n     \n     \n     \n     \n      JNJ\n     \n     \n      Name: \nHost: AWSAZTIRLL000Q.jnj.com\n     \n     \n     \n     \n      SSH to instance\n     \n     \n      \n       In your AWS workspace/Microsoft Remote Desktop (windows) open a terminal\n       \n        ssh 10.157.82.138 -m hmac-sha2-512\n       \n       or\n      \n      \n       Create a shortcut to ssh pointing to\n       \n        C:\\Windows\\System32\\OpenSSH\\ssh.exe 10.157.82.138 -m hmac-sha2-512\n       \n      \n      \n       Click on the shortcut and type your password to access the instance\n      \n     \n     \n     \n     \n      CHAP\n     \n     \n      Name: itx-wcr-EKS workstation\nHost: awswcrnval001n.kenvue.com\n     \n     \n     \n     \n      Request role\n     \n     \n      In your\n      \n       Remote Desktop\n      \n      go to\n      \n       IAM\n      \n      :\n     \n     \n      \n       Request / Star a new request\n      \n      \n       Request the following roles:\n       \n        \n         ITS-ITX-WCR-Datacove-Prd-K8sOperator\n        \n        \n         ITS-ITX-WCR-Datacove-Prd-K8sMonitor\n        \n        \n         ITS-ITX-WCR-Datacove-Prd-K8sAdmin\n        \n        \n         ITS-EP-AWSWCRNVAL001N-LINUX-NA-UNIXSEAdmins\n        \n       \n      \n      \n       Details:\n       \n        \n         Job role: Datacoves Support\n        \n        \n         Application ID: APP000300001207\n        \n        \n         Application Name: DATACOVES-ANALYTICS PRODUCTION WORKBENCH FOR ELT &amp; ORCHESTRATION\n        \n        \n         Describe, in detail, the job functions you perform that REQUIRE this level of privilege: We maintain and support the Datacoves application which runs on Kubernetes.\n        \n        \n         Is the Application Software (includes Web Components, Vendor Application), installed on the Server on which you are requesting Admin Access? No / Yes: No\n        \n        \n         Frequency of Need: Weekly\n        \n       \n      \n      \n       Submit\n      \n     \n     \n     \n     \n      SSH to instance\n     \n     \n      \n       On the terminal run command\n       \n        ssh 10.79.29.123\n       \n      \n      \n       Your user should be added to the following groups in\n       \n        /etc/groups\n       \n      \n     \n     \n     \n     \n      Create your working directory\n     \n     \n      Create your working directory under\n      \n       /app/users\n      \n      , i.e.\n      \n       /app/users/ssassi\n      \n      .\n     \n     \n     \n     \n      Grant you access to docker\n     \n     sudo su -\nvi /etc/group\n\n     \n      Example:\n     \n     datacoves:x:8653:amorer01,&lt;my-user&gt;  # To chap\ndocker:x:187:amorer01,&lt;my-user&gt;\n\n     \n     \n     \n      Configure your home folder (~)\n     \n     \n      \n       Copy the contents of\n       \n        /app/users/datacoves-home-template\n       \n       to your home folder:\n      \n     \n     cp -R /app/users/datacoves-home-template/. ~/\n\n     \n      \n       Exit and reconnect to the instance to ensure that the\n       \n        .bashrc\n       \n       script was ran accordingly\n      \n      \n       Fix kubelogin permissions\n      \n     \n     asdf uninstall kubelogin\nasdf install kubelogin\n\n     \n      \n       Configure your credentials to the clusters\n      \n     \n     kc config get-contexts\nkc config use-context &lt;choose one&gt;\nkc get ns\n\n     \n      Note: you'll need to change your ~/.kube/config permissions:\n     \n     chmod 600 ~/.kube/config\n\n     \n     \n     \n      Clone datacoves deployment repo\n     \n     /app/users/&lt;your username&gt;\ngit clone https://sourcecode.jnj.com/scm/asx-ahrx/datacoves_deployment.git\n\n     \n      After clonning, follow the instructions to reveal secrets and install requirements."},"5":{"url":"/docs_output/client-docs/jnj/4-configure-bitbucket-and-jenkins.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      How to configure Bitbucket project and connect it with Jenkins project\n     \n     \n     \n     \n      Bitbucket\n     \n     \n     \n     \n      Ensure you enable the following hooks in your Bitbucket project\n     \n     \n      \n       JnJ VPCx - Post Receive Repository Hook for SCM\n      \n      \n       Webhook to Jenkins for Bitbucket Server\n      \n     \n     \n      \n     \n     \n     \n     \n      JnJ VPCx - Post Receive Repository Hook for SCM\n     \n     \n      \n     \n     \n     \n     \n      Webhook to Jenkins for Bitbucket Server\n     \n     \n     \n     \n      Tab 1\n     \n     \n      \n     \n     \n     \n     \n      Tab 2\n     \n     \n      \n     \n     \n     \n     \n      Tab 3\n     \n     \n      \n     \n     \n     \n     \n      Enable the following Merge Checks\n     \n     \n      \n     \n     \n     \n     \n      Request access to taqy-docker for the project service account\n     \n     \n      Typically the service account created automatically is\n      \n       sa-itsus-&lt;PROJECT CODE&gt;-devusr\n      \n      .\n     \n     \n      Go to App Dev Tools and request access for that user, like so:\n     \n     \n      \n     \n     \n     \n     \n      Jenkins\n     \n     \n     \n     \n      Ensure Bitbucket plugins were correctly configured\n     \n     \n      Navigate to Manage Jenkins -&gt; Configure System and modify the following plugins:\n     \n     \n      \n     \n     \n      \n     \n     \n     \n     \n      Create Multibranch pipeline project\n     \n     \n      At Home page -&gt; \"+ New Item\":\n     \n     \n      \n     \n     \n     \n     \n      Configure branch sources\n     \n     \n      \n     \n     \n     \n     \n      Configure repo behaviors\n     \n     \n      \n     \n     \n     \n     \n      Set up build configuration and other items\n     \n     \n      \n     \n     \n      \n     \n     \n     \n     \n      Jenkinsfile dependencies\n     \n     \n      You'll need a credential that stores the secrets used to connect to your Data Warehouse.\n     \n     \n      Create a new credential in the Jenkins Admin area. As of Aug. '23 those can be found in:\n     \n     \n      \n       Dashboard -&gt; Credentials -&gt; System -&gt; Global Credentials (unrestricted)\n      \n     \n     \n      \n     \n     \n      \n     \n     \n     \n     \n      Known issues\n     \n     \n      \n       When \"pre hook declined\" it could be due to JIRA issues configuration: from settings -&gt;\n       \n        Jira Issues\n       \n       select \"Use custom settings\" and  be sure \"Don't need a Jira issue key\" is selected"},"6":{"url":"/docs_output/client-docs/jnj/5-deployment.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      How to deploy (or update) datacoves to a kubernetes cluster\n     \n     \n      Prerequisites:\n      \n       cluster and external resources setup\n      \n      .\n     \n     \n      SSH into a machine with kubectl access to the cluster from where you will run\nthe installation scripts. Then:\n     \n     # Set these as needed for your cluster.\ncluster_domain=FILL_IN   # e.g. ensembletest.apps.jnj.com\nkubectl_context=FILL_IN  # e.g. itx-ank-ensemble-test\n\n# Clone the repository into the installation workstation (required once).\ngit clone https://sourcecode.jnj.com/scm/asx-ahrx/datacoves_deployment.git\ncd datacoves_deployment\n\n# Install python dependencies for the installation scripts (required once).\npip3 install --user -r requirements.txt\n\n# Fetch the latest changes and reveal the secrets in the config submodule directory.\ngit pull\ngit submodule update --init\n(cd config/$cluster_domain; git secret reveal -f)\n\n# Install datacoves base dependencies into the cluster (ingress-nginx, etc.)\n# Usually not required after the first time datacoves is released to a cluster.\n./cli.py setup_base $kubectl_context $cluster_domain\n\n# Deploying ingress-nginx will create an ELB. Use the following command to retrieve it's URL.\nkubectl --context $kubectl_context get -A svc | grep LoadBalancer\n\n# Update cluster-params.yaml setting external_dns_url to that URL.\n$EDITOR config/$cluster_domain/cluster-params.yaml\n# Commit the change.\n\n# Install/update datacoves.\n./cli.py install"},"7":{"url":"/docs_output/client-docs/jnj/6-access.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      Access Datacoves on JNJ clusters\n     \n     \n     \n     \n      Requesting Roles\n     \n     \n      \n       \n        NOTE:\n       \n       Please inform Martin Ryan before requesting appdevtools roles\n      \n     \n     \n      In order to have access to all third-party tools (Bitbucket, Jenkins, Artifactory, etc.) you must request specific roles.\n     \n     \n      To do so, you must go to https://appdevtools.jnj.com and request the\n      \n       Datacoves Support\n      \n      Model User template.\n     \n     \n      Make sure to write\n      \n       Needed for Datacoves platform support\n      \n      as requesting reason.\n     \n     \n      \n     \n     \n     \n     \n      Datacoves Access\n     \n     \n      In order to get access as an admin and developer on the different JNJ clusters you need to request the following AD groups:\n     \n     \n     \n     \n      Medical Devices\n     \n     \n      \n       JNJ-APP-HMD-DATACOVES-ADMIN\n      \n      \n       JNJ-APP-HMD-DATACOVES-DEVELOPER\n      \n     \n     \n     \n     \n      Consumer Health / Kenvue\n     \n     \n      Go to iam.kenvue.com, search for\n      \n       Datacoves\n      \n      . There's one role for Admin and one per Environment. You need to request\n      \n       ADMIN\n      \n      ,\n      \n       *-DEVELOPER\n      \n      and\n      \n       *-SYSADMIN\n      \n      roles. For example:\n     \n     \n      \n       ITS_APP_DATACOVES_ADMIN\n      \n      \n       ITS_APP_DATACOVES_DEV_CBI_VMA_DEVELOPER\n      \n      \n       ITS_APP_DATACOVES_DEV_CBI_VMA_SYSADMIN\n      \n      \n       ITS_APP_DATACOVES_DEV_COMX_CUST_DEVELOPER\n      \n      \n       ITS_APP_DATACOVES_DEV_COMX_CUST_SYSADMIN\n      \n      \n       ITS_APP_DATACOVES_DEV_COMX_GLOBAL_DEVELOPER\n      \n     \n     \n     \n     \n      Ensemble\n     \n     \n      \n       JNJ-APP-PCE-DATACOVES-ADMIN\n      \n      \n       JNJ-APP-PCE-DATACOVES-DEVELOPER\n      \n     \n     \n     \n     \n      R&amp;D\n     \n     \n      \n       ITS-APP-DEV-JRDDATACOVES-ADMIN\n      \n      \n       ITS-APP-DEV-JRDDATACOVES-DEVELOPER\n      \n     \n     \n     \n     \n      taqy Access\n     \n     \n      taqy is the docker repository used by all J&amp;J instances.  Access to it is necessary in order to manage images on it.\n     \n     \n      To request access, use https://appdevtools.jnj.com\n     \n     \n      \n       Request Access, By User, Other\n      \n      \n       Enter your username\n      \n      \n       Tool: EAT Jenkins and Artifactory\n      \n      \n       Team: taqy\n      \n      \n       Reason for request: Access to CI images for DataCoves\n      \n      \n       Grant these roles: ITS-ASx-TAQY-DEV-Executors, ITS-ASx-TAQY-DEV-Viewers\n      \n     \n     \n      For reference, the main Ensemble Jenkins user is sa-itsus-jbfl-devusr\n     \n     \n     \n     \n      Snowflake Access\n     \n     \n      As done with the groups above, you must also request\n      \n       JNJ-APP-PCE-SNOWFLAKE-EMEA-DEVELOPER\n      \n     \n     \n     \n     \n      How to request the groups?\n     \n     \n      Using the AWS workspace:\n     \n     \n      \n       Navigate to https://iam.jnj.com\n      \n      \n       Click on\n       \n        Start new request\n       \n      \n      \n       Type the group name on the\n       \n        Find a service item\n       \n       search box.\n      \n      \n       Click on\n       \n        Request\n       \n       button\n      \n      \n       In the popup, leave\n       \n        Valid from\n       \n       and\n       \n        Valid until\n       \n       empty, in the\n       \n        reason\n       \n       field type \"Datacoves support team\"."},"8":{"url":"/docs_output/client-docs/jnj/7-configure-sa-docker-in-kubernates.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      How to configure Service Account Docker in Kubernetes for pull images.\n     \n     \n      \n       JnJ\n      \n      and\n      \n       Kenvue\n      \n      are using their own private Docker artifact repositories. In order to download images from those repositories in Kubernetes we need to create secrets with valid credentials in each Kubernetes cluster.\n     \n     \n      This process is documented by JnJ at\n      \n       conflunce\n      \n      .\n     \n     \n     \n     \n      Select Kubernates context\n     \n     kubectl config get-contexts\nkubectl config use-context &lt;context&gt;\n\n     \n     \n     \n      Delete old service account (If it already exists)\n     \n     kubectl get secrets -n default\nkubectl delete secret taqy-docker -n default\n\n     \n     \n     \n      Create new service account\n     \n     # Create secret in default namespace - Recommended to use the EAT service account username and password for credentials\nkubectl create secret docker-registry taqy-docker --docker-server=jnj.artifactrepo.jnj.com --docker-username=&lt;service-account-username&gt; --docker-password=&lt;service-account-password&gt; -n default\n\n# Annotate secret to sync across all namespaces\nkubectl annotate secret taqy-docker cluster.managed.secret=\"true\" -n default\n\n     \n     \n     \n      Inspect the new secret\n     \n     kubectl -n default get secret taqy-docker -o yaml\n\n     \n      Copy the value from\n      \n       data.dockerconfigjson\n      \n     \n     echo &lt;value&gt; | base64 -d\n\n     \n      Note: Check that the secrets have been replicated to all namespaces. (Can check one or two)\n     \n     kubectl -n &lt;namespace&gt; get secret taqy-docker -o yaml\necho &lt;value&gt; | base64 -d\n\n     \n      If the secret was not replicated, check the pod's logs:\n     \n     kubectl -n kube-system get pods\nkubectl -n kube-system logs namespace-secrets-sync-&lt;hash&gt; --tail 100"},"9":{"url":"/docs_output/client-docs/jnj/8-summary-requirements-new-cluster.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      Summary for the requirements of a new Cluster.\n     \n     \n      For more details check\n      \n       Cluster requirements\n      \n     \n     \n     \n     \n      Database (RDS)\n     \n     \n     \n     \n      Minimum requirements\n     \n     \n      \n       Engine: Postgres\n      \n      \n       Version: 14.9\n      \n      \n       Multi-AZ DB Cluster.\n      \n      \n       Master user: postgres\n      \n      \n       Master password:\n       \n       \n      \n      \n       Instance class: db.r5.large\n      \n      \n       Storage type: Aurora Standard or gp2\n      \n      \n       Allocated_storage: 100GB\n      \n      \n       Enable storage autoscaling\n      \n      \n       Maximum storage threshold: 1TB\n      \n      \n       Authentication: password\n      \n     \n     \n     \n     \n      EKS\n     \n     \n     \n     \n      Configuration\n     \n     \n      \n       External DNS.\n      \n      \n       \n        m5.xlarge\n       \n       instances.\n      \n     \n     \n     \n     \n      Worker groups\n     \n     \n      \n       General\n      \n      \n       Volumed\n      \n      \n       Workers\n      \n     \n     \n     \n     \n      General\n     \n     \n      \n       min_nodes: 1\n      \n      \n       max_nodes: 30\n      \n      \n       root_volume_size: 200\n      \n      \n       labels:\n      \n     \n     labels:\n    ...\n    - key: k8s.datacoves.com/nodegroup-kind\n    value: general\n\n     \n     \n     \n      Volumed\n     \n     \n      \n       min_nodes: 1\n      \n      \n       max_nodes: 30\n      \n      \n       root_volume_size: 200\n      \n      \n       labels:\n      \n     \n     labels:\n    ...\n    - key: k8s.datacoves.com/nodegroup-kind\n    value: volumed\n\n     \n     \n     \n      Workers\n     \n     \n      \n       min_nodes: 1\n      \n      \n       max_nodes: 30\n      \n      \n       root_volume_size: 200\n      \n      \n       labels:\n      \n     \n     labels:\n    ...\n    - key: k8s.datacoves.com/workers\n    value: enabled\n\n     \n     \n     \n      Other configuration.\n     \n     \n      \n       EFS for each environment for\n       \n        Airflow Logs\n       \n       .\n      \n      \n       S3 buckets for each environment for\n       \n        Dags sync\n       \n       , with read-only permissions. (Optional. Can be git-sync).\n      \n      \n       One S3 bucket for\n       \n        Observavility stack\n       \n       . Example\n       \n        ensemble-prd-observability-grafana-loki\n       \n       . (Full permissions)\n      \n      \n       One S3 bucket for\n       \n        dbt-api\n       \n       . Example\n       \n        ensemble-prd-dbt-api\n       \n       . (Full permissions)\n      \n     \n     \n     \n     \n      Example for full S3 bucket permission\n     \n     {\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:PutObject\",\n        \"s3:GetObject\",\n        \"s3:GetObjectVersion\",\n        \"s3:DeleteObject\",\n        \"s3:DeleteObjectVersion\"\n      ],\n      \"Resource\": \"arn:aws:s3:::{your_bucket_name}/*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:ListBucket\",\n        \"s3:GetBucketLocation\"\n      ],\n      \"Resource\": \"arn:aws:s3:::{your_bucket_name}\"\n    }\n  ]\n}"},"10":{"url":"/docs_output/client-docs/jnj/","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      Datacoves deployment\n     \n     \n      This repository contains the datacoves installation scripts. They install\ndatacoves to an existing EKS cluster, based on the configuration files in the\n      \n       config\n      \n      directory. Configuration for each cluster is kept in a separate\nrepository. They are mounted as git submodules under\n      \n       config/{cluster_domain}\n      \n      .\n     \n     \n      Before running the installation scripts the EKS cluster and other required AWS\nresources must be created. See\n      \n       cluster requirements\n      \n      .\n     \n     \n      Then a repository to use as the cluster configuration submodule must be created.\nSee\n      \n       configuration\n      \n      .\n     \n     \n      After that, deployment can begin. See\n      \n       deployment\n      \n      ."},"11":{"url":"/docs_output/client-docs/kenvue/how-to-setup-helm-chart.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      How to set up Helm Chart on kenvue\n     \n     \n      Artifacory: https://kenvue.jfrog.io\nRepository: dco-helm\nCredentials: See 1password\nProtocol: OCI\n     \n     \n      Steps:\n     \n     \n      \n       Artifactory login.\n      \n      \n       Download or build the helm chart.\n      \n      \n       Upload the new helm chart.\n      \n      \n       Check the new helm chart.\n      \n      \n       Install the helm chart.\n      \n     \n     \n     \n     \n      1. Artifactory login\n     \n     helm registry login https://kenvue.jfrog.io/dco-helm\n\n     \n     \n     \n      2. Build or download the helm chart.\n     \n     \n      In this case as an example we are going to download a helm chart from the JnJ artifactory\n     \n     wget --user &lt;my-user&gt; --password &lt;my-password&gt; https://artifactrepo.jnj.com:443/artifactory/jnj-helm-charts/metrics-server-3.12.2.tgz\n\n     \n     \n     \n      3. Upload the new helm chart.\n     \n      helm push metrics-server-3.12.2.tgz oci://kenvue.jfrog.io/dco-helm/metrics-server\n\n     \n     \n     \n      4. Check the new helm chart.\n     \n     helm show all oci://kenvue.jfrog.io/dco-helm/metrics-server\n\n     \n     \n     \n      5. Install the helm chart.\n     \n     helm install my-release oci://kenvue.jfrog.io/dco-helm/metrics-server --version 3.12.2"},"12":{"url":"/docs_output/client-docs/orrum/","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      Datacoves deployment\n     \n     \n      This section contains instructions on how to connect to Orrum infra via azure to build, maintain, and monitor datacoves deployments.\n     \n     \n     \n     \n      VPN Connection\n     \n     \n      kubectl requires connection to Orrum VPN. Download\n      \n       Azure VPN Client\n      \n      .\n     \n     \n      The profile can be downloaded from Azure; login with Support_Datacoves@orrumcorp.onmicrosoft.com with the credentials from 1Password.\n     \n     \n      https://portal.azure.com/#@orrum.com/resource/subscriptions/0f8e4c48-c319-4ed9-af14-ef50501e3a41/resourceGroups/DataCoves/providers/Microsoft.Network/virtualNetworkGateways/DataCovesGateway/pointtositeconfiguration\n     \n     \n      Click \"Download VPN client\" in the header, and you will get a zip file with the profile files; you will want the Azure client profiles, and you can use the Import button in the Azure client to import it.\n     \n     \n      To connect to the vpn, use Support_Datacoves@orrumcorp.onmicrosoft.com, credentials on 1Password.\n     \n     \n     \n     \n      kubectl setup\n     \n     # Ensure Python is Installed\npipx install az-cli --include-deps\n\n# Get login password from 1pswd\naz login -u Support_Datacoves@orrumcorp.onmicrosoft.com\n\n# Install kubectl + kubelogin\naz aks install-cli\n\n# Set subscription\naz account set --subscription 0f8e4c48-c319-4ed9-af14-ef50501e3a41\n\n# Get credentials for new cluster\naz aks get-credentials --resource-group DataCoves --name Datacoves_kube\n\n# List contexts\nkubectl config use-context Datacoves_kube\n\n     \n     \n     \n      Rename Context\n     \n     \n      It is very important that the context be named orrum-new as things such as updating the cluster will have scripts that depend on the context name.\n     \n     kubectl config rename-context Datacoves_kube orrum-new\nkubectl config use-context orrum-new\n\n     \n      Now verify connectivity with\n      \n       kubectl get ns\n      \n     \n     \n     \n     \n      Config DNS on\n      \n       /etc/hosts\n      \n      (Optional)\n     \n     \n      Note: This is probably not necessary anymore.\n     \n     \n      You can force the domain and subdomains DNS if it's not configured.\n     \n     10.10.0.36       datacoves.orrum.com\n10.10.0.36       api.datacoves.orrum.com\n10.10.0.36       authenticate-dev123.datacoves.orrum.com\n10.10.0.36       dev123.datacoves.orrum.com\n10.10.0.36       airbyte-dev123.datacoves.orrum.com\n10.10.0.36       dbt-docs-dev123.datacoves.orrum.com\n10.10.0.36       airflow-dev123.datacoves.orrum.com\n10.10.0.36       superset-dev123.datacoves.orrum.com\n10.10.0.36       grafana.datacoves.orrum.com\n\n# &lt;user&gt;\n10.10.0.36       &lt;user&gt;-1-transform-dev123.datacoves.orrum.com\n10.10.0.36       &lt;user&gt;-1-dbt-docs-dev123.datacoves.orrum.com\n10.10.0.36       &lt;user&gt;-transform-dev123.datacoves.orrum.com\n\n     \n      \n       Note: Check the cluster's Public IP\n       \n        10.10.0.36"},"13":{"url":"/docs_output/dev-logs/2021-06-create-er-diagram.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      How to create an entity relationship diagram\n     \n     ./cli.py pod-sh\napt install graphviz-dev\npip3 install pygraphviz\n./manage.py graph_models -a -X *Mixin,Abstract*,ContentType,Session,Nonce,Partial,TokenProxy -g -o core-erd.png\n\n     \n      Learn more at https://django-extensions.readthedocs.io/en/latest/graph_models.html"},"14":{"url":"/docs_output/dev-logs/2021-09-eks-setup.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      Installation\n     \n     \n     \n     \n      Set up IAM user\n     \n     \n      IAM user needs the following privileges to create the cluster:\n     \n     \n      https://eksctl.io/usage/minimum-iam-policies/\n     \n     \n     \n     \n      AWS CLI\n     \n     \n      Install AWS CLI in your local environment\n     \n     \n      https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html\n     \n     \n     \n     \n      Configure credentials\n     \n     \n      \n       Generate access key\n      \n      \n       Configure your credentials\n      \n     \n     \n     \n     \n      Install eksctl\n     \n     \n      Install eksctl\n     \n     \n      https://docs.aws.amazon.com/eks/latest/userguide/eksctl.html\n     \n     \n     \n     \n      On Mac\n     \n     brew tap weaveworks/tap\nbrew install weaveworks/tap/eksctl\n\n     \n     \n     \n      Create cluster\n     \n     \n      https://eksctl.io/usage/creating-and-managing-clusters/\n     \n     eksctl create cluster -f cluster.yaml --tags service=datacoves\n\n     \n     \n     \n      Install metrics server\n     \n     \n      https://docs.aws.amazon.com/eks/latest/userguide/metrics-server.html\n     \n     kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\n\n     \n     \n     \n      Kubernetes dashboard\n     \n     \n      https://docs.aws.amazon.com/eks/latest/userguide/dashboard-tutorial.html\n     \n     kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.5/aio/deploy/recommended.yaml\nkubectl apply -f eks-admin-service-account.yaml\n\n     \n     \n     \n      Open dashboard\n     \n     kubectl proxy\n\n     \n      http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/#!/login\n     \n     \n      Get a login token with:\n     \n     kubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep eks-admin | awk '{print $1}')\n\n     \n     \n     \n      Configure Docker hub\n     \n     kubectl create ns alpha2\nkubectl create secret docker-registry docker-secret \\\n--docker-server=\"https://index.docker.io/v1/\" \\\n--docker-username=\"&lt;USER_NAME&gt;\" \\\n--docker-password=\"&lt;PASSWORD&gt;\" \\\n--docker-email=\"&lt;EMAIL&gt;\" \\\n--namespace=\"alpha2\"\n\n     \n     \n     \n      EKS (k8s on AWS)\n     \n     # Create the cluster  https://eksctl.io/usage/creating-and-managing-clusters/\neksctl create cluster -f eks/eks-cluster.yaml\n\n# (Optional) Inspect the config that kustomize generates\nkubectl kustomize eks\n\n# Apply the kustomization directory to the cluster\nkubectl apply -k eks\n\n     \n     \n     \n      Kubernetes dashboard\n     \n     \n      To open the dashboard run\n      \n       kubectl proxy\n      \n      and navigate to:\n     \n     \n      http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/#!/login\n     \n     # Get a login token with\nkubectl -n kube-system describe secret $(kubectl -n kube-system get secret | grep eks-admin | awk '{print $1}')"},"15":{"url":"/docs_output/dev-logs/2021-12-jnj-ensembledev-deployment.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      Datacoves deployment\n     \n     \n      This document describes the deployment of the components of a datacoves system\nto a JnJ EKS kubernetes cluster.\n     \n     \n     \n     \n      Prerequisites\n     \n     \n      \n       This confluence page\n      \n      should be followed prior to the steps outlined here to deploy datacoves. It\nshould document how to setup an EKS cluster with the necessary prerequisites,\nand how to create and configure the required AWS services used.\n     \n     \n      We assume here that there is a EKS cluster running with certain services already\ndeployed on it. The cluster is setup through CI from the git repo at\nhttps://sourcecode.jnj.com/projects/ITX-AZT/repos/ensemble.\nWe require the following systems running in the cluster:\n     \n     \n      \n       ingress-nginx as an ingress controller.\n      \n      \n       cert-manager to issue SSL certificates.\n      \n      \n       external-dns to create DNS rules from annotations.\n      \n      \n       A system that creates a new kubernetes secret with a known name with\n  credentials to pull docker images in every namespace of the cluster.\n      \n     \n     \n      The machine from where the deployments scripts will be run must have python3 and\ngit installed, as well as kubectl (client) version 1.21 or higher, configured\nto access the cluster with broad permissions.\n     \n     \n      We also assume the docker registry / repository that you configure to pull\nimages has all the docker images required. Datacoves will build and push this\nimages. The list of images used by a cluster, computed from this repo's\nconfiguration, can be displayed with\n      \n       ./cli.py images ensembledev.apps.jnj.com\n      \n      ,\nor in general\n      \n       ./cli.py images CLUSTER_DOMAIN\n      \n      .\n     \n     \n     \n     \n      Initial setup and configuration\n     \n     \n      Clone the datacoves_deployment git repository and change directory to it.\n     \n     git clone https://sourcecode.jnj.com/scm/asx-ahrx/datacoves_deployment.git\ncd datacoves_deployment\n\n     \n      Configuration is stored in the repo, encrypted using git-secret. You will need\nto be in the repo's git secret keyring to decrypt them. Ask someone already in\nthe keyring for access (e.g. spelufo@its.jnj.com).\n     \n     \n      Decrypt the configuration secrets. The\n      \n       -f\n      \n      flag will overwrite existing files.\n     \n     git secret reveal -f\n\n     \n      The\n      \n       config\n      \n      directory holds configuration files. Each subdirectory holds\nconfiguration for a kubernetes cluster and must be named after the cluster\ndomain name. For example, the configuration for the current (2021) version of\ndatacoves is in\n      \n       config/ensembledev.apps.jnj.com\n      \n      .\n     \n     \n      If deploying to a new cluster, create a new directory under config based on\n      \n       config/ensembledev.apps.jnj.com\n      \n      . You will need to use\n      \n       git secret add\n      \n      and\n      \n       git secret hide\n      \n      to add your new secrets to the repo and encrypt them before\ncommiting them.\n     \n     \n     \n     \n      Deploying datacoves core web application\n     \n     \n      First, make sure your kubectl context is appropiate for the cluster.\n     \n     CLUSTER_DOMAIN=ensembledev.apps.jnj.com\nKCTX=$(kubectl config current-context)\n\n# Deploy the datacoves core api server to the core namespace.\n./cli.py setup_core \"$KCTX\" \"$CLUSTER_DOMAIN\"\n\n     \n      Enter an api server pod and run database migrations:\n     \n     kubectl -n core exec -it $(kubectl -n core get pods -l app=core-api -o name) -- bash\n\n# From inside the pod:\n./manage.py migrate\n./manage.py loaddata */fixtures/*\n\n     \n      Check the server is running:\n     \n     $ kubectl -n core get pods\nNAME                                   READY   STATUS    RESTARTS   AGE\ncore-api-deployment-5f8f64cf69-6rvhd   1/1     Running   0          3d19h\n\n     \n     \n     \n      Deploying datacoves project operator\n     \n     \n      The datacoves project operator manages two\n      \n       CRDs\n      \n      :\ndatacoves.com/Project and datacoves.com/User. To deploy the operator, run:\n     \n     ./cli.py setup_operator \"$KCTX\" \"$CLUSTER_DOMAIN\"\n\n     \n      To check the operator is running, and/or see its logs:\n     \n     $ kubectl -n operator-system get pods\nNAME                                          READY   STATUS    RESTARTS   AGE\noperator-controller-manager-78cc7cfb6-9ddkw   2/2     Running   0          47h\n\n$ kubectl -n operator-system logs -l control-plane=controller-manager -c manager -f\n\n     \n     \n     \n      Deploying a datacoves project namespace\n     \n     \n      Every project is deployed to a namespace named\n      \n       dcp-{project_name}\n      \n      . The\nsetup_project script creates a new namespace and project kubernetes object from\nthe configuration file in\n      \n       config/{cluster_domain}/projects/{project_name}.yaml\n      \n      .\nThe operator will detect changes to this object and create deployments and other\nresources for the project.\n     \n     PROJECT_NAME=emeadev\n./cli.py setup_project \"$KCTX\" \"$CLUSTER_DOMAIN\" \"$PROJECT_NAME\"\n\n     \n      To watch for pod status changes as the operator create's the project resources:\n     \n     kubectl -n \"dcp-$PROJECT_NAME\" get pods --watch"},"16":{"url":"/docs_output/dev-logs/2022-04-jnj-artemisdev-configuration.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      Configuring datacoves\n     \n     \n      Requirements: Access to a datacoves configuration git repo and being in it's git secret keyring.\n     \n     \n      First pull the latest changes and reveal the git secrets.\n     \n     git checkout main\ngit pull\ngit secret reveal -f\n\n     \n      I've marked with\n      \n       TODO\n      \n      the values that need to be filled in:\n     \n     \n      \n       Airflow DB connection in:\n       \n        environments/dev123/airflow.secret.yaml\n       \n      \n      \n       Airflow EFS volume_handle (fs id) in:\n       \n        environments/dev123/airflow.secret.yaml\n       \n      \n      \n       Datacoves api DB host (\n       \n        DB_HOST\n       \n       ) and password (\n       \n        DB_PASS\n       \n       ) in\n       \n        secrets/core-api.env\n       \n      \n      \n       PING_CLIENT_ID and PING_CLIENT_SECRET in\n       \n        secrets/core-api.env\n       \n      \n     \n     \n      After editing those files to add the required values commit the changes with:\n     \n     git secret hide\ngit diff # Review your changes, all sensitive data should be encrypted.\ngit add .\ngit commit -m 'Updated secrets.'\ngit push"},"17":{"url":"/docs_output/dev-logs/2022-04-jnj-ensembletest-deployment.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      Datacoves deployment\n     \n     \n      This repository contains the datacoves installation scripts. They install datacoves to an existing EKS cluster, based on the configuration files in the\n      \n       config\n      \n      directory. Configuration for each cluster is kept in a separate repository. They are mounted as git submodules under\n      \n       config/{cluster_domain}\n      \n      .\n     \n     \n      Prior to this, the EKS cluster and other required AWS resources must be created. The clusters are created through CloudX pipelines, from\n      \n       cluster.yaml\n      \n      files in other repositories like\n      \n       itx-ank/ensemble\n      \n      . Additional AWS resources are created using terraform from the\n      \n       iac\n      \n      repository.\n     \n     \n      Once these prerequisites are done, and the configuration repository for the cluster has been updated accordingly, the installation is as follows.\n     \n     # Set these as needed for your cluster.\ncluster_domain=ensembletest.apps.jnj.com\nkubectl_context=itx-ank-ensemble-test\n\n# Clone this repository into the installation workstation.\ngit clone https://sourcecode.jnj.com/scm/asx-ahrx/datacoves_deployment.git\ncd datacoves_deployment\ngit submodule update --init\n\n# Reveal the secrets in the config submodule directory.\n(cd config/$cluster_domain; git secret reveal -f)\n\n# Install python dependencies for the installation scripts.\npip3 install --user -r requirements.txt\n\n# Install datacoves base dependencies into the cluster (ingress-nginx, etc.)\n./cli.py setup_base $kubectl_context $cluster_domain\n\n# Install datacoves.\n./cli.py install $kubectl_context $cluster_domain"},"18":{"url":"/docs_output/dev-logs/2022-05-setup-aks-postgres-flexible-server.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      Set up postgres flexible server on Azure\n     \n     \n      \n       Find it\n       \n        here\n       \n      \n      \n       Connect to it using this command:\n      \n     \n     psql -h datacoves-east-us.postgres.database.azure.com -U dcmaster -d postgres\n\n     \n      \n       Create the\n       \n        datacoves\n       \n       user that will be used by Django:\n      \n     \n     CREATE USER datacoves password '&lt;PASSWORD&gt;';\nALTER USER datacoves CREATEDB CREATEROLE;\nGRANT datacoves TO dcmaster;\nCREATE DATABASE datacoves OWNER datacoves;\nGRANT CONNECT ON DATABASE datacoves TO datacoves;\n\n     \n      \n       Dump data from internal Database\n      \n     \n     pg_dump -U postgres -h postgres-svc -d datacoves -Fc &gt; dump.sql\n\n     \n      \n       Restore data on new Azure DB\n      \n     \n     pg_restore -U datacoves -h datacoves-east-us.postgres.database.azure.com -d datacoves --no-owner --role=datacoves dump.sql\n\n     \n      \n       Repeate steps 4 and 5 with the rest of the services that need to be migrated\n      \n     \n     \n      Keep in mind that database objects owner could be changed, reassign the owner to the corresponding service account, i.e.:\n     \n     REASSIGN OWNED BY datacoves TO dev123_airbyte;\n\n     \n      If migrating\n      \n       temporal\n      \n      and\n      \n       temporal_visibility\n      \n      databases, you also need to update the database name on\n      \n       schema_versions\n      \n      .\n     \n     \n      \n       \n        Set\n        \n         airbyte_db_external: true\n        \n        ,\n        \n         airflow_db_external: true\n        \n        and\n        \n         superset_db_external: true\n        \n        accordingly\n       \n      \n      \n       \n        Configure\n        \n         postgres_db_provisioner\n        \n        using the master user connection/credentials"},"19":{"url":"/docs_output/dev-logs/","snip":"Edit on github"},"20":{"url":"/docs_output/how-tos/administrate-east-us-a-aks-cluster.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      Administrate east-us-a AKS cluster\n     \n     \n     \n     \n      Permissions\n     \n     \n      \n       Ask an administrator to create you a datacoves (microsoft) user. https://admin.microsoft.com.\n      \n      \n       Ask an administrator to add you to the\n       \n        DevOps\n       \n       \n        group\n       \n       .\n      \n     \n     \n     \n     \n      Configure kubectl\n     \n     \n      \n       Download Azure CLI\n      \n      .\n     \n     \n      Login to your account:\n     \n     az login\n\n     \n      Then, run the following commands:\n     \n     az account set --subscription 91bd2205-0d74-42c9-86ad-41cca1b4822b\naz aks get-credentials --resource-group datacoves --name east-us-a\n\n     \n      This will add a new context to\n      \n       kubectl\n      \n      , so you can now run:\n     \n     kubectl get pods -A\n\n     \n     \n     \n      Manage nodepools\n     \n     \n     \n     \n      List nodepools\n     \n     \n      List nodepools in the\n      \n       datacoves\n      \n      resource group,\n      \n       east-us-a\n      \n      cluster:\n     \n     az aks nodepool list --cluster-name east-us-a --resource-group datacoves\n\n     \n     \n     \n      Add workers nodepool\n     \n      az aks nodepool add --cluster-name east-us-a --resource-group datacoves --name workerslarge --mode User --enable-cluster-autoscaler --min-count 1 --max-count 10 --node-vm-size Standard_D4s_v3 --labels k8s.datacoves.com/workers=enabled\n\n     \n     \n     \n      Modify existing nodepool to add new labels\n     \n     \n      Let's add a new label\n      \n       k8s.datacoves.com/workers=enabled\n      \n      to an existing nodepool which already has the label\n      \n       k8s.datacoves.com/nodegroup-kind=general\n      \n      . Old a new labels need to be specified.\n     \n     az aks nodepool update --cluster-name east-us-a --resource-group datacoves --name generallarge --labels {k8s.datacoves.com/workers=enabled,k8s.datacoves.com/nodegroup-kind=general}"},"21":{"url":"/docs_output/how-tos/airflow-configuration.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      Environment variables override\n     \n     \n      Airflow has a feature that lets you override system's defaults on a per-task basis (see https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/executor/kubernetes.html#pod-override).\n     \n     \n      \n       Example \"Log level override\"\n      \n      :\n     \n     \"pod_override\": k8s.V1Pod(\n    spec=k8s.V1PodSpec(\n        containers=[\n            k8s.V1Container(\n                name=\"base\",\n                image=f\"{IMAGE_REPO}:{IMAGE_TAG}\",\n                env=[\n                    k8s.V1EnvVar(\n                        name=\"AIRFLOW__LOGGING__LOGGING_LEVEL\",\n                        value=\"DEBUG\"\n                    )\n                ]\n            )\n        ]\n    )\n),"},"22":{"url":"/docs_output/how-tos/billing-system.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      Datacoves Billing System\n     \n     \n      This document provides comprehensive information on Datacoves’ billing integration with Stripe.\n     \n     \n     \n     \n      Introduction\n     \n     \n      Datacoves integrates with Stripe to manage billing by listening to Stripe events and adjusting Datacoves data accordingly. The system also modifies subscriptions when changes occur in services, users, or tally marks.\n     \n     \n      The connection between Datacoves and Stripe begins when a user creates a subscription through the Setup Wizard, or when a Datacoves Admin sets up a subscription directly in the Stripe UI.\n     \n     \n      \n       Note\n      \n      : Free trial accounts\n      \n       are not connected\n      \n      to Stripe.\n     \n     \n     \n     \n      Account Setup Wizard\n     \n     \n      \n     \n     \n     \n     \n      Customer Types\n     \n     \n      For billing, we distinguish between three customer types:\n     \n     \n      \n       Free trial customers\n      \n      \n       Credit card customers\n      \n      \n       Check / bank transfer customers\n      \n     \n     \n     \n     \n      Free trial customers\n     \n     \n      These type of customers are not connected to stripe while they're on trial. During the trial period, Stripe does not have information about these accounts.\n     \n     \n      Free trial customers will see a button on the header inviting them to finalize the trial and create a subscription. Upon subscribing, they transition to\n      \n       credit card customers\n      \n      .\n     \n     \n     \n     \n      Credit card customers\n     \n     \n      Credit card customers workflow is completely managed by Datacoves:\n     \n     \n      \n       \n        Customer selects\n        \n         Growth Plan\n        \n        and after clicking on\n        \n         Next\n        \n        Datacoves creates the stripe customer, sets the\n        \n         customer_id\n        \n        and redirects them to Stripe billing page where the stripe billing process begins.\n       \n      \n      \n       \n        Once the customer enters their credit card and completes the Stripe billing process, Datacoves receives a notification and sets the\n        \n         subscription\n        \n        payload on the brand new account.\n       \n      \n      \n       \n        From this point, any updates to services/users/tally marks in the Datacoves account are automatically reflected in Stripe, adjusting invoices accordingly.\n       \n      \n     \n     \n     \n     \n      Check / bank customers\n     \n     \n      For customers preferring bank transfers or checks, setup is managed manually through the Stripe UI.\n     \n     \n     \n     \n      Customer Setup\n     \n     \n      \n       Customer creates a Trial account as described earlier.\n      \n      \n       An Stripe Admin\n       \n        manually creates the customer\n       \n       using the Stripe UI. In order to follow the same convention used by Datacoves, please use account's\n       \n        slug\n       \n       as the stripe customer name, and account's\n       \n        owner email\n       \n       as the stripe customer email. Add an address to calculate taxes automatically.\n      \n      \n       Once you got a customer id on stripe, modify the Datacoves account on the admin panel and set it on the\n       \n        Customer Id\n       \n       field.\n      \n      \n       Modify the Datacoves account once more and set the right\n       \n        plan\n       \n       and\n       \n        variant\n       \n       . We typically use\n       \n        growth\n       \n       plan for these accounts, the\n       \n        variant\n       \n       will be determined by Sales depending on the pricing negotiated.\n      \n     \n     \n     \n     \n      Subscription Setup\n     \n     \n      The Stripe Admin now\n      \n       creates a subscription on Stripe\n      \n      for the recently created customer. Please be cautious with the products included in the subscription, they should match exactly the products included in the plan. You can inspect them\n      \n       here\n      \n      .\n     \n     \n      You don't need to add the metered products on a new subscription, Datacoves will modify the subscription and add them later. Unless the customer prepaid for developer seats and services, you include the developers seat product specifying the total user licenes and then one product line per service (airbyte, airflow, superset, datahub).\nIn the following example, there are 5 user licences, 1 Airbyte, 1 Airflow and 1 Superset server:\n     \n     \n      \n     \n     \n      NOTE: Certain customers (like Guitar Center) could opt to prepay the developer seats and services costs via Bank transfer / check. In those cases, you only include the metered products in the subscription.\n     \n     \n      Don't forget to set the right plan on the subscription metadata, it's usually\n      \n       growth-monthly\n      \n      , if you need a different one, type the\n      \n       slug\n      \n      field of the chosen one.\n     \n     \n      \n     \n     \n      On Payment, select\n      \n       Email invoice to the customer to pay manually\n      \n      and uncheck\n      \n       Include a stripe-hosted link to an invoice payment page in the invoice\n      \n      .\n     \n     \n      Finalize by clicking on\n      \n       Create subscription\n      \n      .\n     \n     \n      Go to the Django admin panel and check that the account has a\n      \n       JSON\n      \n      dict on the field\n      \n       subscription\n      \n      . If it does, it means the connection is set, you can now finalize the trial by setting a past end date in the\n      \n       Trial ends at\n      \n      field (or by just removing trial start and end dates).\n     \n     \n     \n     \n      Add credit to customer\n     \n     \n      Once the subscription was created, the customer will start generating a debt.\nAs soon as Datacoves receives a check or wire, a Stripe Admin needs to register it on the Django Admin, as follows:\nNote that credits have a validity period, during that period the developer licences or services specified will be discounted from the invoice.\n     \n     \n      \n       Go to Accounts, select the customer's account and edit it.\n      \n      \n       Scroll down until you see the\n       \n        Credits\n       \n       area.\n      \n      \n       Click on\n       \n        Add another credit\n       \n       and complete the required fields including as much information as possible in the reference field.\n      \n      \n       Click on\n       \n        Save\n       \n       .\n      \n     \n     \n      \n     \n     \n     \n     \n      F.A.Q.\n     \n     \n     \n     \n      How do I configure my local environment to test Stripe?\n     \n     \n      First of all, you need to set to\n      \n       True\n      \n      the feature\n      \n       accounts_signup\n      \n      on the only record you have in the\n      \n       Cluster\n      \n      model.\n     \n     \n      Then, if you're using\n      \n       datacoveslocal.com\n      \n      and you were granted permissions automatically to the\n      \n       local\n      \n      account, you need \nto remove all the permissions to such account, doing that the Datacoves UI will allow you creating a new account using the\nsetup wizard.\n     \n     \n      You should also set\n      \n       setup enabled\n      \n      on\n      \n       True\n      \n      on the admin panel for you user.\n     \n     \n      Then, navigate to https://datacoveslocal.com/account-setup/ and follow the instructions to create an account using Stripe.\n     \n     \n     \n     \n      How do I run the stripe webhooks locally to test billing integration?\n     \n     \n      Run\n      \n       ./cli.py stripe_webhooks\n      \n      and follow the instructions.\n     \n     \n     \n     \n      How to sync stripe live products with test products?\n     \n     \n      Sometimes you modified the live products (prices/descriptions) and you need to update the test ones.\n     \n     \n      Just run\n      \n       ./cli.py copy_to_stripe_test\n      \n      to run the live -&gt; test sync process."},"23":{"url":"/docs_output/how-tos/celery-monitoring.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      Celery monitoring\n     \n     \n      For authoritative, more detailed information, see\n      \n       celery's monitoring guide\n      \n      .\n     \n     \n     \n     \n      UI\n     \n     \n      We run the flower UI at\n      \n       https://flower.{cluster_domain}\n      \n      . You can see executed\ntasks by clicking on tasks, or navigating to\n      \n       https://flower.{cluster_domain}/tasks\n      \n      .\nYou'll want to sort tasks to see the latest Started or Received at the top.\nYou can filter by task using the Search input. The UI doesn't refresh live.\nIncreasing the number of shown entries can be helpful.\n     \n     \n     \n     \n      CLI\n     \n     \n      From a core-api pod (\n      \n       kcc exec -it $api_pod_name -- bash\n      \n      ) you can invoke\ncelery inspect. One useful thing to do is check the stats.\n     \n     celery -A datacoves inspect stats\n\n     \n      Here's an excerpt from the output.\n     \n     ...\n        \"total\": {\n            \"billing.tasks.inform_billing_events\": 113,\n            \"billing.tasks.tally_account_resource_usage\": 1,\n            \"billing.tasks.tally_resource_usage\": 1,\n            \"celery.backend_cleanup\": 1,\n            \"clusters.workspace.sync_task\": 1211,\n            \"iam.tasks.clear_tokens\": 1,\n            \"iam.tasks.remove_missing_user_groups\": 1,\n            \"notifications.tasks.send_slack_notification\": 7,\n            \"projects.tasks.delete_unused_project_keys\": 1,\n            \"projects.tasks.remove_unused_environments\": 1,\n            \"projects.tasks.remove_unused_user_volumes\": 1,\n            \"projects.tasks.stop_sharing_codeservers\": 38,\n            \"projects.tasks.turn_off_unused_workspaces\": 1134\n        },\n        \"uptime\": 68132\n...\n\n     \n      The uptime is 68132 seconds, and the sync_task has run 1211 times, so there's\nbeen one run every 56 seconds in average."},"24":{"url":"/docs_output/how-tos/choose-ec2-nodes.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      Choosing an ec2 instance type and estimating pods per node\n     \n     \n      \n       AWS docs.\n      \n     \n     \n     \n     \n      Pod limit from network constraints\n     \n     \n      Every pod must have an IP. EC2 instances have a maximum number of IPs, which\nlimits the number of pods per node.\n      \n       source\n      \n     \n     \n      With CNI version 1.9 or higher and nitro instances,\n      \n       the pod limit can be increased\n      \n      .\nFor example:\n     \n      $ ./max-pods-calculator.sh --instance-type m5.large --cni-version 1.9.0\n29\n\n $ ./max-pods-calculator.sh --instance-type m5.large --cni-version 1.9.0 --cni-prefix-delegation-enabled\n110\n\n# For ensembledev.apps.jnj.com:\n$ ./max-pods-calculator.sh --instance-type m5.4xlarge --cni-version 1.7.1\n110\n\n     \n      \n       List of ENI and IP limits per instance type\n      \n      .\n     \n     \n     \n     \n      Pod limit from volume attachment limits\n     \n     \n      Currently some of our pods (code-server) require ELB volumes. EC2 instances have\na maximum number of volumes that can be attached. For \"most\" nitro instances, the\nsum of ENIs, volume attachments and instance store volumes must be less than 28.\n      \n       source\n      \n      . Volume attachments seem capped by 26 because the mount\npoints use the a letter of the alphabet each."},"25":{"url":"/docs_output/how-tos/codeserver-images.html","snip":"Edit on github\n       \n      \n     \n     \n      Check the versions, but these are the standard Datacoves VS Code extensions:\n     \n     \n      SQLFluff is a SQL linter with dbt support\nhttps://datacoves-vs-code-images.s3.amazonaws.com/dorzey.vscode-sqlfluff-3.2.0.vsix\n     \n     \n      This extensions adds grid (excel like) editing for CSV files\nhttps://datacoves-vs-code-images.s3.amazonaws.com/janisdd.vscode-edit-csv-0.10.0.vsix\n     \n     \n      Standard VS Code Python extension\nhttps://datacoves-vs-code-images.s3.amazonaws.com/ms-python.python-2024.14.1.vsix\n     \n     \n      This adds yml validations\nhttps://datacoves-vs-code-images.s3.amazonaws.com/redhat.vscode-yaml-1.15.0.vsix\n     \n     \n      This adds \"short cuts\" to VS Code like the \"run current model\" and \"more..\" button\nhttps://datacoves-vs-code-images.s3.amazonaws.com/RobertOstermann.better-status-bar-1.0.9.vsix\n     \n     \n      This adds Jinja support, I think it is dbt-jinja\nhttps://datacoves-vs-code-images.s3.amazonaws.com/samuelcolvin.jinjahtml-0.20.0.vsix\n     \n     \n      This adds items to the file context menu like \"Duplicate\"\nhttps://datacoves-vs-code-images.s3.amazonaws.com/sleistner.vscode-fileutils-3.10.3.vsix\n     \n     \n      This adds spell checking\nhttps://datacoves-vs-code-images.s3.amazonaws.com/streetsidesoftware.code-spell-checker-3.0.1.vsix\n     \n     \n      This is our Power User Extension that adds things like query preview and near real time linting\nhttps://datacoves-vs-code-images.s3.amazonaws.com/vscode-datacoves-power-user-0.9.16.vsix\n     \n     \n      Python Ruff linter, main use case is to show vars and imports not being used in a .py file\nhttps://datacoves-vs-code-images.s3.amazonaws.com/charliermarsh.ruff-2024.56.0.vsix\n     \n     \n      This adds colors to each column of a CSV file\nhttps://datacoves-vs-code-images.s3.amazonaws.com/mechatroner.rainbow-csv-3.3.0.vsix\n     \n     \n      This is part of the Datacoves install for Snowflake Envs\nhttps://datacoves-vs-code-images.s3.amazonaws.com/snowflake.snowflake-vsc-1.10.5.vsix\n     \n     \n      SQLTools I cant find where you got this from on github and it no longer in Orrum since I deleted it.\nIt is used on non-Snowflake envs like Artemis\n     \n     \n      This is a chat gpt extension that is NOT our default, but has been added in a few places, like orrum and cold bore. Datacoves co-pilot will make this obsolete\nhttps://datacoves-vs-code-images.s3.amazonaws.com/timkmecl.chatgpt-1.1.2.vsix"},"26":{"url":"/docs_output/how-tos/connect-to-kenvue-cluster-using-a-bastion.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      How to connect to kenvue cluster using a bastion\n     \n     \n     \n     \n      SSH to bastion\n     \n     \n      ssh\n      \n       @AWSWEXNVAL0001.kenvue.com\n      \n     \n     \n     \n     \n      Set up your user enviornment\n     \n     \n      Install kubectl and aws-iam-authenticator\n     \n     mkdir bin\ncd bin\ncurl -Lo aws-iam-authenticator https://github.com/kubernetes-sigs/aws-iam-authenticator/releases/download/v0.5.9/aws-iam-authenticator_0.5.9_linux_amd64\nchmod +x aws-iam-authenticator\n\ncd ..\ncurl -Lo kuberlr.tar.gz https://github.com/flavio/kuberlr/releases/download/v0.4.2/kuberlr_0.4.2_linux_amd64.tar.gz\ntar -xzvf kuberlr.tar.gz\n\ncd kuberlr_0.4.2_linux_amd64/\nmv kuberlr ../bin/\ncd ../bin\nln -s kuberlr kubectl\ncd ..\n\n     \n     \n     \n      Configure your ~/.kube/config\n     \n     mkdir .kube\ncat &lt;&lt; EoF &gt; .kube/config2\napiVersion: v1\nclusters:\n- cluster:\n    server: https://BD0F1A58014FCF446B668A876EE7DF2A.gr7.us-east-1.eks.amazonaws.com\n    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM1ekNDQWMrZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJeU1UQXlOVEV4TlRNMU1Gb1hEVE15TVRBeU1qRXhOVE0xTUZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBT2JpCmFhOUFvSDVlWGpMeFdnQzBONE5JUHVQSVptNmpLNmxBM29sTVAwUHYyd1hlalphcEFsVnFOWVdxcHl3aCtZZm8KT1lLR1Nuc2hPdE9DbnVyU094SVhoY1BnR1ZmN1REVlZGbU04WW5KSzBmOHdLWmxLdDNIYU9oWFJkekNZYkJoMgoydnpZSGx0ZGREbHkvTHpwaWpNQlpNRHY1UUtkeEhNSEF0aUd6aG4xS2xvT2xkRGozV1lpV1VJV0ladzZheWV2CnNhYm1Rd3A1REJwQjBVN3V2bEdMd1RUQ3RZc3NhdnI2dDZ6MWtzNHhNUUMxVTlONUlHV0UxdEUrZGZwMmZzWDYKZ3d1c0tEOGNESkFiVmFrL2lwK3pkcXRxRnJHOVFNeDBEelpQYzRtU1dnVDZyVXZjbTlBbTlrMVNsSXc5ODlGRApHelh6bGxQcXZySWNnU1RWSW9jQ0F3RUFBYU5DTUVBd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZLNnJEeXBRK3VReGgxWU8zS0JKbmthYU1TNUdNQTBHQ1NxR1NJYjMKRFFFQkN3VUFBNElCQVFCdk52clZjRjFaZ1FDMzNpbDZrR0gzcHJJN3RWRmcvOTF3UVNZZkM2SFM2cWRiVERucwpNYXhoeEYvblZzbFEyKzRmN0UxVUZodUdsOUdUZlVvS2FiQzB1cWx6bUpQaDJVUXJRZ3hZQnd3eGxTOSszcHJNCnlUOGZ5M29uM21jaWR0azZlSllIcm5wZS9QZnlWN1J5eUhva0pVVGIwcWFVakxoMVZHVFoyRmJLK0ZjeG50SHcKdWJ4bnlSMHZlcGExdDFoOVljNDFJYnFzUGRBMVFDZVYvR1hNdWN4Z0U4bUd1VFZQQlU1MEdYbG1qWnRZVjg5dgp3TVpYTVVobzNmakdQNVVnMnlFTmtXaW9Ra2hqUkRMRUZGQXpZUzMrSU5TWnAwMklBUTRRNkNSYnJ0Vmc5ZDFrCkY4d1FzaytJUXUrMnE3T25WOUs5cUdYeXdrakNSd0ZTV1N2UwotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==\n  name: kubernetes\ncontexts:\n- context:\n    cluster: kubernetes\n    user: aws\n  name: aws\n- context:\n    cluster: kubernetes\n    user: aduser\n  name: user\ncurrent-context: aws\nkind: Config\npreferences: {}\nusers:\n- name: aws\n  user:\n    exec:\n      apiVersion: client.authentication.k8s.io/v1beta1\n      command: aws-iam-authenticator\n      args:\n        - \"token\"\n        - \"-i\"\n        - \"itx-wcr-datacove-development\"\n        - \"-r\"\n        - \"arn:aws:iam::551241293703:role/itx/service/EKS/VPCxEKSRole\"\n- name: aduser\n  user:\n    auth-provider:\n      config:\n        apiserver-id: \"22f9d484-b818-4b21-a278-00b264446505\"\n        client-id: \"22f9d484-b818-4b21-a278-00b264446505\"\n        environment: AzurePublicCloud\n        tenant-id: \"7ba64ac2-8a2b-417e-9b8f-fcf8238f2a56\"\n      name: azure\nEoF\n\n     \n     \n     \n      Connect to cluster\n     \n     kubectl get nodes"},"27":{"url":"/docs_output/how-tos/custom-dns.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      About this Documentation\n     \n     \n      Some customers (like Orrum) require a custom internal DNS.  This will require adding a new coredns custom config map:\n     \n     apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: coredns-custom\n  namespace: kube-system\ndata:\n  sftp.orrum.com.server: |\n    sftp.orrum.com:53 {\n        forward . 172.31.150.10 172.31.160.20\n    }\n\n     \n      Change 'sftp.orrum.com' to whatever pattern needs to go to the custom DNS, and the IP addresses to the addresses of the DNS servers to resolve the address.\n     \n     \n      Then you can patch the coredns deployment:\n     \n     kubectl -n kube-system patch deployment coredns \\\n  --type='json' \\\n  -p='[\n    {\n      \"op\": \"add\",\n      \"path\": \"/spec/template/spec/volumes/-\",\n      \"value\": {\n        \"name\": \"custom-coredns\",\n        \"configMap\": {\n          \"name\": \"coredns-custom\"\n        }\n      }\n    },\n    {\n      \"op\": \"add\",\n      \"path\": \"/spec/template/spec/containers/0/volumeMounts/-\",\n      \"value\": {\n        \"name\": \"custom-coredns.server\",\n        \"mountPath\": \"/etc/coredns/custom\"\n      }\n    }\n  ]'\n\n     \n      Then restarts the deployment:\n     \n     kubectl rollout restart deployment coredns -n kube-system\n\n     \n      And test with nslookup:\n     \n     kubectl -n core exec -ti workbench-c6599969b-k4p5w -- nslookup sftp.orrum.com"},"28":{"url":"/docs_output/how-tos/datacoves-versioning.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      Datacoves versioning\n     \n     \n      We use\n      \n       semantic versioning\n      \n      in all our images, and datacoves releases.\n     \n     \n      \n       MAJOR.MINOR.PATCH\n      \n     \n     \n      where\n      \n       MAJOR.MINOR\n      \n      are read from\n      \n       .version.yaml\n      \n      and used every time a new image is\npushed to docker repository and\n      \n       PATCH\n      \n      is autogenerated (timestamp).\n     \n     \n     \n     \n      Our criteria\n     \n     \n     \n     \n      When do we bump the\n      \n       MAJOR\n      \n      version?\n     \n     \n      When we make incompatible changes or we introduce compatible changes but deprecate features:\n     \n     \n      \n       Any python library upgrade (including dbt) that requires changes in the customer's analytics(dbt) git repo\n      \n      \n       Airbyte, Airflow, DataHub, Superset upgrades that require reconfiguration\n      \n      \n       Datacoves core changes that require human intervention\n      \n      \n       Airbyte, Airflow, DataHub, Superset that do not require reconfiguration, but several features are being deprecated\n      \n     \n     \n     \n     \n      When should we bump the\n      \n       MINOR\n      \n      version?\n     \n     \n      \n       When we make compatible changes, such as new features or upgrade dependencies\n      \n      \n       Patch version changes to dbt e.g. 1.8.3 to 1.8.5\n      \n      \n       Compatible updates to dbt e.g. 1.7.x to 1.8.x\n      \n      \n       Compatible update to Airbyte, Airflow, DataHub, Superset that do not require reconfiguration\n      \n     \n     \n     \n     \n      Everything else is a\n      \n       PATCH\n      \n     \n     \n      \n       Bug fixes, performance enhancements\n      \n     \n     \n     \n     \n      Images tags\n     \n     \n      Images are pushed with the folling tags:\n     \n     \n      \n       MAJOR\n      \n      \n       MAJOR.MINOR\n      \n      \n       MAJOR.MINOR.PATCH\n      \n      \n       MAJOR.MINOR.PATCH-\\&lt;commit sha&gt;\n      \n      \n       latest\n      \n     \n     \n      CI servers that eventually use datacoves images could reference any of them, depending on how specific they need to be.\n     \n     \n     \n     \n      Releases\n     \n     \n      Releases follow the same versioning criteria, they are generated by running the\n      \n       ./cli.py generate_release\n      \n      command, or by triggering the\n      \n       Generate Release\n      \n      GitHub workflow."},"29":{"url":"/docs_output/how-tos/debug-airflow-workers.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      Debug Airflow Workers\n     \n     \n     \n     \n      How to review if there are error in git-sync/s3-sync containers?\n     \n     \n      We have already enabled the functionality in\n      \n       git-sync\n      \n      to retry a maximum of three times. If the synchronization with\n      \n       git-sync\n      \n      or\n      \n       s3-sync\n      \n      is not successful, the worker will fail, therefore the Airflow task will also fail.\n     \n     \n      To get the logs from\n      \n       git-sync\n      \n      or\n      \n       s3-sync\n      \n      we need to filter by namespace and by container. Below are some examples of how to do it.\n     \n     \n      \n       Go to\n       \n        Grafana\n       \n       i.g.\n       \n        https://grafana.&lt;domain&gt;\n       \n      \n      \n       \n        Go to\n        \n         Explore\n        \n        select\n        \n         Loki\n        \n        datasource and perform the query with the following filters:\n       \n      \n      \n       \n        \n         Namespace\n        \n        =\n        \n         dcw-my-slug-environment\n        \n       \n      \n      \n       \n        Container\n       \n       =~\n       \n        git-sync\n       \n       \n        /\n       \n       s3-sync**/\n      \n     \n     \n      Examples:\n     \n     # git-sync\n{namespace=\"dcw-dnr240\", container=\"git-sync\"} |= ``  \n\n# s3-sync\n{namespace=\"dcw-dnr240\", container=\"s3-sync\"} |= ``  \n\n     \n      \n     \n     \n     \n     \n      How to get Airflow workers?\n     \n     \n      \n       Go to\n       \n        Grafana\n       \n       i.g.\n       \n        https://grafana.&lt;domain&gt;\n       \n      \n      \n       \n        Go to\n        \n         Explore\n        \n        select\n        \n         Loki\n        \n        datasource and perform the query with the following filters:\n       \n      \n      \n       \n        \n         Namespace\n        \n        =\n        \n         dcw-my-slug-environment\n        \n       \n      \n      \n       \n        Pod\n       \n       =~\n       \n        my-slug-environmet-airflow-scheduler.\n       \n       *\n      \n      \n       \n        Line contains\n       \n       |=\n       \n        my-task\n       \n      \n     \n     \n      \n       Note: Remember that you have to adjust the date and time parameters depending on the search you want to perform.\n      \n     \n     \n      E.g.:\n     \n     {namespace=\"dcw-prd001\", pod=~\"prd001-airflow-scheduler.*\"} |= `t_id_MDM_extraction_V_ENS2_SALES_ADJUSTMENTS_streamsets`\n\n     \n      \n     \n     \n      \n       Copy the pod name\n      \n      \n       \n        Go to\n        \n         Explore\n        \n        select\n        \n         Loki\n        \n        and perform the query with the following filters:\n       \n      \n      \n       \n        \n         Namespace\n        \n        =\n        \n         dcw-my-slug-environment\n        \n       \n      \n      \n       \n        Pod\n       \n       =\n       \n        pod-name\n       \n      \n     \n     \n      E.g.:\n     \n     {namespace=\"dcw-prd001\", pod=\"emeaelmdmprdtidmdmextractionve-295567f106ff46139ad4edf24e52fc31\"} |= ``"},"30":{"url":"/docs_output/how-tos/debug-dbt-errors-when-return-code-is-not-zero.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      How to debug dbt on production environments, i.e. Airflow?\n     \n     \n      Sometimes when you run a dbt command on the command line, i.e.\n      \n       dbt deps\n      \n      ,\n      \n       dbt compile\n      \n      , there are silent errors, and you just got an errorcode &gt; 0.\n     \n     \n      To debug it, you should run it programatically using python:\n     \n     \n     \n     \n      Run python in the command line\n     \n     $ python\n\n     \n     \n     \n      Run the desired command right in the python console\n     \n     from dbt.cli.main import dbtRunner, dbtRunnerResult\n\n# initialize\ndbt = dbtRunner()\n\n# create CLI args as a list of strings\ncli_args = [\"deps\"]\n\n# run the command\nres: dbtRunnerResult = dbt.invoke(cli_args)\n\n# inspect the results\nfor r in res.result:\n    print(f\"{r.node.name}: {r.status}\")\n\n     \n      To know more, see https://docs.getdbt.com/reference/programmatic-invocations."},"31":{"url":"/docs_output/how-tos/grafana-grant-permisions.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      Grant permissions to Grafana\n     \n     \n      \n       Go to\n       \n        Django admin groups\n       \n       .\n      \n      \n       Edit a group that has your user.\n      \n      \n       Search\n       \n        Grafana\n       \n       permissions and\n       \n        Choose all\n       \n       (See image).\n      \n      \n       Save the group.\n      \n      \n       Go to\n       \n        Grafana"},"32":{"url":"/docs_output/how-tos/grafana-loki-storage-config-providers.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      Grafana Loki Storage\n     \n     \n     \n     \n      Providers\n     \n     \n      \n       \n        AWS S3\n       \n      \n      \n       \n        Azure Blob Storage\n       \n      \n     \n     \n     \n     \n      AWS S3\n     \n     \n     \n     \n      Permission\n     \n     \n      Limited: List, Read, Write\n     \n     {\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:PutObject\",\n        \"s3:GetObject\",\n        \"s3:GetObjectVersion\",\n        \"s3:DeleteObject\",\n        \"s3:DeleteObjectVersion\"\n      ],\n      \"Resource\": \".../*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": \"s3:ListBucket\",\n      \"Resource\": \"...\"\n    }\n  ]\n}\n\n     \n     \n     \n      Create and Configure Life Cycle\n     \n     \n      \n       Find and select\n       \n        S3 Services\n       \n       .\n      \n      \n       Click on\n       \n        Create Bucket\n       \n       .\n      \n      \n       \n        General configuration\n       \n       you must choose the region y the name of bucket. Other values can be defaulted.\n      \n      \n       We need to create two\n       \n        Lifecycle rules\n       \n       to rotate our logs.\n      \n      \n       Select the new Bucket and then select\n       \n        Management\n       \n       tab.\n      \n      \n       Click\n       \n        Create lifecycle rule\n       \n       .\n      \n      \n       \n        Lifecycle rule configuration\n       \n       you have to fill in the name: e.g:\n       \n        Delete all fake objects after 30 days\n       \n       .\n      \n      \n       \n        Prefix\n       \n       you fill in\n       \n        fake/\n       \n      \n      \n       \n        Lifecycle rule actions\n       \n       you has to check\n       \n        Expire current versions of objects\n       \n       ,\n      \n      \n       \n        Days after object creation\n       \n       you must complete\n       \n        30\n       \n      \n      \n       Save changes.\n      \n      \n       \n        Lifecycle rule configuration\n       \n       you have to fill in the name: e.g:\n       \n        Delete all index objects after 30 days\n       \n      \n      \n       \n        Prefix\n       \n       you fill in\n       \n        index/\n       \n      \n      \n       \n        Lifecycle rule actions\n       \n       you has to check\n       \n        Expire current versions of objects\n       \n       ,\n      \n      \n       \n        Days after object creation\n       \n       you must complete\n       \n        30\n       \n      \n      \n       Save changes.\n      \n     \n     \n      \n       We must have two rules.\n      \n     \n     \n      \n     \n     \n      \n       Example of a rule.\n      \n     \n     \n      \n     \n     \n      \n     \n     \n     \n     \n      Azure Blob Storage\n     \n     \n     \n     \n      Create and configure Azure Blob Storage\n     \n     \n      \n       Create new resource\n       \n        Storage account\n       \n       .\n      \n      \n       Select your\n       \n        Subscription\n       \n       and\n       \n        Resource group\n       \n       .\n      \n      \n       Complete the\n       \n        Storage account name\n       \n       .\n      \n      \n       Click\n       \n        Review\n       \n       (Other values can be defaulted).\n      \n      \n       Click\n       \n        Create\n       \n       (Other values can be defaulted).\n      \n     \n     \n      \n     \n     \n      \n       Select your new\n       \n        Storage account\n       \n       .\n      \n      \n       Click on\n       \n        Containers\n       \n       and add new container.\n      \n     \n     \n      \n     \n     \n      \n     \n     \n      \n       Select\n       \n        Lifecycle management\n       \n       and\n       \n        Add a rule\n       \n       to create new rule to rotate our logs.\n      \n     \n     \n      \n     \n     \n      \n       On\n       \n        Details\n       \n       tab we must complete the name (Delete all objects after 30 days) and select\n       \n        Limit blobs with filter\n       \n       .\n      \n     \n     \n      \n     \n     \n      \n       On\n       \n        Filter set\n       \n       tab we must add two\n       \n        Blob prefix\n       \n       :\n       \n        &lt;container-name&gt;/fake/\n       \n       and\n       \n        &lt;container-name&gt;/index/\n       \n       .\n      \n     \n     \n      \n     \n     \n      \n       Click\n       \n        Create\n       \n      \n     \n     \n     \n     \n      Get configuration data\n     \n     \n      \n       \n        Account name\n       \n       is the name of\n       \n        storage account\n       \n       .\n      \n      \n       Click on\n       \n        Account key\n       \n       (Key1)\n      \n     \n     \n      \n     \n     \n      \n       Select your\n       \n        Container\n       \n       and then\n       \n        Properties"},"33":{"url":"/docs_output/how-tos/grafana-loki-storage-config.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      Grafana Loki Storage Configuration\n     \n     \n      There are three different providers to configure\n      \n       Loki\n      \n      storage:\n     \n     \n      \n       AWS S3\n      \n      \n       Azure Blob Storage\n      \n      \n       Minio (Local development)\n      \n     \n     \n     \n     \n      Notes\n     \n     \n      \n       Minio is not responsible for log rotation, the logs lifecycle must be configured in your provider.\n      \n      \n       How to configure the provider?\n       \n        here\n       \n      \n     \n     \n      To configure the cluster you must add the configuration to the configuration repository as a secret in\n      \n       &lt;domain&gt;/cluster-params.secret.yaml\n      \n      for example to our local environment\n      \n       datacoveslocal.com/cluster-params.secret.yaml\n      \n     \n     \n     \n     \n      Minio (Local development)\n     \n     grafana:\n  ...\n  loki:\n    provider: minio\n    password: ...\n\n     \n     \n     \n      AWS S3\n     \n     grafana:\n  ...\n  loki:\n    provider: aws\n    region: &lt;us-east-1&gt;\n    access_key: ...\n    secret_key: ...\n    bucket: &lt;bucket-name&gt;\n\n     \n     \n     \n      Azure Blob Storage\n     \n     grafana:\n  ...\n  loki:\n    provider: azure\n    account_name: ...\n    account_key: ...\n    container_name: &lt;container-name&gt;\n    endpoint_suffix: &lt;blob.core.windows.net&gt;"},"34":{"url":"/docs_output/how-tos/hotfix.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      How to Create a Hotfix\n     \n     \n      A hotfix is defined as doing a targetted fix to an existing release.  The idea behind a hotfix is to do the absolute minimum change to correct a high priority issue in a live release.\n     \n     \n      To create a hotfix, one must first do the fix.  First, create a branch from the release tag you wish to hot fix.  Let's say you're hot-fixing release 'TAG_NAME'.  You would do the following commands:\n     \n     git fetch --all --tags\ngit checkout -b BRANCH_NAME refs/tags/TAG_NAME\n\n     \n      You will now have a branch that is a copy of the release tag.  You can either do your hotfix work directly on that branch and merge it to main later, or you can use\n      \n       git cherry-pick\n      \n      to pick commits from the main branch onto your new branch.  If you need to use cherrypick and you don't know how, that is a larger topic than I want to cover here; Stephen can help you directly with that.\n     \n     \n      Once you have done your work, you should\n      \n       commit\n      \n      to your branch and then compare your branch to the original tag.  This will make sure you only changed what was needed:\n     \n     git diff BRANCH_NAME..refs/tags/TAG_NAME\n\n     \n      This command\n      \n       is very important if you cherry-pick\n      \n      to make sure you don't accidentally bring additional features or code that you do not intend to.  However, it is good practice to review all code going into a hotfix very carefully.\n     \n     \n      Once you are certain your hotfix is good,\n      \n       push\n      \n      it to the git repository.  Now you're ready to build a hotfix release with cli.py.  Do the following command:\n     \n     ./cli.py generate_hotfix\n\n     \n      It will first show you\n      \n       git status\n      \n      to make sure your code is committed.  Make sure there are no extra files or anything you don't want built into the release docker image present in your code tree.\n     \n     \n      After you confirm, it will ask you which release you are making a hotfix from.  This release must already be present in your\n      \n       releases/\n      \n      directory; if it is not, download the release with\n      \n       ./cli.py download_releases\n      \n      or download the appropriate manifest directly from github.\n     \n     \n      Then, it will ask you which images you wish to build.  Select one or more images to build, or none if you are changing another dependency.\n     \n     \n      After that, it will ask you if you want to change the version of any other image that is in the release.  You can select none if you only want to build new images and you don't need to change any other dependencies.\n     \n     \n      Finally, it will build your release and push it up as a draft in github.  From that point, it is a normal release and you can take it through the normal process to get it installed."},"35":{"url":"/docs_output/how-tos/how-to-create-a-ssl-certificate.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      How to create an SSL certificate\n     \n     \n      \n       \n        Install\n        \n         acme.sh\n        \n       \n      \n      \n       \n        Configure the\n        \n         cloudflare API token\n        \n        (getting\n        \n         CF_Key\n        \n        and\n        \n         CF_Email\n        \n        from 1Password).\n       \n      \n      \n       \n        Run:\n       \n      \n     \n     # Let's Encrypt issuer\n# https://github.com/acmesh-official/acme.sh/wiki/Server\nacme.sh --issue --server letsencrypt --dns dns_cf -d &lt;DOMAIN&gt; --debug 2\n\n# then\nacme.sh --issue --server letsencrypt --dns dns_cf -d '*.&lt;DOMAIN&gt;' --debug 2\n\n     \n      \n       Get certificate information (Optional)\n      \n     \n     openssl x509 -text -noout -in &lt;cert&gt;\n\n     \n      \n       \n        Copy ceftificates\n       \n      \n      \n       \n        Use\n        \n         &lt;DOMAIN&gt;/fullchain.cer\n        \n        and\n        \n         &lt;DOMAIN&gt;/&lt;DOMAIN&gt;.key\n        \n        as the root certificate and private key. Usually copied then to\n        \n         base/root.cer\n        \n        and\n        \n         base/root.key\n        \n        .\n       \n      \n      \n       Also, use\n       \n        *.&lt;DOMAIN&gt;/fullchain.cer\n       \n       and\n       \n        *.&lt;DOMAIN&gt;/&lt;DOMAIN&gt;.key\n       \n       as the wildcard certificate and private key. Usually copied then to\n       \n        base/wildcard.cer\n       \n       and\n       \n        base/wildcard.key\n       \n       ."},"36":{"url":"/docs_output/how-tos/","snip":"Edit on github"},"37":{"url":"/docs_output/how-tos/install-python-reqs-on-jnj-bastion.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      Install python requirements on bastion in JNJ\n     \n     wget --no-check-certificate https://bootstrap.pypa.io/pip/3.6/get-pip.py &amp;&amp; python3 get-pip.py --user\n\n     \n      Then, cd into the datacoves_deployment cloned repo folder, and run:\n     \n     pip install -r requirements.txt"},"38":{"url":"/docs_output/how-tos/list-code-server-pods-processes.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      List python processes running on certain namespace's code server pods\n     \n     #!/bin/bash\nns=\"dcw-dev001\npods=$(kubectl -n $ns get pods | grep code-server | awk '{print $1, $8}')\nfor pod in $pods; do\n  kubectl -n $ns exec -ti $pod -- bash -c 'ps auxwf' | grep python\ndone"},"39":{"url":"/docs_output/how-tos/make-and-install-a-release.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      Make a new release\n     \n     \n      To make a new release, from your development machine:\n     \n     cluster_domain=ensembletest.apps.jnj.com\n\n# Generate a new release.\ngit checkout main\ngit pull\n\n#  Check that images are properly created in Github Actions\n./cli.py generate_release\nrelease= # The name of the release just generated.\n\n# [If release is targeted to a submodule customer]\n#   Check if any there's any config change requirement\n./cli.py combined_release_notes     # Inspect the output to check for configuration changes\n\n# Update the cluster configuration to reference the new release.\n./cli.py set_release\ncd config/$cluster_domain/\ngit secret reveal -f # Only required if you modified secrets.\nchange configuration as required # Only required if you modified secrets.\ngit secret hide      # Only required if you modified secrets.\ngit add -A\ngit diff --cached    # Review what will be commited.\ngit commit\ngit push\n\n# Commit and push the changes to datacoves.\ncd ../..\ngit add -A\ngit diff --cached\ngit commit\ngit push\n\n     \n     \n     \n      Apply the release to a cluster\n     \n     \n     \n     \n      Localhost\n     \n     ./cli.py install\n\n     \n     \n     \n      JNJ\n     \n     \n      For jnj there's a git repository, datacoves_deployment, that mirrors the structure of\nthe datacoves repo but only contains scripts and configuration, not sources.\n     \n     \n      To deploy first update the mirror:\n     \n     # Clone if needed.\nmkdir -p ../jnj/asx-ahrx/datacoves_deployment\ngit clone ssh://git@sourcecode.jnj.com:3268/asx-ahrx/datacoves_deployment.git ../jnj/asx-ahrx/datacoves_deployment\n\n# Rsync the installer files into the datacoves_deployment repo\n./cli.py rsync_installer ../jnj/asx-ahrx/datacoves_deployment/\n\n# Point the config submodule to the latest version.\ncd config/$cluster_domain/\ngit pull\ncd ../..\n\n# Commit the changes.\ngit add -A\ngit diff --cached\ngit commit\n\n     \n      SSH into a jnj machine with kubectl access to the cluster. Then follow\n      \n       datacoves_deployment\n      \n      's\n      \n       documentation\n      \n      to run the installation scripts."},"40":{"url":"/docs_output/how-tos/manage-profiles-and-image-sets.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      Managing profiles and image sets\n     \n     \n     \n     \n      How to create and use a profile + image set?\n     \n     \n     \n     \n      1. Create profile\n     \n     \n      A profile is used to create a reusable preconfigured environment.\n     \n     \n      \n       Navigate to\n       \n        profiles admin page\n       \n       and create a new one clicking on \"Add Profile\".\n      \n      \n       Review the checkboxes and uncheck the ones that are not appropiate, you might like to keep them all checked as suggested.\n      \n      \n       Add profile files accordingly. You might like to copy the exact same profile files configured on the\n       \n        default profile\n       \n       .\n      \n     \n     \n     \n     \n      2. Create image set\n     \n     \n      Image sets are associated to profiles and they are used to build the images that will end up being used by code-server and/or airflow.\n     \n     \n      \n       Navigate to the\n       \n        Image set admin page\n       \n       and click on \"Create new image set\".\n      \n      \n       Choose the profile you just created in\n       \n        Profile\n       \n       .\n      \n      \n       Choose the release from where the new images are going to be based on, typically the last release.\n      \n      \n       Set the common python requirements for both airflow and code-server images in the\n       \n        Python requirements\n       \n       field. Take a look at the help text under the field.\n      \n      \n       Set the specific python requirements for airflow or code server in the fields\n       \n        Airflow requirements\n       \n       or\n       \n        Code server requirements\n       \n       .\n      \n      \n       Finally, configure the extensions you need installed in code-server by adding urls to the .vsix files in the\n       \n        Code server extensions\n       \n       field.\n      \n      \n       Hit \"Save and continue editing\".\n      \n      \n       Click on \"Build image set\" button in the top right corner of the form. A background process will be triggered to build the images.\n      \n      \n       Keep refreshing the page every 1 minute until the field\n       \n        Images\n       \n       get populated with the final images cooked.\n      \n     \n     \n     \n     \n      3. Start using you profile\n     \n     \n      Once you profile and image set are ready, you need to edit the environment you want to change and set the corresponding\n      \n       profile\n      \n      in such field. Environments are edited\n      \n       here\n      \n      .\n     \n     \n      \n     \n     \n     \n     \n      4. Reload the workbench page\n     \n     \n      That's all, reload the page and don't forget to prepare your\n      \n       mate\n      \n      to enjoy your analytics journey even more ;)"},"41":{"url":"/docs_output/how-tos/move-a-gpg-secret-key.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      How to move a gpg secret key\n     \n     \n      You should not reuse private gpg keys without thinking. However, it is more\nconvenient to have a single private key for your jnj email that is in all the\ngit secret keyrings of all the cluster config repos that you have access to.\n     \n     \n      An easy way to transfer a key to a new installation server is to copy and paste\nits base64:\n     \n     # From the machine that already has the key:\ngpg --list-secret-keys\ngpg --export-secret-key youremail@its.jnj.com | base64\n# Copy the output.\n\n     # From the installation machine:\ncat | base64 -d &gt; key.asc\n# Paste and hit control D.\ngpg --import key.asc\ngpg --list-secret-keys\nrm key.asc"},"42":{"url":"/docs_output/how-tos/onboard-a-new-project-on-datacoves.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      1. Create service accounts on snowflake (manually).\n     \n     \n      \n       svc_datacoves: to change user private key\n      \n      \n       svc_orchestration: airflow jobs\n      \n      \n       svc_loader: airbyte/fivetran jobs\n      \n      \n       svc_continuous_integration: CI jobs\n      \n      \n       svc_business_intelligence: BI tool connection (optional)\n      \n      \n       svc_business_intelligence_pii: BI tool connection for PII data (optional)\n      \n     \n     \n     \n     \n      2. Create user accounts on snowflake (manually)\n     \n     \n     \n     \n      3. New project on appdevtools (on JnJ):\n     \n     \n      \n       Bitbucket\n      \n      \n       Jenkins\n      \n      \n       Confluence\n      \n     \n     \n     \n     \n      4. Configure git service account access to repo\n     \n     \n     \n     \n      5. Add SQL hook and template to set users private key on snowflake\n     \n     \n     \n     \n      6. Create git repo structure using balboa repo as a reference:\n     \n     \n      \n       load\n      \n      \n       orchestrate\n      \n      \n       automate\n      \n      \n       dbt\n      \n      \n       profiles.yml\n      \n      \n       sample_blue_green.py\n      \n      \n       docs\n      \n      \n       secure\n      \n      \n       .gitignore\n      \n     \n     \n      Depending on CI:\n     \n     \n      \n       .github\n      \n      \n       .gitlab-ci.yml\n      \n      \n       Jenkinsfile\n      \n     \n     \n      CI job deploy to prod that:\n     \n     \n      \n       generate dbt docs on dbt-docs branch\n      \n      \n       runs dbt build on prod\n  CI job on PR that:\n      \n      \n       validate branch names\n      \n      \n       run pre-commit hooks\n      \n     \n     \n     \n     \n      7. Add airbyte connection on airflow\n     \n     \n     \n     \n      8. Add new branch “airflow_\n      \n       ” for every env that is not\n       \n        production\n       \n      \n     \n     \n     \n     \n      9. New dbt-docs branch\n     \n     \n     \n     \n      10. Jenkins configuration\n     \n     \n      \n       Git SA\n      \n      \n       Snowflake SA\n      \n     \n     \n     \n     \n      11. Enable dbt-docs once index.html was placed on dbt-docs branch"},"43":{"url":"/docs_output/how-tos/prometheus-queries.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      Useful prometheus queries\n     \n     \n     \n     \n      node status with pressure\n     \n     sum by(node) (kube_node_status_condition{status=\"true\", condition=\"DiskPressure\"}) +\nsum by(node) (kube_node_status_condition{status=\"true\", condition=\"MemoryPressure\"}) +\nsum by(node) (kube_node_status_condition{status=\"true\", condition=\"PIDPressure\"})\n\n     \n     \n     \n      pods memory filtering by pod name with regex\n     \n     sum by(pod) (container_memory_usage_bytes{namespace=\"&lt;NAMESPACE&gt;\", pod=~\"&lt;PREFIX&gt;.*\"})\n\n     \n     \n     \n      containers cpu usage by node\n     \n     sum by(node) (rate(container_cpu_usage_seconds_total{node=\"&lt;NODE&gt;\"}[5m]))\n\n     \n     \n     \n      Node memory\n     \n     node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100\n\n     \n     \n     \n      Loki ingester chunk stored size\n     \n     loki_ingester_chunk_stored_bytes_total{job=\"loki\"}\n\n     \n     \n     \n      Pods killed bec exceeding memory limit\n     \n     sum by(pod) (kube_pod_container_status_terminated_reason{reason=\"OOMKilled\", namespace=\"dcw-prd001\"})\n\n     \n     \n     \n      Total worker nodes (measued by nodes running airflow worker pods)\n     \n     count (sum by (node) (kube_pod_info and on (pod) kube_pod_labels{label_airflow_worker!=\"\"}) &gt; 0)"},"44":{"url":"/docs_output/how-tos/q-and-a.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      Questions and Answers\n     \n     \n      These are simple items that don't necessarily fit in elsewhere or need their own articles.\n     \n     \n     \n     \n      How do I start codeserver without validating the git repository credentials?\n     \n     \n      Code servers use User Repository settings, and currently User Repositories only work with SSH keys.  Sometimes, this is hard to deal with; if we can only use https authentication (i.e. from within J&amp;J pulling an external repository) and we need a work-around.\n     \n     \n      The workaround is simple; go to the Django panel.\n     \n     \n      Pick User Repositories\n     \n     \n      Pick the correct User Repository for your user and repo.\n     \n     \n      Put a date and time in the \"validated at\" field and save it.  So long as that isn't blank, it will allow you to start code server."},"45":{"url":"/docs_output/how-tos/recover-disk-on-aks.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      Recover disk (PV) from Azure Kubernetes Service.\n     \n     \n      This guide describes how to move a disk from one Kubernetes cluster to another cluster.\n      \n       More info\n      \n     \n     \n      Steps:\n     \n     \n      \n       \n        Edit old pvc to Retain policy.\n       \n      \n      \n       \n        Get PV name.\n       \n      \n      \n       \n        Delete PVC to release the PV in the old cluster.\n       \n      \n      \n       \n        Move the PV resource to new cluster using az cli.\n       \n      \n      \n       \n        Delete the PVC in the new cluster.\n       \n      \n      \n       \n        Create the PV and PVC in the new cluster.\n       \n      \n     \n     \n     \n     \n      Edit old pvc to Retain policy\n     \n     \n      The\n      \n       persistent volume (PV)\n      \n      that are created for\n      \n       code server\n      \n      has the delete policy, that means that when a disk is unbounded it is automatically deleted, therefore this policy must be modified to\n      \n       Retain\n      \n      .\n     \n     # Get the persistent volumes. E.g:\nkubectl get pv\n\n# Edit the persistent volume. E.g:\nkubectl patch pv pvc-2552cd9b-8231-409d-8b4b-a9d047415b53 -p '{\"spec\":{\"persistentVolumeReclaimPolicy\":\"Retain\"}}'\n\n     \n     \n     \n      Get PV name\n     \n     # Get the persistent volumes. E.g:\nkubectl get pv\n\n     \n     \n     \n      Delete PVC to release the PV in the old cluster\n     \n     \n      It is necessary to remove the\n      \n       persistent volume claim (PVC)\n      \n      to release the\n      \n       persistent volume (PV)\n      \n      .\n     \n     # Get the persistent volumes. E.g:\nkubectl -n dcw-dev123 get pvc\n\n# Edit the persistent volume. E.g:\nkubectl -n dcw-dev123 delete pvc code-server-bru-10-config-volume\n\n     \n     \n     \n      Move the PV resource to new cluster using az cli\n     \n     \n      \n       Get the\n       \n        cluster name\n       \n       and\n       \n        subcription id\n       \n       .\n      \n     \n     \n      \n     \n     \n      \n       Get the node resources groups. We will need the origin and destination.\n      \n     \n     # Get the node resources group\naz aks show --resource-group &lt;name-resource-group&gt; --name &lt;cluser-node&gt; --query nodeResourceGroup -o tsv\n\n     \n      \n       Get the id disk.\n      \n     \n     # Get the origien node resource group. E.g:\naz disk list --resource-group &lt;node-resouorce-group&gt;\n\n     \n      \n     \n     \n      \n       Move the disk.\n      \n     \n     az resource invoke-action --action moveResources --ids \"/subscriptions/&lt;origin-subcription-id&gt;/resourceGroups/&lt;origin-node-resource-group&gt;\" --request-body \"{  \\\"resources\\\": [\\\"&lt;disk_id&gt;\\\"],\\\"targetResourceGroup\\\":\\\"/subscriptions/&lt;destination-subcription-id&gt;/resourceGroups/&lt;destination-node-resource-group&gt;\\\" }\"\n\n     \n     \n     \n      Delete the PVC in the new cluster.\n     \n     \n      This step is only necessary if the\n      \n       persistent volume claim (PVC)\n      \n      already exists.\n     \n     # Get the persistent volumes. E.g:\nkubectl -n dcw-dev123 get pvc\n\n# Edit the persistent volume. E.g:\nkubectl -n dcw-dev123 delete pvc code-server-bru-10-config-volume\n\n     \n     \n     \n      Create the PV and PVC in the new cluster\n     \n     \n      Create the following file\n      \n       pvc.yaml\n      \n      with the names and namespace correct.\n     \n     \n      \n       \n        pv-name\n       \n       : E.g:\n       \n        pvc-2581bfb0-b56a-4fbd-b302-67cf0ab43499\n       \n      \n      \n       \n        pvc-name\n       \n       : If you deleted the pvc, the name should be the same. E.g:\n       \n        code-server-bru-10-config-volume\n       \n      \n      \n       \n        namespace\n       \n       : Kubernetes namespace to be applied.\n      \n      \n       \n        disk-id-full-path\n       \n       : E.g:\n       \n        /subscriptions/91bd2205-0d74-42c9-86ad-41cca1b4822b/resourceGroups/MC_datacoves_east-us-a_eastus/providers/Microsoft.Compute/disks/pvc-fddcd2fc-7d35-40e9-b631-49c64bd87cbf\n       \n      \n     \n     apiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: &lt;pv-name&gt;\nspec:\n  capacity:\n    storage: 20Gi\n  accessModes:\n    - ReadWriteOnce\n  persistentVolumeReclaimPolicy: Retain\n  storageClassName: default\n  csi:\n    driver: disk.csi.azure.com\n    readOnly: false\n    volumeHandle: &lt;disk-id-full-path&gt;\n\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: &lt;pvc-name&gt;\n  namespace: &lt;namespace&gt;\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 20Gi\n  volumeName: &lt;pv-name&gt;\n  storageClassName: default\n\n     \n      Create the resources in Kubernetes\n     \n     kubectl apply -f pvc.yaml\n\n# Check the resources\nkubectl get pvc | grep &lt;pv-name&gt; # pvc-2552cd9b-8231-409d-8b4b-a9d047415b53\nkubectl -n dcw-dev123 get pvc code-server-bru-10-config-volume"},"46":{"url":"/docs_output/how-tos/register-github-self-hosted-runner.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      Self hosted Github Runner\n     \n     \n      \n       Create new runnner\n       \n        in Github\n       \n       . You must have\n       \n        Owner\n       \n       privileges.\n      \n      \n       Create a virtual machine, e.g. in Azure, and run the scritps that Github gave you on the previous step.\n      \n      \n       Install dependencies on the machine you created\n      \n     \n     # Update and Upgrade\nsudo apt-get update\nsudo apt-get upgrade -y\n\n# Add Kubernetes repository and key\ncurl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\necho \"deb https://apt.kubernetes.io/ kubernetes-xenial main\" | sudo tee -a /etc/apt/sources.list.d/kubernetes.list\n\n# Add Helm repository and key\ncurl https://baltocdn.com/helm/signing.asc | sudo apt-key add -\necho \"deb https://baltocdn.com/helm/stable/debian/ all main\" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list\n\n# Update package list again after adding the Kubernetes and Helm repositories\nsudo apt-get update\n\n# Install software/packages\nsudo apt-get install -y apt-transport-https gnupg2 kubectl tmux python3-pip docker.io golang helm\n\n# Python symbolic link\nsudo ln -s /usr/bin/python3 /usr/bin/python\n\n# Docker post-installation step for the current user\nsudo usermod -aG docker $USER\n\n# Go and kind installation\ngo install sigs.k8s.io/kind@v0.20.0\nsudo ln -s /home/datacoves/go/bin/kind /usr/local/bin/kind\n\n     \n      \n       run\n       \n        tmux\n       \n       to do not close the session when detached from ssh connection.\n      \n      \n       Follow any instruction you got from Github on step 1 and install the runner as a service:\n       \n        sudo ./svc.sh install datacoves\n       \n      \n      \n       Boost inotify limits for system performance. Update the following values in the specified files:\n       ```Boost inotify limits for system performance. Update the following values in the specified files:\n~$ cat /proc/sys/fs/inotify/max_user_instances\n1024\n~$ cat /proc/sys/fs/inotify/max_user_watches\n524288\n~$ cat /proc/sys/fs/inotify/max_queued_events\n16384\n```"},"47":{"url":"/docs_output/how-tos/release-notes.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      Statement of Purpose\n     \n     \n      The purpose of this document is to describe the process by which we manage release notes to deliver to our customers.\n     \n     \n     \n     \n      Source of Authority\n     \n     \n      Release notes all come from Github:\n     \n     \n      https://github.com/datacoves/datacoves/releases\n     \n     \n      The notes begin live as auto-generated notes that are created when the release branch is built.  Then, we hand-edit the release notes to match the following format:\n     \n     Breaking Changes\n* Items that are breaking changes, in list.\n\nNew Features\n* New features, in list.\n\nEnhancements\n* Enhancements to old features, in list\n\nFixes\n* Bug fixes, in list\n\nUnder the Hood\n* Notes relevant to us internally which we would like to keep, but not important to customers.\n\n**Full Changelog**: This is a URL that is provided automatically, just leave it in the change log.\n\n     \n     \n     \n      Generating Release Notes\n     \n     \n      Release notes are generated per-customer and have all the changes from their current release to the latest release you currently have downloaded in your 'releases' folder.  Make sure you have the customer's cluster configuration checked out into your 'config' directory; if you do not, stop and ask for help before continuing.\n     \n     \n      You can control which release notes are generated; make sure you have downloaded the releases first:\n     \n     ./cli.py download_releases\n\n     \n      If desired or necessary, you can delete files out of your 'releases' directory; for instance, if the customer is getting updated to the latest 2.2 series release but there are 2.3 series releases available, you could delete all the 2.3 release files out of your 'releases' directory and notes for those releases will not be produced.\n     \n     \n      Release notes are then generated using the\n      \n       cli.py\n      \n      thusly:\n     \n     ./cli.py combined_release_notes\n\n     \n      It will make a file\n      \n       combined.md\n      \n      in the same directory as\n      \n       cli.py\n      \n      , and that will have the combined release notes for all the releases involved.  This file can then be delivered to the customer as part of the announcement to upgrade them."},"48":{"url":"/docs_output/how-tos/request-access-to-a-cloud-pc-on-kenvue.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      How to request access to a cloud PC on Kenvue\n     \n     \n      \n       \n        Navigate to this\n        \n         form\n        \n        .\n       \n      \n      \n       \n        Complete it accordingly:"},"49":{"url":"/docs_output/how-tos/reset-datahub.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      Resetting Datahub\n     \n     \n      Datahub uses PostgreSQL, ElastiCache, and Kafka.  If any of these three things gets out of sync for any reason, Datahub will behave very strangely.  For instance, it will claim secrets exist but not show them up in the UI.\n     \n     \n      In such an event, you will need to reset Datahub.  This can be done with the following steps:\n     \n     \n      In all these examples, replace\n      \n       xxx\n      \n      with the slug (such as dev123).\n     \n     \n     \n     \n      Turn Off Datahub\n     \n     \n      Go to the environment you wish to reset, and disable Datahub.  Save and sync the environment and wait until Datahub come offline by monitoring the Datahub pods:\n     \n     kubectl get pods -n dcw-xxx | grep datahub\n\n     \n      This will take awhile.\n     \n     \n     \n     \n      Delete Metadata in PostgreSQL\n     \n     ./cli.py pod_sh\n./manage.py dbshell\n\\c xxx_dh\ndrop table metadata_aspect_v2\n\n     \n     \n     \n      Delete Persistent Volume Claims\n     \n     kubectl delete pvc -n dcw-xxx elasticsearch-master-elasticsearch-master-0\nkubectl delete pvc -n dcw-xxx data-xxx-kafka-broker-0\nkubectl delete pvc -n dcw-xxx data-xxx-kafka-zookeeper-0\n\n     \n     \n     \n      Verify Persistent Volumes are deleted\n     \n     kubectl get pv -n dcw-xxx | grep xxx | grep elasticsearch\nkubectl get pv -n dcw-xxx | grep xxx | grep kafka\n\n     \n      These should show no results.  These should delete automatically when the PVC is deleted, make sure they are gone.\n     \n     \n     \n     \n      Re-enable Datahub\n     \n     \n      Go back to the environment, turn Datahub back on, and re-sync."},"50":{"url":"/docs_output/how-tos/security-vulnerabilities-fix.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      How to run security vulnerabilities check and fix them\n     \n     \n     \n     \n      React app\n     \n     \n     \n     \n      Install\n     \n     yarn add yarn-audit-fix -D\n\n     \n     \n     \n      Run\n     \n     yarn-audit-fix\n\n     \n      Learn more: https://yarnpkg.com/package?name=yarn-audit-fix\n     \n     \n     \n     \n      Django app\n     \n     \n     \n     \n      Install\n     \n     pip install pip-audit\n\n     \n     \n     \n      Run\n     \n     pip-audit -r ./requirements.txt"},"51":{"url":"/docs_output/how-tos/set-maintenance-mode.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      How to set the cluster in \"Maintenance Mode\"\n     \n     \n      Turning it on:\n     \n     ./cli.py set_maintenance_mode &lt;kubectl context&gt; &lt;cluster domain&gt; \"on\" \"today at 9PM UTC\" \"support@datacoves.com\" \"our Support Team\"\n\n     \n      Turning it off:\n     \n     ./cli.py set_maintenance_mode &lt;kubectl context&gt; &lt;cluster domain&gt; \"off\""},"52":{"url":"/docs_output/how-tos/setup-oauth-on-azure.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      How to set up oAuth authentication on Azure\n     \n     \n      \n       \n        NOTE:\n       \n       This guide was based on this\n       \n        Auth0 help page\n       \n       , it could require some adjustments.\n      \n     \n     \n      This is done using Azure AD / Entra ID apps.\n     \n     \n     \n     \n      Register new app\n     \n     \n      \n       Navigate to App registrations on Azure Portal\n      \n     \n     \n      \n     \n     \n      \n       Register a new App, choosing a name, selecting \"Accounts in this organizational directory only (Datacoves Inc. only - Single tenant)\" \nand providing a redirect url in the form of \"https://api.{cluster_domain}/complete/azuread-tenant-oauth2\"\n      \n     \n     \n      \n     \n     \n      \n       Once created, get the client id and tenant id from the overview page\n      \n     \n     \n      \n     \n     \n     \n     \n      Generate Client Secret\n     \n     \n      Navigate to 'Certificates &amp; Secrets' and Generate a new client secret\n     \n     \n      \n     \n     \n      Keep the value safe.\n     \n     \n     \n     \n      Configure permissions\n     \n     \n      Navigate to app permissions and then 'Add permissions'. Select 'Microsoft Graph', then 'Delegated permissions', and the following OpenId permissions.\n     \n     \n      \n     \n     \n      Also add permissions to read groups memberships if they're going to be used to determine permissions in Datacoves.\n     \n     \n      \n     \n     \n      Finally, consent as an Admin the permissions granted by clicking on this button:\n     \n     \n      \n     \n     \n     \n     \n      Configure token\n     \n     \n      We need to include the groups claim in both the ID and access token, to do so, go to Token configuration:\n     \n     \n      \n     \n     \n      Click on \"Add groups claim\", select \"Security groups\", make sure \"Group ID\" is selected in both ID and Access tokens and click on Add.\n     \n     \n      \n     \n     \n     \n     \n      Configure Datacoves\n     \n     \n      Configure the Client ID, Tenant ID and Client Secret accordingly on Datacoves using the env variables AZUREAD_CLIENT_ID, AZUREAD_TENANT_ID, and AZUREAD_CLIENT_SECRET."},"53":{"url":"/docs_output/how-tos/setup-s3-for-dbt-api.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      Create a S3 bucket for dbt api artifacts\n     \n     \n     \n     \n      Create bucket on AWS console\n     \n     \n      \n       Create an S3 bucket.\n      \n      \n       Choose a bucket name, we suggest using\n       \n        _dbt_api where\n        \n         could be\n         \n          ensemble\n         \n         ,\n         \n          ensembletest\n         \n         , etc.\n        \n       \n      \n      \n       Create an IAM user with a policy to access the bucket, like the one below,\n  replacing\n       \n        {your_bucket_name}\n       \n       with your bucket's name.\n      \n      \n       Create an access key for the user. Share it with the Datacoves team.\n      \n     \n     {\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:PutObject\",\n        \"s3:GetObject\",\n        \"s3:GetObjectVersion\",\n        \"s3:DeleteObject\",\n        \"s3:DeleteObjectVersion\"\n      ],\n      \"Resource\": \"arn:aws:s3:::{your_bucket_name}/*\"\n    },\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"s3:ListBucket\",\n        \"s3:GetBucketLocation\"\n      ],\n      \"Resource\": \"arn:aws:s3:::{your_bucket_name}\"\n    }\n  ]\n}\n\n     \n     \n     \n      Configure Datacoves accordingly\n     \n     \n      For the cluster being configured, set the following environment variables in the\n      \n       core-dbt-api.env\n      \n      file:\n     \n     STORAGE_ADAPTER=s3\nS3_BUCKET_NAME=fill_in\nS3_ACCESS_KEY=fill_in\nS3_SECRET_ACCESS_KEY=fill_in\nS3_REGION=fill_in"},"54":{"url":"/docs_output/how-tos/testing-alerts.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      How to create and test alerts\n     \n     \n     \n     \n      Stack\n     \n     \n      \n       Alert Manager\n      \n      \n       Loki Alert Ruler\n      \n      \n       Grafana\n      \n     \n     \n     \n     \n      Test Loki Alert\n     \n     \n      \n       Add the new alert on\n       \n        scripts/data/loki-rules.yaml\n       \n       file.\n      \n      \n       Install\n       \n        Observability Stack\n       \n       .\n      \n      \n       Force some logs.\n      \n     \n     \n      Example:\n     \n     # Option 1\nkubectl -n core exec -it api-75567b8958-7b7rx -- bash\n\n# Option 2\n./cli.py pod_sh\n\n./manage.py shell_plus\n\n     import requests\nimport time\n\npayload = {\n  \"streams\": [\n    {\n      \"stream\": {\n        \"agent_hostname\": \"eventhandler\",\n        \"job\": \"test\",\n        \"namespace\": \"core\"\n      },\n      \"values\": [[ str(int(time.time() * 1e9)), \"max node group size reached\" ]]\n    }\n  ]\n}\n\nrequests.post(\n  url=\"http://loki-loki-distributed-gateway.prometheus.svc.cluster.local/loki/api/v1/push\",\n  json=payload,\n  headers={\"Content-Type\": \"application/json\"}\n)\n\n     \n      \n       Now you can see the alert on\n       \n        Cluster Alerts"},"55":{"url":"/docs_output/how-tos/trigger-cloudx-pipeline-on-kenvue-cluster.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      How to trigger a cloudx pipeline manually after changing cluster.yml on a kenvue cluster\n     \n     \n      \n       Go to the bastion\n      \n      \n       Run the curl command you can find in 1Password named\n       \n        Run cloudx pipelines using curl on Kenvue clusters\n       \n      \n      \n       Not that the\n       \n        Branch\n       \n       queryparam references the repo branch you changed."},"56":{"url":"/docs_output/how-tos/update-kubernetes-and-datacoves.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      Statement of Purpose\n     \n     \n      The purpose of this document is to describe common upgrade procedures for both updating Kubernetes and updating Datacoves on customer clusters.\n     \n     \n     \n     \n      Updating Kubernetes\n     \n     \n      The procedure varies for Azure vs. AWS.  We generally prefer to use the web console to do the upgrade.\n     \n     \n     \n     \n      Gain Kubernetes command line access to the cluster\n     \n     \n      Make sure you are set up for Kubernetes command line access.\n     \n     \n      \n       For Orrum the instructions are here: https://github.com/datacoves/datacoves/tree/main/docs/client-docs/orrum\n      \n     \n     \n      Access whatever VPN is necessary.  Switch to the correct Kubernetes context:\n     \n     kubectl config get-contexts\nkubectl config use-context context-name\n\n     \n      If you aren't set up to do this, stop now and get help.\n     \n     \n     \n     \n      Disable Sentry Alarms\n     \n     \n      Sentry is going to complain very loudly about all this.\n     \n     \n      Currently, it looks like there is no way to disable this without the Sentry Business Plan which we do not have.  But if that ever changes, we'll update this section.\n      \n       For now, there is nothing to do.\n      \n     \n     \n     \n     \n      Check and Prepare PDB's\n     \n     \n      The Kubernetes PDBs can cause an upgrade to hang, as it will prevent a pod from shutting down to receive the update.  Check the PDBs like this:\n     \n     kubectl get pdb -A\n\n     \n      You will get an output similar to:\n     \n     NAMESPACE       NAME                           MIN AVAILABLE   MAX UNAVAILABLE   ALLOWED DISRUPTIONS   AGE\ncalico-system   calico-typha                   N/A             1                 1                     273d\ncore            api                            1               N/A               0                     232d\ncore            beat                           1               N/A               0                     232d\ncore            redis                          1               N/A               0                     232d\ncore            workbench                      1               N/A               0                     232d\ncore            worker                         1               N/A               0                     232d\ndcw-dev123      dev123-airflow-scheduler-pdb   N/A             1                 1                     26h\ndcw-dev123      dev123-airflow-webserver-pdb   N/A             1                 1                     26h\nkube-system     coredns-pdb                    1               N/A               1                     273d\nkube-system     konnectivity-agent             1               N/A               1                     273d\nkube-system     metrics-server-pdb             1               N/A               1                     273d\n\n     \n      Note the core namespace clusters with ALLOWED DISRUPTIONS at 0.  You will need to patch those so that they will allow a disruption, and then revert the patch when done.\n     \n     \n      The following commands will allow for a disruption:\n     \n     kubectl patch pdb -n core api -p '{\"spec\":{\"minAvailable\":0}}'\nkubectl patch pdb -n core beat -p '{\"spec\":{\"minAvailable\":0}}'\nkubectl patch pdb -n core redis -p '{\"spec\":{\"minAvailable\":0}}'\nkubectl patch pdb -n core workbench -p '{\"spec\":{\"minAvailable\":0}}'\nkubectl patch pdb -n core worker-long -p '{\"spec\":{\"minAvailable\":0}}'\nkubectl patch pdb -n core worker-main -p '{\"spec\":{\"minAvailable\":0}}'\nkubectl patch pdb -n core dbt-api -p '{\"spec\":{\"minAvailable\":0}}'\nkubectl patch pdb -n prometheus cortex-tenant -p '{\"spec\":{\"minAvailable\":0}}'\n\n     \n      You can apply this to any other PDBs that prevent disruptions.\n      \n       Take note of all the PDBs that you altered in this fashion.\n      \n     \n     \n     \n     \n      Upgrade Kubernetes\n     \n     \n      This varies based on the cloud provider.\n     \n     \n     \n     \n      On Azure\n     \n     \n      Go to:\n     \n     \n      https://portal.azure.com/#view/HubsExtension/BrowseResource/resourceType/Microsoft.ContainerService%2FmanagedClusters\n     \n     \n      Make sure you are logged into the correct client account (check the upper right corner).\n     \n     \n      Locate the cluster you want to work with.  Often you will have to alter the default filters so that \"Subscription equals all\".\n     \n     \n      Pick the cluster you are updating.  If you are not sure which one, ask.\n     \n     \n      On the overview screen that comes up by default, you will see \"Kubernetes version\" in the upper right area.  Click the version number.\n     \n     \n      It will show version details; click Upgrade Version.\n     \n     \n      \n       Pick Automatic upgrade: Enabled with patch (recommended)\n      \n      \n       Pick Kubernetes version: the version you wish to upgrade to\n      \n      \n       Pick upgrade scope: Upgrade control plane + all node pools\n      \n      \n       Click save\n      \n     \n     \n      The upgrade will start in a few moments.\n     \n     \n     \n     \n      Wait for it to come back\n     \n     \n      The update can take quite awhile.  Keep an eye on the pods and watch them update:\n     \n     kubectl get pods -A\n\n     \n      You will see a lot of activity, pods shutting down and restarting.  Once it's all back online, you can restore the PDBs (see next step) and you can verify the update (see bottom of this file).\n     \n     \n     \n     \n      Restore PDB's\n     \n     \n      We need to put the PDB's back in place.\n     \n     kubectl get pdb -A\n\n     \n      You will get an output similar to:\n     \n     NAMESPACE       NAME                           MIN AVAILABLE   MAX UNAVAILABLE   ALLOWED DISRUPTIONS   AGE\ncalico-system   calico-typha                   N/A             1                 1                     273d\ncore            api                            0               N/A               1                     232d\ncore            beat                           0               N/A               1                     232d\ncore            redis                          0               N/A               1                     232d\ncore            workbench                      0               N/A               1                     232d\ncore            worker                         0               N/A               1                     232d\ndcw-dev123      dev123-airflow-scheduler-pdb   N/A             1                 1                     26h\ndcw-dev123      dev123-airflow-webserver-pdb   N/A             1                 1                     26h\nkube-system     coredns-pdb                    1               N/A               1                     273d\nkube-system     konnectivity-agent             1               N/A               1                     273d\nkube-system     metrics-server-pdb             1               N/A               1                     273d\n\n     \n      The following commands will re-enable the PDBs:\n     \n     kubectl patch pdb -n core api -p '{\"spec\":{\"minAvailable\":1}}'\nkubectl patch pdb -n core beat -p '{\"spec\":{\"minAvailable\":1}}'\nkubectl patch pdb -n core redis -p '{\"spec\":{\"minAvailable\":1}}'\nkubectl patch pdb -n core workbench -p '{\"spec\":{\"minAvailable\":1}}'\nkubectl patch pdb -n core worker-main -p '{\"spec\":{\"minAvailable\":1}}'\nkubectl patch pdb -n core worker-long -p '{\"spec\":{\"minAvailable\":1}}'\nkubectl patch pdb -n core dbt-api -p '{\"spec\":{\"minAvailable\":1}}'\nkubectl patch pdb -n prometheus cortex-tenant -p '{\"spec\":{\"minAvailable\":1}}'\n\n     \n      Also restore any additional PDBs you had to disable in the prior step.\n     \n     \n     \n     \n      Updating DataCoves\n     \n     \n      Updating DataCoves is relatively simple.  However, some of the access details can be compllicated.\n     \n     \n     \n     \n      First Time Setup: Set Up Deployment Environment and Get Needed Access\n     \n     \n      J&amp;J, Kenvue, and Orrum have some complexity around access.  AKS access is relatively easy.  These are one-time steps you need to take to get access to each environment.\n     \n     \n     \n     \n      AKS\n     \n     \n      Accessing AKS is documented here: https://github.com/datacoves/datacoves/blob/main/docs/how-tos/administrate-east-us-a-aks-cluster.md\n     \n     \n      Installation is done using your development system's checked out copy of the Datacoves repository.  AKS' configuration repository is located at: https://github.com/datacoves/config-datacoves-east-us-a and should be checked out into your 'config' directory.\n     \n     \n     \n     \n      Orrum\n     \n     \n      Accessing Orrum is documented here: https://github.com/datacoves/datacoves/tree/main/docs/client-docs/orrum\n     \n     \n      Installation is done using your development system's checked out copy of the Datacoves repository.  Note that Orrum requires a VPN, but the access is described above.  Orrum's configuration repository is here: https://github.com/datacoves/config-datacoves-orrum and must be checked out into your 'config' directory.\n     \n     \n     \n     \n      CCS\n     \n     \n      To access CCS, your Datacoves account must be added to CCS' Azure organization.  Eugine Kim can assist with this.\n     \n     \n      Then, you must download and install the Azure VPN client.  For Macs, this is done through the Apple Store.\n     \n     \n      And finally, you need the Azure command line tools which you probably already have installed if you followed our README instructions for setting up this repository.  You should also be logged into Azure with\n      \n       az login\n      \n      .\n     \n     \n      Then, on the VPN, you can shell into the Bastion as follows:\n     \n     az ssh vm --subscription 3099b8af-7ca1-4ff4-b9c5-1960d75beac7 ssh vm --ip 10.0.2.4\n\n     \n      Once on the Bastion, the tools are installed with Linux Brew:  So, edit your\n      \n       .bashrc\n      \n      file in your home directory with your favorite editor and add this to the end:\n     \n     eval $(/home/linuxbrew/.linuxbrew/bin/brew shellenv)\n\n     \n      Log out and log back in.\n      \n       python3 --version\n      \n      should reveal a modern\n      \n       3.1x\n      \n      python version.\n     \n     \n      From this point, it is simply check out the datacoves repository and do the installation like any other system.\n     \n     \n     \n     \n      J&amp;J / Kenvue\n     \n     \n      J&amp;J access is complex; going into the details of all the setup is out of the scope of this documentation.  However, we will cover how to get set up on the bastion so you can get to work.\n     \n     \n      It is a good idea to read this documentation if you haven't already: https://github.com/datacoves/datacoves/tree/main/docs/client-docs/jnj\n     \n     \n      In order to do deployments in J&amp;J or Kenvue, you have to do the work from a bastion server, which is a Linux machine accessible via your Cloud PC.  J&amp;J and Kenvue have different bastions, however configuring them is basically the same.\n     \n     \n      The IP address for the J&amp;J Bastion is:\n      \n       10.157.82.138\n      \n      and the IP address for the Kenvue bastion is: (... I am unable to log into Kenvue right now! Great!)\n     \n     \n      I make a\n      \n       .bat\n      \n      file that runs\n      \n       ssh IP\n      \n      where the IP is the one above.\n     \n     \n      Once you log into the bastion, there's a few things to note:\n     \n     \n      \n       You can sudo to root thusly:\n       \n        sudo su -\n       \n       .  Any other\n       \n        sudo\n       \n       command will not work, you can only\n       \n        sudo su -\n       \n       .\n      \n      \n       The default home directory you log into on the bastion does not have much disk space, so we use a volume mount on\n       \n        /app\n       \n       for most of our work.\n      \n      \n       We use\n       \n        brew\n       \n       to manage packages.\n      \n     \n     \n      To get set up initially, take the following steps:\n     \n     \n     \n     \n      Copy base configuration\n     \n     \n      \n       cp -R /app/users/datacoves-home-template/. ~/\n      \n     \n     \n     \n     \n      Add brew to your bash rc\n     \n     \n      Edit your\n      \n       .bashrc\n      \n      file in your home directory with your favorite editor and add this to the end:\n     \n     eval $(/home/linuxbrew/.linuxbrew/bin/brew shellenv)\n\n     \n      Log out and log back in.\n      \n       python3 --version\n      \n      should reveal a modern\n      \n       3.1x\n      \n      python version.\n     \n     \n     \n     \n      Login to Kubernetes\n     \n     kubectl config get-contexts\n\n     \n     \n     \n      Set up your deployment repository\n     \n     sudo su -\nmkdir -p /app/users/$USER\nchown -R $USER /app/users/$USER\nexit\ncd /app/users/$USER\ngit clone https://github.com/datacoves/datacoves.git\ncd datacoves\npython3 -m venv .venv\nsource .venv/bin/activate\npip3 install -r requirements.txt\n\n     \n     \n     \n      Set up your configuration repository\n     \n     \n      For each environment you will deploy to, you need to check out its config repository into your 'configs' directory.  The list of repositories is here:\n     \n     \n      https://github.com/datacoves/datacoves/blob/main/docs/client-docs/jnj/1-cluster-requirements.md\n     \n     \n     \n     \n      Before Deployment: Create your Plan\n     \n     \n      Before a deployment is done, you must first check to see if there's any special installation steps.  I use a Word document template, and I update it according to each release adding any special steps that I need to.  Then I print it out and use it as a physical check list.  My template file is\n      \n       here\n      \n      .\n     \n     \n      First, look at the version of the cluster you will be updating.  You can get this version from the cluster-params.yaml.  The easiest way to do this is to check the difference between two versions in GitHub.  Here's an example of a comparison between two versions:\n     \n     \n      https://github.com/datacoves/datacoves/compare/v3.2.202410250048...v3.2.202411140044\n     \n     \n      Look at all the pull requests that are in your new releae and check to see if you have any that are labeled \"special release step\" and add any special steps to your release document.  Post your finished work on the Slack dev channel for commentary.\n     \n     \n     \n     \n      Perform the installation\n     \n     \n      Release documentation is here: https://www.notion.so/datacoves/Release-Instructions-1b5ea827f87280f98620dccc1600727c\n      \n       Be very sure you are releasing from the correct release branch\n      \n      .  You need to release from the tag you are releasing.  You can check out a tag thusly:\n     \n     git fetch -a\ngit checkout refs/tags/v1.2.34234523452524\n\n     \n      Replace the tag name with the version you are deploying.  If you deploy from main or the wrong branch, you risk using installation scripts that are newer and have features that aren't supported yet by the images you are edeploying.\n     \n     \n     \n     \n      How to run migrations on a stuck install process\n     \n     \n      Sometimes migrations do not run automatically because the new pod containing the migrations fails before they can be applied. When this occurs we need to execute them manually. So we need to remove the\n      \n       LivenessProbe\n      \n      and\n      \n       ReadinessProbe\n      \n      , this makes the new pod run correctly and allows us to enter it and execute the migrations ourselves.\n     \n     kubectl patch deployments -n core api -p '{\"spec\": {\"template\": {\"spec\": {\"containers\":[{\"name\": \"api\", \"livenessProbe\": null, \"readinessProbe\": null}]}}}}'\n\n     \n      When the pod run correctly.\n     \n     kubectl -n core get pods\nkubectl -n core exec -it api-&lt;hash&gt; -- bash\n./manage.py migrate\n\n     \n     \n     \n      Create Profile Image Set for New Release\n     \n     \n      This may be necessary if an error about Profile Image Sets occurs; it is a bit of a chicken and the egg problem, as the release needs to exist prior to creating the profile image set, but the release won't exist until the install process is attempted.\n     \n     \n      Log into the customer's API panel.\n     \n     \n      \n       Orrum's is: https://api.datacoves.orrum.com/panel\n      \n      \n       CCS' is: https://api.datacoves.cssperfusion.com/panel\n      \n     \n     \n      Under \"Projects\" pick \"Profile Image Sets\".  Go to the existing Profile Image Set for the old release, and copy / paste the 4 JSON blocks into an editor.  Take a note of what is in the 'profile' field.\n     \n     \n      Go back to the listing of Profile Image Sets and click\n      \n       + Add profile image set\n      \n      in the corner.  Make the profile the same as the previous release's, and choose the new release from the release select box.\n     \n     \n      Then, paste in the four JSON blocks into the new Profile Image Set.  Check your release YAML file in\n      \n       releases\n      \n      and note the section 'code_server_libraries'; compare that to the Python libraries in the profile image set.  Update versions as needed, but never downgrade.  There's no need to add libraries that are in the release YAML but not in the profile image entry.\n     \n     \n      Also check 'code_server_extensions' against 'code server extensions' and apply the same logic to update extensions that are in the Profile Image Set.\n     \n     \n      Save the new profile image set, and making sure to keep all the data from the old profile image set just in case you need it, go back into that one and delete it.\n     \n     \n      You can now re-run installation and it should get past this error.\n     \n     \n     \n     \n      Verify Installation\n     \n     \n      Verifying the installation is the same no matter what process you're engaging in with DataCoves clusters, be it a Kubernetes update or a DataCoves update.\n     \n     \n      \n       Make sure no helm chart failed and retry if needed:\n       \n        ./cli.py retry_helm_charts\n       \n      \n      \n       Log into the customer's API panel and make sure that is working.\n      \n      \n       Log into the customer's launchpad and make sure that is working.\n      \n      \n       Pick one of the customer's environments and make sure you can get into it.\n       \n        \n         Try to use code server (\"Transform\")\n        \n        \n         Open a terminal in code server and run\n         \n          dbt-coves --version\n         \n        \n        \n         Try to use Airflow (\"Orchestrate\")\n        \n        \n         Look at logs in one of the DAGs\n        \n       \n      \n     \n     \n      If your user does not have permission to get into the customer's cluster, temporarily add yourself to the necessary groups to check the cluster."},"57":{"url":"/docs_output/how-tos/update-ssl-certificates.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      Statement of Purpose\n     \n     \n      The purpose of this document is to describe the process of upgrading SSL certificates for customers that are using custom certificates (i.e. not using Let's Encrypt).\n     \n     \n     \n     \n      Step 1: Prepare and Verify Certificate Files\n     \n     \n      \n       This should be done soon after certificate files are received, and not last minute.\n      \n     \n     \n      Ultimately, we need the following files:\n     \n     \n      \n       root.cer\n      \n      \n       root.secret.key\n      \n      \n       wildcard.cer\n      \n      \n       wildcard.secret.key\n      \n     \n     \n      The root.cer is the certificate for the root domain, i.e. datacoves.orrum.com\n     \n     \n      wildcard.cer is a wildcard, i.e. *.datacoves.orrum.com\n     \n     \n      All of these files should be in pem format; the cer files should have the complete keychain.  A pem format looks like this:\n     \n     -----BEGIN CERTIFICATE-----\nMIIEjTCCAvWgAwIBAgIQQ71EG0d4110tqpc8I8ur/jANBgkqhkiG9w0BAQsFADCB\npzEeMBwGA1UEChMVbWtjZXJ0IGRldmVsb3BtZW50IENBMT4wPAYDVQQLDDVzc2Fz\nc2lAU2ViYXN0aWFucy1NYWNCb29rLVByby5sb2NhbCAoU2ViYXN0aWFuIFNhc3Np\n....\nJbszQlyzkyzBxQ5eiK3OUNdsB+n5Zo+TshRRL45wA9fZmvAizzmtehxJWUbidGL7\neqqMWqdt11MTLJ3feOjGlryMFO6TIt/aH/91VkoLyVhsemuk5LukZ1nIxoWvzHcf\ny2cC+I3F8bWbYkRr92fmb8A=\n-----END CERTIFICATE-----\n\n     \n      There should be several BEGIN / END certificate blocks in wildcard.cer and root.cer file; the wildcard.csr and root.csr files should have a complete certificate stack and should be suspect if they only contain a single certificate block.\n     \n     \n      The key files will have a slightly different header, looking like this:\n     \n     -----BEGIN PRIVATE KEY-----\nMIIEvAIBADANBgkqhkiG9w0BAQEFAASCBKYwggSiAgEAAoIBAQCLf9Q17CQlOWDB\nCwWOuzL4+aalFwj2PR+OTuPnjHCI8stDedvmy5jtxSkdAL+5PgNu7ZJbKFhbODgT\n...\nOpuSfWnGVhOmii2aiYePtvNqDsLQv59MUxpUi8R6aw/XhG2Vb7t14+hbmUtRScUV\nLcGdNBdJyB8NaHYR/sNF1w==\n-----END PRIVATE KEY-----\n\n     \n      \n       If you receive a pfx format file, we cover that in a section below.  Read that section and go through those steps, then return to this section to complete verification.\n      \n     \n     \n      You can verify the certs with the following commands:\n     \n     # Verify root\nopenssl crl2pkcs7 -nocrl -certfile root.cer | openssl pkcs7 -print_certs -noout -text\n\n# Verify wildcard\nopenssl crl2pkcs7 -nocrl -certfile wildcard.cer | openssl pkcs7 -print_certs -noout -text\n\n     \n      And you will see several blocks with a Certificate header.  One block should contain the host name for the certificate.  In our example, datacoves.orrum.com:\n     \n     Certificate:\n    Data:\n        Version: 3 (0x2)\n        Serial Number:\n            01:cb:00:21:05:34:94:76:2b:f8:68:cf:8a:09:4c:02\n        Signature Algorithm: sha256WithRSAEncryption\n        Issuer: C=US, O=DigiCert Inc, OU=www.digicert.com, CN=Thawte TLS RSA CA G1\n        Validity\n            Not Before: Apr 22 00:00:00 2024 GMT\n            Not After : Apr 21 23:59:59 2025 GMT\n        Subject: CN=datacoves.orrum.com\n\n     \n      Note the hostname under 'Subject'; make sure that is the correct host.  root will appear as above, as a single host name; wildcard should look like this instead:\n     \n     Certificate:\n    Data:\n        Version: 3 (0x2)\n        Serial Number:\n            0d:7f:e3:36:2c:db:b0:65:78:9a:c1:88:f8:06:12:4f\n        Signature Algorithm: sha256WithRSAEncryption\n        Issuer: C=US, O=DigiCert Inc, OU=www.digicert.com, CN=Thawte TLS RSA CA G1\n        Validity\n            Not Before: Apr 22 00:00:00 2024 GMT\n            Not After : Apr 21 23:59:59 2025 GMT\n        Subject: CN=*.datacoves.orrum.com\n\n     \n      Note the * symbol there in the subject.  Also take note of the issuer;\n      \n       CN=Thawte TLS RSA CA G1\n      \n      .\n     \n     \n      Elsewhere in the certificate output, you should see a certificate for the issuer, such as:\n     \n     Certificate:\n    Data:\n        Version: 3 (0x2)\n        Serial Number:\n            09:0e:e8:c5:de:5b:fa:62:d2:ae:2f:f7:09:7c:48:57\n        Signature Algorithm: sha256WithRSAEncryption\n        Issuer: C=US, O=DigiCert Inc, OU=www.digicert.com, CN=DigiCert Global Root G2\n        Validity\n            Not Before: Nov  2 12:24:25 2017 GMT\n            Not After : Nov  2 12:24:25 2027 GMT\n        Subject: C=US, O=DigiCert Inc, OU=www.digicert.com, CN=Thawte TLS RSA CA G1\n\n     \n      Note the subject matches the issuer name.  And finally, this certificate has an issuer as well; make sure that one is in the file.  In this case,\n      \n       DigiCert Global Root G2\n      \n      .  In our example, you can find it here:\n     \n     Certificate:\n    Data:\n        Version: 3 (0x2)\n        Serial Number:\n            03:3a:f1:e6:a7:11:a9:a0:bb:28:64:b1:1d:09:fa:e5\n        Signature Algorithm: sha256WithRSAEncryption\n        Issuer: C=US, O=DigiCert Inc, OU=www.digicert.com, CN=DigiCert Global Root G2\n        Validity\n            Not Before: Aug  1 12:00:00 2013 GMT\n            Not After : Jan 15 12:00:00 2038 GMT\n        Subject: C=US, O=DigiCert Inc, OU=www.digicert.com, CN=DigiCert Global Root G2\n\n     \n      Note again the 'subject' line.  Typically PEM files will have certificates in the following order:\n     \n     \n      \n       Host's certificate\n      \n      \n       One or More Intermediate\n      \n      \n       Root certificate\n      \n     \n     \n      If you have to assemble a certificate from multiple parts, please be aware that this is the recommended ordering; however I don't think it will cause an error if you get the ordering wrong.\n     \n     \n      Once your certificates are in order, you can verify the key with the following commands:\n     \n     openssl rsa -check -noout -in wildcard.secret.key\nopenssl rsa -check -noout -in root.secret.key\n\n     \n      Both should say:\n      \n       RSA key is okay\n      \n     \n     \n      Now compare the modulus of the key and the cert:\n     \n     # These two should match\nopenssl rsa -modulus -noout -in wildcard.secret.key | openssl md5\nopenssl x509 -modulus -noout -in wildcard.cer | openssl md5\n\n# And these two should match\nopenssl rsa -modulus -noout -in root.secret.key | openssl md5\nopenssl x509 -modulus -noout -in root.cer | openssl md5\n\n     \n      If the modulus doesn't match, it may be because the server certificate isn't the first certificate in the .cer file.  Make sure the order is correct and try again.\n     \n     \n     \n     \n      Converting pfx format files\n     \n     \n      We have received files in pfx format instead of pem and these require special handling.  Follow the following directions to convert them to usable cer and key files, then use the following commands:\n     \n     # Assuming we have files wildcard.pfx and root.pfx\n#\n# Note: The --legacy option seems to be needed for most people, however\n#       some are able to do this without --legacy ... you can try without\n#       it first if you want.\n#\n# You will be asked for an \"Import Password\" -- just hit enter to skip that\n# If you get an error after the Import Password, you need --legacy\n\nopenssl pkcs12 -in wildcard.pfx -cacerts -out wildcard_ca.cer -nodes -nokeys --legacy\nopenssl pkcs12 -in root.pfx -cacerts -out root_ca.cer -nodes -nokeys --legacy\n\n     \n      Edit the wildcard.cer and root.cer files, and remove the header above\n      \n       -----BEGIN CERTIFICATE-----\n      \n      .  This header will resemble this:\n     \n     Bag Attributes: &lt;No Attributes&gt;\nsubject=C=US, O=DigiCert Inc, OU=www.digicert.com, CN=Thawte TLS RSA CA G1\nissuer=C=US, O=DigiCert Inc, OU=www.digicert.com, CN=DigiCert Global Root G2\n\n     \n      \n       WARNING: Check the ENTIRE file, as there will probably be multiple of the headers.  Any text not between\n       \n        -----BEGIN CERTIFICATE-----\n       \n       and\n       \n        -----END CERTIFICATE-----\n       \n       must be removed!\n      \n     \n     \n      Next, you need to extract the server certs, thusly:\n     \n     # See notes above regarding --legacy and \"Import Password\"\n\nopenssl pkcs12 -in wildcard.pfx -clcerts -nokeys -out wildcard.single.cer --legacy\nopenssl pkcs12 -in root.pfx -clcerts -nokeys -out wildcard.single.cer --legacy\n\n     \n      Once again, delete the header(s) above\n      \n       -----BEGIN CERTIFICATE-----\n      \n      in these files.  Afterwards, run the following command:\n     \n     cat wildcard.single.cer wildcard_ca.cer &gt; wildcard.cer\ncat root.single.cer root_ca.cer &gt; root.cer\n\n     \n      Now we're going to generate the private keys.  When generating the private keys, set a temporary password (just the word\n      \n       password\n      \n      is fine); we will remove the password in the subsequent step.\n     \n     # See notes above regarding --legacy and \"Import Password\"\nopenssl pkcs12 -in wildcard.pfx -nocerts -out wildcard.secrets.withpass.key --legacy\nopenssl pkcs12 -in root.pfx -nocerts -out root.secrets.withpass.key --legacy\n\n     \n      And finally, strip the passwords out for the final key files:\n     \n     openssl rsa -in wildcard.secrets.withpass.key -out wildcard.secret.key\nopenssl rsa -in root.secrets.withpass.key -out root.secret.key\n\n     \n      Now you have the files in PEM format, and you can go back to the section above to verify them.\n     \n     \n     \n     \n      Step 2: Update Cluster\n     \n     \n      This step may vary from customer to customer, so see the appropriate subsection.\n     \n     \n     \n     \n      Orrum\n     \n     \n      First, make sure you have the configuration repository checked out.  In your\n      \n       config\n      \n      directory, clone it thusly:\n     \n     git clone https://github.com/datacoves/config-datacoves-orrum.git datacoves.orrum.com\n\n     \n      In the\n      \n       datacoves.orrum.com\n      \n      directory, reveal the secrets.  If you call this command within a sub directory, you'll get an error about\n      \n       core-api.env.secret\n      \n      cannot be found.\n     \n     git secret reveal -f\n\n     \n      TODO: add instructions for setting up git secret\n     \n     \n      Then in the\n      \n       base\n      \n      directory you will find\n      \n       root.cer\n      \n      ,\n      \n       root.secret.key\n      \n      ,\n      \n       wildcard.cer\n      \n      , and\n      \n       wildcard.secret.key\n      \n      .  Replace these files with the new, verified files from step 1.\n     \n     \n      Connect to the Orrum VPN.  Instructions are here: https://github.com/datacoves/datacoves/tree/main/docs/client-docs/orrum\n     \n     \n      Make sure you are in your Orrum context, whatever that is named:\n     \n     # Use:\n# kubectl config get-contexts\n# To get context list if needed.\nkubectl config use-context orrum_new\n\n     \n      Then run setup base.  Return to the root directory of your git checkout to run\n      \n       cli.py\n      \n      thusly:\n     \n     # Activate your venv first if necessary\n./cli.py setup_base\n\n     \n      After the cluster is updated (ingress will be updated), check the certificate:\n     \n     curl https://api.datacoves.orrum.com -vI\n\n     \n      This should output a bunch of information about the certificate, including:\n     \n     * Server certificate:\n*  subject: CN=*.datacoves.orrum.com\n*  start date: Apr  8 07:33:48 2024 GMT\n*  expire date: Jul  1 07:33:47 2024 GMT\n*  subjectAltName: host \"api.datacoves.orrum.com\" matched cert's \"*.datacoves.orrum.com\"\n*  issuer: C=US; O=DigiCert Inc; OU=www.digicert.com; CN=Thawte TLS RSA CA G1\n*  SSL certificate verify ok.\n\n     \n      (The CN should be the correct host, and the expire date should be correct).\n     \n     \n      Check the non-wildcard version as well:\n     \n     curl https://datacoves.orrum.com -vI\n\n     \n      Log into Orrum's launchpad and go into one of the environments to make sure pomerium doesn't have any issues; pomerium is particularly sensitive to certificate problems such as not having the full certificate chain in the root.cer / wildcard.cer files.\n     \n     \n      If everything works alright, let's push the secrets.  Be careful to not push up the key files as they will show up as \"Untracked Files\" in a\n      \n       git status\n      \n      .  It is recommended you manually add the files thusly:\n     \n     # Go back to the config directory\ncd config/datacoves.orrum.com\n\n# See what files changed\ngit status\n\n# Add only the changed files, do NOT add the .key files or the original .pfx\ngit add .gitsecret/paths/mapping.cfg base/root.cer base/wildcard.cer secrets/core-api.env.secret secrets/docker-config.secret.json.secret secrets/rabbitmq.env.secert\n\n# You can also add any other safe file that you modified, just not those keys!\n\ngit commit -m \"Update certificates\"\ngit push\n\n     \n      And it should be done!"},"58":{"url":"/docs_output/how-tos/upgrade-dbt-or-related-tools.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      How to upgrade dbt or related tools\n     \n     \n     \n     \n      dbt-coves\n     \n     \n      \n       Pull Request on dbt-coves and merge. This will deploy a new pypi version\n      \n     \n     \n     \n     \n      All libraries\n     \n     \n      \n       Get current version of new libraries\n      \n      \n       Upgrade code-server (src/code-server/code-server) docker image requirements.txt and labels\n      \n      \n       Upgrade ci images libraries: ci/airflow and ci/basic, update labels.\n      \n      \n       Upgrade airflow image libraries, install the new libraries in the environment targeted for dag runs, update labels accordingly.\n      \n      \n       Run script that updates labels on docker files"},"59":{"url":"/docs_output/how-tos/work-on-a-pre-release-locally.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      Make and work on a pre-release locally\n     \n     \n      Sometimes you need to change images and test them locally without affecting production releases.\n     \n     \n      To do so:\n     \n     \n     \n     \n      Build the image you just changed\n     \n     ./cli.py build_and_push &lt;path to service&gt;  # i.e. src/core/api\n\n     \n      You'll need to specify the issue #\n     \n     \n      This command will build and push a new image prefixing its name with the ticket number your provided.\n     \n     \n     \n     \n      Generate the pre-release\n     \n     \n      Once the image was pushed, you can create a new pre-release to try that image:\n     \n     ./cli.py generate_release\n\n     \n      This will create a new release file under /releases and will also be pushed to GitHub releases so other devs can reuse it.\n     \n     \n     \n     \n      Set the pre-release on datacoveslocal.com cluster\n     \n     ./cli.py set_release\n\n     \n      Select\n      \n       datacoveslocal.com\n      \n      .\n     \n     \n      You might need to undo the file changes before pushing to PR branch.\n     \n     \n     \n     \n      Upgrade datacoves in local cluster\n     \n     ./cli.py install\n\n     \n      Select\n      \n       datacoveslocal.com"},"60":{"url":"/docs_output/implementation/","snip":"Edit on github"},"61":{"url":"/docs_output/implementation/operator.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      Operator documentation\n     \n     \n     \n     \n      Overview\n     \n     \n      The datacoves\n      \n       operator\n      \n      is a kubernetes\n      \n       controller\n      \n      , written in go,\nscaffolded using\n      \n       kubebuilder\n      \n      . It is responsible for setting up and\nmanaging the kubernetes resources that make up a\n      \n       workspace\n      \n      (a.k.a. an\n      \n       environment\n      \n      ). Each workspace has its own k8s namespace. The operator's source\ncode is in\n      \n       src/core/operator/\n      \n      .\n     \n     \n      The operator watches a few custom resources that specify what to set up. They\nare defined in\n      \n       api/v1/\n      \n      .\n     \n     \n      \n       \n        \n         Workspace\n        \n        : The main resource, fully describing a workspace. Parts of the\nconfiguration are held in other resources, but the workspace references them all\nand is the root of the configuration. Whenever a change to a model in the core\napi database impacts a workspace configuration, the core-api's workspace.sync\ntask recomputes and (re-)writes the corresponding workspace k8s resource. The\noperator detects the resource update and runs the reconciliation process to\napply any required changes to the kubernetes resources that compose the workspace.\n       \n      \n      \n       \n        \n         User\n        \n        : Each workspace has a set of users, and each user gets certain resources,\nsuch as a code-server deployment.\n       \n      \n      \n       \n        \n         HelmRelease\n        \n        : Most services set up by the operator are installed using helm.\nA HelmRelease specifies that a helm chart should be installed, using a certain\nversion and helm values.\n       \n      \n     \n     \n     \n     \n      Background\n     \n     \n      Some useful background knowledge to have and resources to review:\n     \n     \n     \n     \n      Go\n     \n     \n      \n       The\n       \n        go spec\n       \n       is short, readable and precise. Use it.\n      \n      \n       \n        Effective go\n       \n       and the\n       \n        go FAQ\n       \n       .\n      \n      \n       Understanding go's concurrency constructs. CSP, goroutines and channels.\n      \n      \n       Understanding that go (like C) is pass by value, so the distinction between\n  struct types and pointers to structs is often important.\n      \n      \n       Understanding that errors are values in go.\n      \n      \n       Understanding the\n       \n        context\n       \n       package.\n      \n      \n       \n        How controller-runtime's does logging\n       \n       .\n      \n     \n     \n     \n     \n      Kubernetes\n     \n     \n      \n       \n        API concepts\n       \n      \n      \n       \n        API conventions\n       \n      \n      \n       \n        The kubebuilder book\n       \n      \n      \n       Understand resourceVersion and generation.\n      \n      \n       Understand ownerReferences and finalizers.\n      \n     \n     \n     \n     \n      Implementation: Reconcilers\n     \n     \n     \n     \n      Change detection and reconciliation\n     \n     \n      The entry points to our code are the\n      \n       Reconcile\n      \n      methods for each resource, in\n      \n       controllers/*_controller.go\n      \n      . The framework\n      \n       watches\n      \n      kubernetes resources to\ndetermine when to call\n      \n       Reconcile\n      \n      . The\n      \n       SetupWithManager\n      \n      method can be used\nto influence when\n      \n       Reconcile\n      \n      should be called.\n     \n     \n      Reconciliation must be idempotent. If an error is returned, or there's a panic,\nthe framework will retry calling\n      \n       Reconcile\n      \n      repeatedly, less frequently each\ntime.\n     \n     \n      To simplify change detection and ensure deployments are restarted when a secret\nor configmap that affects them changes, we treat secrets and configmaps as\nimmutable values. We include a hash of their contents in their names. This means\nto start using the new version references to them must be updated. This implies\nthat resources using them will change too, which means all changes can be detected\nby watching the resource that has the reference, without checking the contents\nof the secret or configmap.\n     \n     \n     \n     \n      Applying changes to derived resources\n     \n     \n      Reconciliation is conceptualy stateless. We compute a set of derived resources\nfrom the current value of the Workspace resource. We would like to have a\nprimitive that is the equivalent of\n      \n       kubectl apply\n      \n      in our go code. Unfortunately\nreusing that mechanism is/was not available when writing the operator so we had\nto build our own resource diffing. These are the\n      \n       reconcile*\n      \n      functions in\n      \n       controllers/reconcilers.go\n      \n      .\n     \n     \n     \n     \n      Concurrency\n     \n     \n      The framework runs\n      \n       Reconcile\n      \n      concurrently for different resource types. It also\nruns the reconciliation for different resources concurrently, at most\n      \n       MaxConcurrentReconciles\n      \n      at once. Reconciliation of multiple changes to a single resource happens serially.\n     \n     \n      We take advantage of this fact to isolate failures. The Workspace reconciler\napplies changes to HelmRelease and User resources. This way the reconciliaton of\na HelmRelease or a User failing won't make the whole Workspace reconciliation fail.\n     \n     \n     \n     \n      Implementation: Helm runner\n     \n     \n      Before having the\n      \n       helm\n      \n      module carry out the installation of helm charts by\nstarting helm subprocesses we used to call into helm's go code directly from\nthe helmrelease controller. This caused two problems:\n     \n     \n      \n       When the operator was restarted the helm release (stored by helm in a k8s secret)\n  could be left in a pending-upgrade state, which should only happen if helm is\n  still running. This is due to helm not cleaning up when interrupted.\n      \n      \n       We run out of memory, most likely due to a memory leak involving helm state.\n      \n     \n     \n      To address these issues we implemented the\n      \n       helm\n      \n      module, which schedules helm\nsupbrocesses so that we can control their execution. It is a separate module\nthat runs a singleton scheduler process and receives requests to run helm over a\nchannel. The helmrelease_controller simply sends requests to this process\nwithout waiting or checking results.\n     \n     \n      Currently helm install failures will be logged but won't be retried. Manual\nintervention is required in this case. In any case, retrying the whole helm\ninstall is unlikely to succeed if nothing changed. Certain kinds of intermitent\nfailures could be detected and retried within an operation if desired. But in\nthis case, not retrying the helmrelease reconciliation as a whole is best, I think.\n     \n     \n      The meat of the implementation is in the\n      \n       run\n      \n      function. It keeps track of\nrunning and pending operations (and their potential memory usage) and spawns new\ngoroutines for each install/upgrade/uninstall operation. It is somewhat subtle\ncode. You should understand goroutines and channels well before touching it.\n     \n     \n      When the operator is signaled by kubernetes to exit, we must be as gentle as\npossible with helm subprocesses to avoid leaving the releases in a bad state.\nThere's a grace period between the first signal that the program will exit\nand forceful termination. We use it to send SIGTERM to all the helm subprocesses,\nwhich should allow them to exit more cleanly than if they were SIGKILLed. We\nhaven't seen any more chart's left in\n      \n       pending-upgrade\n      \n      after this change."},"62":{"url":"/docs_output/","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      config\n     \n     \n      This directory holds configuration, organized by the cluster's domain name.\n     \n     \n      Most sudirectories are git submodules, to keep their configuration separate.\n     \n     \n      Every cluster configuration directory can have the following subdirectories:\n     \n     \n      \n       base: Kustomize directory for the kubernetes cluster global components and configuration.\n      \n      \n       kind: Configuration to create a kind cluster.\n      \n      \n       eks: Configuration to create an eks cluster.\n      \n      \n       cluster-params.yaml, cluster-params.secret.yaml: Cluster configuration.\n      \n      \n       secrets: Secrets, those that are not specific to an environment.\n      \n      \n       environments: Environment configurations, with one subdirectory per environment.\n      \n     \n     \n      The datacoveslocal.com cluster, for example, looks like this:\n     \n     config/\n├── datacoveslocal.com/\n│   ├── base/\n│   ├── environments/\n│   ├── kind/\n│   ├── secrets/\n│   ├── cluster-params.secret.yaml\n│   ├── cluster-params.secret.yaml.secret\n│   └── cluster-params.yaml\n...\n\n     \n     \n     \n      docs\n     \n     \n      Documentation.\n     \n     docs\n├── client-docs                            For clients.\n│   ├── jnj\n│   └──  ...\n├── how-tos                                For devops. How to do certain things.\n│   ├── do-thing-x\n│   └──  ...\n├── dev-logs                               Developer logs. Record something you did for future reference. Be careful not to include secrets.\n│   ├── 2021-09-eks-setup.md\n│   └── ...\n├── issues-resolutions                     For support team. How to solve common user issues\n│\n└── ...\n\n     \n     \n     \n      scripts\n     \n     \n      Python scripts to manage the project. Usually called by ./cli.py commands.\n     \n     \n     \n     \n      src\n     \n     \n      Datacoves source code and docker image definitions. The core components are in\n      \n       src/core\n      \n      ."},"63":{"url":"/docs_output/issues-resolutions/airflow-corrupted-dag-logs.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      DAG logs were serialized with a newer version of pickle than the installed on Airflow webserver\n     \n     \n     \n     \n      Logs\n     \n     Traceback (most recent call last):\n  File \"/home/airflow/.local/bin/airflow\", line 8, in &lt;module&gt;\n    sys.exit(main())\n  File \"/home/airflow/.local/lib/python3.7/site-packages/airflow/__main__.py\", line 38, in main\n    args.func(args)\n  File \"/home/airflow/.local/lib/python3.7/site-packages/airflow/cli/cli_parser.py\", line 51, in command\n    return func(*args, **kwargs)\n  File \"/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/cli.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"/home/airflow/.local/lib/python3.7/site-packages/airflow/cli/commands/scheduler_command.py\", line 75, in scheduler\n    _run_scheduler_job(args=args)\n  File \"/home/airflow/.local/lib/python3.7/site-packages/airflow/cli/commands/scheduler_command.py\", line 46, in _run_scheduler_job\n    job.run()\n  File \"/home/airflow/.local/lib/python3.7/site-packages/airflow/jobs/base_job.py\", line 244, in run\n    self._execute()\n  File \"/home/airflow/.local/lib/python3.7/site-packages/airflow/jobs/scheduler_job.py\", line 739, in _execute\n    self._run_scheduler_loop()\n  File \"/home/airflow/.local/lib/python3.7/site-packages/airflow/jobs/scheduler_job.py\", line 827, in _run_scheduler_loop\n    num_queued_tis = self._do_scheduling(session)\n  File \"/home/airflow/.local/lib/python3.7/site-packages/airflow/jobs/scheduler_job.py\", line 909, in _do_scheduling\n    callback_to_run = self._schedule_dag_run(dag_run, session)\n  File \"/home/airflow/.local/lib/python3.7/site-packages/airflow/jobs/scheduler_job.py\", line 1151, in _schedule_dag_run\n    schedulable_tis, callback_to_run = dag_run.update_state(session=session, execute_callbacks=False)\n  File \"/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py\", line 68, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagrun.py\", line 522, in update_state\n    info = self.task_instance_scheduling_decisions(session)\n  File \"/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py\", line 68, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagrun.py\", line 640, in task_instance_scheduling_decisions\n    tis = list(self.get_task_instances(session=session, state=State.task_states))\n  File \"/home/airflow/.local/lib/python3.7/site-packages/airflow/utils/session.py\", line 68, in wrapper\n    return func(*args, **kwargs)\n  File \"/home/airflow/.local/lib/python3.7/site-packages/airflow/models/dagrun.py\", line 441, in get_task_instances\n    return tis.all()\n  File \"/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/query.py\", line 2683, in all\n    return self._iter().all()\n  File \"/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/result.py\", line 1335, in all\n    return self._allrows()\n  File \"/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/result.py\", line 408, in _allrows\n    rows = self._fetchall_impl()\n  File \"/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/result.py\", line 1243, in _fetchall_impl\n    return self._real_result._fetchall_impl()\n  File \"/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/result.py\", line 1636, in _fetchall_impl\n    return list(self.iterator)\n  File \"/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/orm/loading.py\", line 120, in chunks\n    fetch = cursor._raw_all_rows()\n  File \"/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/result.py\", line 400, in _raw_all_rows\n    return [make_row(row) for row in rows]\n  File \"/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/engine/result.py\", line 400, in &lt;listcomp&gt;\n    return [make_row(row) for row in rows]\n  File \"/home/airflow/.local/lib/python3.7/site-packages/sqlalchemy/sql/sqltypes.py\", line 1816, in process\n    return loads(value)\n  File \"/home/airflow/.local/lib/python3.7/site-packages/dill/_dill.py\", line 275, in loads\n    return load(file, ignore, **kwds)\n  File \"/home/airflow/.local/lib/python3.7/site-packages/dill/_dill.py\", line 270, in load\n    return Unpickler(file, ignore=ignore, **kwds).load()\n  File \"/home/airflow/.local/lib/python3.7/site-packages/dill/_dill.py\", line 472, in load\n    obj = StockUnpickler.load(self)\nValueError: unsupported pickle protocol: 5\n\n     \n     \n     \n      Solution\n     \n     \n      Connect to scheduler or triggerer pod and then remove DAG by running:\n     \n     airflow dags delete &lt;dag id&gt;"},"64":{"url":"/docs_output/issues-resolutions/dbt-core-debugging.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      Python dbt-core debugging\n     \n     \n     \n     \n      Context: dbt does not respond to any of it's commands\n     \n     \n      Due to changes in environment variable handling on dbt-core side, a read-only\n      \n       $DBT_PROJECT_DIR\n      \n      led to dbt not responding to anything but the\n      \n       --version\n      \n      call.\n     \n     \n      \n     \n     \n      All dbt commands returned\n      \n       exit code 2\n      \n     \n     2 The dbt invocation completed with an unhandled error (eg. ctrl-c, network interruption, etc).\n\n     \n     \n     \n      Solution\n     \n     \n      Using dbt-core python library and it's\n      \n       dbtRunner\n      \n      gives us the possibility to receive that\n      \n       \"unhandled error\"\n      \n     \n     &gt;&gt;&gt; from dbt.cli.main import dbtRunner\n&gt;&gt;&gt; dbt_cli = dbtRunner()\n&gt;&gt;&gt; dbt_cli.invoke([\"ls\"])\ndbtRunnerResult(success=False, exception=OSError(30, 'Read-only file system'), result=None)"},"65":{"url":"/docs_output/issues-resolutions/docker-image-debugging.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      Debugging images outside Datacoves.\n     \n     \n      Sometimes we need to review images that are running in Datacoves in a simpler way to debug processes, review the versions of libraries, versions of pipelines, etc.\n     \n     \n      \n       Create\n       \n        compose.yaml\n       \n       or\n       \n        docker-compose.yaml\n       \n       file\n      \n     \n     version: '3'\n\nservices:\n  snowflake:\n    image: \"taqy-docker.artifactrepo.jnj.com/datacoves/ci-basic-dbt-snowflake:3.1\"\n    command: bash -c \"sleep infinity\"\n\n     \n      \n       Run commands\n      \n     \n     docker compose run --rm snowflake bash -c \"pip show dbt-core dbt-snowflake\"\n\n     \n      \n       Get a terminal\n      \n     \n     docker compose up -d\ndocker ps\ndocker exec -ti &lt;container-id&gt; /bin/bash"},"66":{"url":"/docs_output/issues-resolutions/docker-push-stopped-working.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      Reset docker config authentication\n     \n     \n      If that was the case, you might need to log in and log out again after a password reset:\n     \n     docker logout\n\n     \n      Then, remove the entry for taqy-docker.artifactrepo.jnj.com in\n      \n       ~/.docker/config.json\n      \n      .\n     \n     \n      Finally, login again:\n     \n     docker login taqy-docker.artifactrepo.jnj.com\n\n     \n     \n     \n      Unlock your artifactory account\n     \n     \n      Sometimes your account can get blocked and you need to unlock it.\n     \n     \n      \n       Go to\n       \n        appdevtools\n       \n      \n      \n       Under support, user acces, click on\n       \n        Unlock Artifactory Account\n       \n       ."},"67":{"url":"/docs_output/issues-resolutions/helm-chart.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      Helm Chart Resolutions\n     \n     \n     \n     \n      How to patch releases?\n     \n     \n      Sometimes we want to change a value in the\n      \n       Helm Chart\n      \n      , but to do this we need to edit some component such as an\n      \n       adapter\n      \n      or the\n      \n       Operator\n      \n      and generate a new release, so this functionality is very useful to be able to skip that whole process and do our tests more quickly.\n     \n     \n     \n     \n      Option No.1\n     \n     \n      \n       Get the values from the release.\n      \n     \n     # helm get values &lt;release&gt; -n &lt;namespace&gt;\nhelm get values dev123-datahub -n dcw-dev123 &gt; values.yaml\n\n     \n      \n       Edit/add the values to the file.\n      \n     \n     vi values.yaml\n\n     \n      \n       Add the repository if does not exists.\n      \n     \n     # helm repo add &lt;name&gt; &lt;url&gt;\nhelm repo add datahub https://helm.datahubproject.io/\n\n     \n      \n       Patch the helm chart.\n      \n     \n     # helm upgrade --version &lt;x.x.x&gt; -f values.yaml &lt;release&gt; &lt;repository&gt; -n &lt;namespace&gt;\nhelm upgrade --version 0.4.16 -f values.yaml dev123-datahub datahub/datahub -n dcw-dev123\n\n     \n     \n     \n      Option No.2\n     \n     \n      \n       Patch the helm chart.\n      \n     \n     # helm upgrade &lt;release&gt; &lt;chart&gt; -n &lt;namespace&gt; --set key1=value1,key2=value2\nhelm upgrade dev123-datahub datahub/datahub -n dcw-dev123 --set key1=value1,key2=value2\n\n     \n      \n       More info"},"68":{"url":"/docs_output/issues-resolutions/pomerium-not-allowing-access.html","snip":"Edit on github\n       \n      \n     \n     \n     \n     \n      Pomerium does not allow access to environments\n     \n     \n     \n     \n      Problem\n     \n     \n      Launchapd works OK, but pomerium returning timeout, logs like these are found:\n     \n     {\"level\":\"info\",\"X-Forwarded-For\":[\"10.255.255.2,10.10.0.8\"],\"X-Forwarded-Host\":[\"authenticate-dev123.orrum.datacoves.com\"],\"X-Forwarded-Port\":[\"443\"],\"X-Forwarded-Proto\":[\"http\"],\"X-Real-Ip\":[\"10.255.255.2\"],\"ip\":\"127.0.0.1\",\"user_agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\",\"request-id\":\"834a4284-9d39-474a-abb5-cd7203755386\",\"error\":\"Bad Request: internal/sessions: session is not found\",\"time\":\"2023-08-17T13:13:39Z\",\"message\":\"authenticate: session load error\"}\n{\"level\":\"info\",\"service\":\"envoy\",\"upstream-cluster\":\"pomerium-control-plane-http\",\"method\":\"GET\",\"authority\":\"authenticate-dev123.orrum.datacoves.com\",\"path\":\"/.pomerium\",\"user-agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36\",\"referer\":\"\",\"forwarded-for\":\"10.255.255.2,10.10.0.8\",\"request-id\":\"834a4284-9d39-474a-abb5-cd7203755386\",\"duration\":15000.251354,\"size\":24,\"response-code\":504,\"response-code-details\":\"upstream_response_timeout\",\"time\":\"2023-08-17T13:13:55Z\",\"message\":\"http-request\"}\n\n     \n     \n     \n      Cause\n     \n     \n      This is a DNS resolution issue that pomerium is having. Typically this happens when the cluster model has wrong values on\n      \n       internal_ip\n      \n      or\n      \n       external_ip\n      \n      .\nThis could have happened when the DB was copied to a different cluster, of the cluster changed their IPs.\n     \n     \n     \n     \n      Solution\n     \n     \n      Remove the values on those 2 fields and save the cluster model again. On\n      \n       save\n      \n      , it will regenerate those IPs and Pomerium will be reinstalled."},"69":{"url":"/index.template.html","snip":"Edit on github\n                        \n                    \n                    @content"}},"dirtCount":0,"index":[["└──",{"0":{"62":5}}],["│",{"0":{"62":14}}],["├──",{"0":{"62":14}}],["zookeeper",{"0":{"49":1}}],["zip",{"0":{"12":1}}],["`t",{"0":{"29":1}}],["```",{"0":{"46":1}}],["```boost",{"0":{"46":1}}],["``",{"0":{"29":3}}],["=~",{"0":{"29":2}}],["=",{"0":{"29":4,"30":3,"43":1,"54":1,"63":8,"64":1}}],["quickly",{"0":{"67":1}}],["quite",{"0":{"56":1}}],["queued",{"0":{"46":1,"63":1}}],["questions",{"0":{"44":1}}],["queries",{"0":{"43":1}}],["queryparam",{"0":{"55":1}}],["query",{"0":{"25":1,"29":3,"45":1,"63":1}}],["q",{"0":{"22":1}}],["739",{"0":{"63":1}}],["75",{"0":{"63":1}}],["75567b8958",{"0":{"54":1}}],["7c",{"0":{"57":1}}],["7ca1",{"0":{"56":1}}],["78",{"0":{"57":1}}],["78cc7cfb6",{"0":{"15":1}}],["7f",{"0":{"57":1}}],["76",{"0":{"57":1}}],["7b7rx",{"0":{"54":1}}],["7ba64ac2",{"0":{"26":1}}],["7d35",{"0":{"45":1}}],["7",{"0":{"23":1,"24":1,"28":1,"42":1,"63":28,"68":2}}],["79",{"0":{"4":1}}],["55z",{"0":{"68":1}}],["551241293703",{"0":{"26":1}}],["504",{"0":{"68":1}}],["522",{"0":{"63":1}}],["524288",{"0":{"46":1}}],["51",{"0":{"63":1}}],["512",{"0":{"0":1,"4":2}}],["57",{"0":{"57":1}}],["5b",{"0":{"57":1}}],["59",{"0":{"57":4}}],["5m",{"0":{"43":1}}],["537",{"0":{"68":4}}],["53",{"0":{"27":1}}],["56",{"0":{"23":1,"25":1}}],["5f8f64cf69",{"0":{"15":1}}],["5",{"0":{"11":1,"14":1,"18":1,"22":1,"25":1,"26":2,"28":1,"42":1,"63":1,"68":2}}],["|=",{"0":{"29":5}}],["|",{"0":{"6":1,"8":2,"14":4,"27":1,"38":3,"41":2,"45":1,"46":4,"49":5,"57":6}}],["$dbt",{"0":{"64":1}}],["$user",{"0":{"46":1,"56":4}}],["$pod",{"0":{"38":1}}],["$pods",{"0":{"38":1}}],["$project",{"0":{"15":2}}],["$8",{"0":{"38":1}}],["$ns",{"0":{"38":2}}],["$api",{"0":{"23":1}}],["$kctx",{"0":{"15":3}}],["$kubectl",{"0":{"6":2,"17":2}}],["$1",{"0":{"14":2,"38":1}}],["$",{"0":{"14":2,"15":4,"24":3,"30":1,"56":2}}],["$editor",{"0":{"6":1}}],["$cluster",{"0":{"3":2,"6":3,"15":3,"17":3,"39":2}}],["+x",{"0":{"26":1}}],["+",{"0":{"5":1,"12":1,"40":1,"43":2,"56":2}}],["+how+to+guides",{"0":{"1":1}}],["+how+to+request+access+",{"0":{"1":1}}],["~$",{"0":{"46":3}}],["~",{"0":{"4":4,"26":1,"56":1,"66":1}}],["834a4284",{"0":{"68":2}}],["88",{"0":{"57":1}}],["8a",{"0":{"57":1}}],["8a2b",{"0":{"26":1}}],["8b4b",{"0":{"45":2}}],["8",{"0":{"28":3,"42":1,"57":1,"63":1,"68":2}}],["86ad",{"0":{"20":1,"45":1}}],["8653",{"0":{"4":1}}],["8001",{"0":{"14":2}}],["827",{"0":{"63":1}}],["8231",{"0":{"45":2}}],["82",{"0":{"4":2,"56":1}}],["xhg2vb7t14+hbmutrscuv",{"0":{"57":1}}],["xxx",{"0":{"49":12}}],["xenial",{"0":{"46":1}}],["x509",{"0":{"35":1,"57":2}}],["xzvf",{"0":{"26":1}}],["xlarge",{"0":{"9":1}}],["x",{"0":{"4":2,"13":1,"28":2,"62":1,"67":3,"68":7}}],["39z",{"0":{"68":1}}],["33",{"0":{"57":2}}],["3a",{"0":{"57":1}}],["34",{"0":{"57":1}}],["34234523452524",{"0":{"56":1}}],["3268",{"0":{"39":1}}],["327112934799",{"0":{"2":1}}],["31",{"0":{"27":2}}],["38",{"0":{"23":1,"63":1}}],["3d19h",{"0":{"15":1}}],["36",{"0":{"12":13,"57":1,"68":4}}],["3",{"0":{"3":1,"5":1,"11":4,"25":6,"28":1,"37":1,"40":1,"42":1,"47":2,"56":2,"57":4,"65":2}}],["3099b8af",{"0":{"56":1}}],["30",{"0":{"0":1,"9":3,"32":5,"64":1}}],["utils",{"0":{"63":4}}],["utc",{"0":{"51":1}}],["ultimately",{"0":{"57":1}}],["ui",{"0":{"22":4,"23":3,"49":1}}],["u",{"0":{"12":1,"18":3}}],["url=",{"0":{"54":1}}],["url",{"0":{"6":3,"47":1,"52":1,"67":1}}],["urls",{"0":{"2":1,"40":1}}],["urlsafe",{"0":{"2":1}}],["unhandled",{"0":{"64":2}}],["unsupported",{"0":{"63":1}}],["unpickler",{"0":{"63":1}}],["unlock",{"0":{"66":3}}],["unlikely",{"0":{"61":1}}],["unless",{"0":{"22":1}}],["unfortunately",{"0":{"61":1}}],["undo",{"0":{"59":1}}],["understand",{"0":{"61":3}}],["understanding",{"0":{"61":4}}],["under",{"0":{"2":1,"3":4,"4":1,"10":1,"15":1,"17":1,"40":1,"47":1,"56":1,"57":1,"59":1,"66":1}}],["untracked",{"0":{"57":1}}],["until",{"0":{"7":1,"22":1,"40":1,"49":1,"56":1}}],["unable",{"0":{"56":1}}],["unavailable",{"0":{"56":2}}],["unbounded",{"0":{"45":1}}],["unused",{"0":{"23":4}}],["uncheck",{"0":{"22":1,"40":1}}],["unrestricted",{"0":{"5":1}}],["uninstall",{"0":{"4":1,"61":1}}],["unixseadmins",{"0":{"4":1}}],["upstream",{"0":{"68":2}}],["upgrading",{"0":{"57":1}}],["upgrades",{"0":{"28":1}}],["upgrade",{"0":{"28":2,"46":2,"47":1,"56":10,"58":4,"59":1,"61":3,"67":4}}],["upper",{"0":{"56":2}}],["uptime",{"0":{"23":2}}],["upon",{"0":{"22":1}}],["updating",{"0":{"12":1,"56":7}}],["updates",{"0":{"22":1,"28":1,"58":1}}],["updated",{"0":{"3":1,"16":1,"17":1,"47":1,"57":2,"61":1}}],["update",{"0":{"3":3,"6":4,"17":1,"18":1,"20":1,"22":1,"28":1,"39":2,"46":6,"56":10,"57":2,"58":2,"61":1,"63":2}}],["upload",{"0":{"2":1,"11":2}}],["up",{"0":{"2":1,"5":1,"11":1,"14":1,"18":1,"22":1,"26":1,"34":1,"40":1,"49":1,"52":1,"56":9,"57":3,"61":5,"65":1}}],["usable",{"0":{"57":1}}],["usage",{"0":{"14":3,"23":2,"43":3,"61":1}}],["usr",{"0":{"46":3}}],["usually",{"0":{"2":1,"3":1,"6":1,"22":1,"35":2,"62":1}}],["us",{"0":{"2":4,"18":2,"20":6,"26":1,"33":1,"45":1,"47":1,"56":3,"64":1}}],["uses",{"0":{"49":1}}],["useful",{"0":{"3":1,"23":1,"43":1,"61":1,"67":1}}],["used",{"0":{"2":3,"3":1,"5":1,"7":1,"15":2,"18":1,"22":1,"25":2,"28":1,"40":3,"52":1,"61":2}}],["use",{"0":{"0":2,"2":3,"4":1,"5":1,"6":1,"7":1,"8":2,"10":1,"12":4,"15":1,"22":2,"24":1,"25":1,"28":2,"34":2,"35":2,"40":1,"44":2,"56":8,"57":3,"61":2}}],["usermod",{"0":{"46":1}}],["userguide",{"0":{"14":4}}],["username=",{"0":{"8":1,"14":1}}],["username",{"0":{"4":1,"7":1,"8":2}}],["users",{"0":{"2":1,"4":5,"22":2,"26":1,"42":1,"56":4,"61":1}}],["user",{"0":{"0":1,"2":12,"4":3,"5":1,"6":1,"7":3,"9":1,"11":2,"12":4,"14":3,"15":1,"17":1,"18":4,"20":2,"22":4,"23":2,"25":2,"26":6,"31":1,"37":1,"42":2,"44":5,"46":3,"53":2,"56":1,"61":4,"62":1,"66":1,"68":2}}],["using",{"0":{"0":2,"2":6,"7":1,"8":1,"15":1,"17":1,"18":2,"22":4,"23":1,"26":1,"30":1,"40":1,"42":1,"45":2,"47":1,"52":2,"53":1,"55":1,"56":3,"57":2,"61":5,"64":1}}],["9d39",{"0":{"68":2}}],["9ddkw",{"0":{"15":1}}],["909",{"0":{"63":1}}],["99",{"0":{"63":1}}],["9a",{"0":{"57":1}}],["94",{"0":{"57":1}}],["91vkolyvhsemuk5lukz1nixowvzhcf",{"0":{"57":1}}],["91bd2205",{"0":{"20":1,"45":1}}],["9pm",{"0":{"51":1}}],["9b8f",{"0":{"26":1}}],["9",{"0":{"2":1,"9":1,"24":3,"25":2,"26":2,"42":1}}],["jul",{"0":{"57":1}}],["just",{"0":{"22":2,"30":1,"39":1,"40":1,"47":1,"56":1,"57":3,"59":1}}],["jbszqlyzkyzbxq5eik3oundsb+n5zo+tshrrl45wa9fzmvaizzmtehxjwubidgl7",{"0":{"57":1}}],["jbfl",{"0":{"7":1}}],["jan",{"0":{"57":1}}],["janbgkqhkig9w0baqsfadcb",{"0":{"57":1}}],["janisdd",{"0":{"25":1}}],["journey",{"0":{"40":1}}],["job=",{"0":{"43":1}}],["jobs",{"0":{"42":3,"63":5}}],["job",{"0":{"4":2,"42":2,"54":1,"63":8}}],["jinjahtml",{"0":{"25":1}}],["jinja",{"0":{"25":2}}],["jira",{"0":{"5":3}}],["json=payload",{"0":{"54":1}}],["json",{"0":{"22":1,"27":1,"54":1,"56":2,"57":1,"66":1}}],["jfrog",{"0":{"11":5}}],["j",{"0":{"7":2,"44":2,"56":12}}],["jrddatacoves",{"0":{"7":2}}],["jenkinsfile",{"0":{"5":1,"42":1}}],["jenkins",{"0":{"2":2,"5":6,"7":3,"42":2}}],["jnjitod",{"0":{"1":1}}],["jnj",{"0":{"1":7,"2":15,"3":2,"4":3,"5":2,"6":2,"7":10,"8":3,"11":3,"15":8,"17":2,"24":1,"37":1,"39":8,"41":2,"42":1,"56":2,"62":1,"65":1,"66":2}}],["08",{"0":{"68":2}}],["07",{"0":{"57":2}}],["03",{"0":{"57":1}}],["0e",{"0":{"57":1}}],["06",{"0":{"57":1}}],["0d",{"0":{"57":1}}],["0d74",{"0":{"20":1,"45":1}}],["02",{"0":{"57":1}}],["09",{"0":{"57":4,"62":1}}],["05",{"0":{"57":1}}],["00",{"0":{"57":11}}],["00b264446505",{"0":{"26":2}}],["01",{"0":{"57":1}}],["0x2",{"0":{"57":4}}],["0f8e4c48",{"0":{"12":2}}],["0",{"0":{"2":3,"3":2,"12":13,"14":1,"15":2,"24":2,"25":11,"26":3,"27":1,"30":1,"43":1,"46":1,"49":3,"56":20,"67":1,"68":12}}],["ls",{"0":{"64":1}}],["ls0tls1crudjtibdrvjusuzjq0furs0tls0tck1jsum1ekndqwmrz0f3sujbz0lcqurbtkjna3foa2lhoxcwqkfrc0zbrefwtvjnd0vrwurwuvferxdwcmrxsmwky201bgrhvnpnqjryrfrjeu1uqxlovev4tlrnmu1gb1heve15tvrbeu1qrxhove0xtuzvd0zurvrnqkvhqtfvrqpbee1lytnwavpysnvawfjsy3pdq0ftsxdeuvlks29aswh2y05buuvcqlfbrgdnrvbbrendqvfvq2dnrujbt2jpcmfhoufvsdvlwgpmefdnqzbone5juhvqsvptnmplnmxbm29stvawuhyyd1hlalphcefsvnfowvdxchl3actzzm8kt1llr1nuc2hpde9dbnvyu094svhoy1bnr1zmn1revlzgbu04ww5kszbmohdlwmxlddniyu9owfjkeknzykjomgoydnpzsgx0zgrebhkvthpwawpnqlpnrhy1uutkeehnsef0aud6ag4xs2xvt2xkrgozv1lpv1vjv0ladzzhewv2cnnhym1rd3a1rejwqjbvn3v2bedmd1ruq3rzc3nhdni2ddz6mwtznhhnuumxvtlonulhv0uxdeurzgzwmmzzwdykz3d1c0teogneskfivmfrl2lwk3pkcxrxrnjhovfnedbeelpqyzrtu1dnvdzyvxzjbtlbbtlrmvnssxc5odlgraphelh6bgxqcxzyswnnu1rwsw9jq0f3rufbyu5dtuvbd0rnwurwujbqqvfil0jbuurbz0trtue4r0exvwrfd0vcci93uuznqu1cqwy4d0hrwurwujbpqkjzruzlnnjeexbrk3vreggxwu8zs0jkbmthyu1tnudnqtbhq1nxr1njyjmkrfffqkn3vufbnelcqvfcdk52clzjrjfaz1fdmznpbdzrr0gzchjjn3rwrmcvotf3uvnzzkm2sfm2cwriverucwpnyxhoeeyvblzzbfeykzrmn0uxvuzodudsouduzlvvs2fiqzb1cwx6bupqadjvuxjrz3hzqnd3egxtosszchjncnluogz5m29um21jawr0azzlsllicm5wzs9qznlwn1j5euhva0pvvgiwcwfvakxomvzhvfoyrmjlk0zjeg50shckdwj4bnlsmhzlcgexddfoovljndfjynfzugrbmvfdzvyvr1hndwn4z0u4bud1vfzqqlu1medybg1qwnrzvjg5dgp3tvpytvvobznmakdqnvvnmnlftmtxaw9ra2hqukrmruzgqxpzuzmrsu5twnawmklbutrrnknsynj0vmc5zdfrcky4d1fzaytjuxurmne3t25wous5cudyexdraknsd0ztv1n2uwotls0tluvorcbdrvjusuzjq0furs0tls0tcg==",{"0":{"26":1}}],["lcgdnbdjyb8nahyr",{"0":{"57":1}}],["ln",{"0":{"26":1,"46":2}}],["l",{"0":{"15":2}}],["led",{"0":{"64":1}}],["left",{"0":{"61":2}}],["legacy",{"0":{"57":11}}],["less",{"0":{"24":1,"61":1}}],["letter",{"0":{"24":1}}],["letsencrypt",{"0":{"35":2}}],["lets",{"0":{"21":1}}],["let",{"0":{"20":1,"34":1,"35":1,"57":2}}],["leaving",{"0":{"61":1}}],["leave",{"0":{"7":1,"47":1}}],["leak",{"0":{"61":1}}],["learn",{"0":{"13":1,"50":1}}],["level",{"0":{"4":1,"21":2,"68":2}}],["lt",{"0":{"4":4,"5":1,"8":7,"11":2,"12":4,"14":3,"18":1,"26":2,"28":1,"29":2,"32":2,"33":5,"35":9,"43":3,"45":14,"51":4,"56":1,"57":1,"59":1,"63":3,"65":1,"67":11}}],["loop",{"0":{"63":2}}],["looking",{"0":{"57":1}}],["looks",{"0":{"56":1,"57":1,"62":1}}],["look",{"0":{"2":1,"40":1,"56":3,"57":1}}],["lot",{"0":{"56":1}}],["located",{"0":{"56":1}}],["locate",{"0":{"56":1}}],["locally",{"0":{"22":1,"59":2}}],["localhost",{"0":{"14":2,"39":1}}],["local",{"0":{"14":1,"22":2,"33":3,"46":1,"54":1,"59":1,"63":29}}],["loudly",{"0":{"56":1}}],["long",{"0":{"44":1,"56":2}}],["longer",{"0":{"25":1}}],["lo",{"0":{"26":2}}],["logout",{"0":{"66":1}}],["logic",{"0":{"56":1}}],["login",{"0":{"11":3,"12":3,"14":4,"20":2,"56":2,"66":2}}],["logged",{"0":{"56":2,"61":1}}],["logging",{"0":{"21":2,"61":1}}],["log",{"0":{"21":1,"33":1,"47":1,"56":10,"57":1,"66":2}}],["logs",{"0":{"2":4,"8":2,"9":1,"15":2,"29":1,"32":2,"33":1,"54":1,"56":1,"62":2,"63":2,"68":1}}],["loki",{"0":{"9":1,"29":3,"32":1,"33":5,"43":3,"54":6}}],["loads",{"0":{"63":2}}],["loading",{"0":{"63":1}}],["loader",{"0":{"42":1}}],["loaddata",{"0":{"15":1}}],["loadbalancer",{"0":{"2":1,"6":1}}],["load",{"0":{"2":2,"42":1,"63":5,"68":1}}],["lib",{"0":{"63":28}}],["libraries",{"0":{"56":3,"58":5,"65":1}}],["library",{"0":{"28":1,"64":1}}],["life",{"0":{"32":1}}],["lifecycle",{"0":{"2":1,"32":7,"33":1}}],["limited",{"0":{"32":1}}],["limits",{"0":{"24":3,"46":2}}],["limit",{"0":{"24":3,"32":1,"43":1}}],["livenessprobe",{"0":{"56":2}}],["live",{"0":{"22":3,"23":1,"34":1,"47":1}}],["licences",{"0":{"22":2}}],["licenes",{"0":{"22":1}}],["linting",{"0":{"25":1}}],["linter",{"0":{"25":2}}],["link",{"0":{"22":1,"46":1}}],["line",{"0":{"22":1,"29":1,"30":2,"56":3,"57":1,"63":29}}],["linuxbrew",{"0":{"56":4}}],["linux",{"0":{"4":1,"26":3,"56":2}}],["listcomp",{"0":{"63":1}}],["listing",{"0":{"56":1}}],["listening",{"0":{"22":1}}],["listbucket",{"0":{"2":2,"9":1,"32":1,"53":1}}],["list",{"0":{"2":1,"3":2,"12":1,"15":1,"20":3,"24":1,"30":1,"32":1,"38":1,"41":2,"45":1,"46":5,"47":4,"56":2,"57":1,"63":2}}],["likely",{"0":{"61":1}}],["like",{"0":{"2":3,"5":1,"17":1,"22":1,"25":6,"27":1,"40":2,"47":1,"53":1,"56":3,"57":3,"61":2,"62":1,"68":3}}],["ll",{"0":{"2":3,"4":1,"5":1,"23":1,"56":1,"57":1,"59":1}}],["launchapd",{"0":{"68":1}}],["launchpad",{"0":{"56":1,"57":1}}],["last",{"0":{"40":1,"57":1,"63":1}}],["labeled",{"0":{"56":1}}],["label",{"0":{"20":2,"43":1}}],["labels",{"0":{"0":6,"9":6,"20":4,"43":1,"58":4}}],["larger",{"0":{"34":1}}],["large",{"0":{"2":1,"9":1,"24":2}}],["latest",{"0":{"2":1,"3":1,"6":1,"13":1,"14":5,"16":1,"23":1,"28":1,"39":1,"47":2}}],["later",{"0":{"0":2,"22":1,"34":1}}],["270",{"0":{"63":1}}],["275",{"0":{"63":1}}],["273d",{"0":{"56":8}}],["2f",{"0":{"57":1}}],["2fmanagedclusters",{"0":{"56":1}}],["2c",{"0":{"57":1}}],["22",{"0":{"57":2}}],["22f9d484",{"0":{"26":2}}],["2b",{"0":{"57":1}}],["251354",{"0":{"68":1}}],["255",{"0":{"68":6}}],["2552cd9b",{"0":{"45":2}}],["25",{"0":{"57":2}}],["2581bfb0",{"0":{"45":1}}],["2683",{"0":{"63":1}}],["26h",{"0":{"56":4}}],["26",{"0":{"24":1}}],["28",{"0":{"24":1,"57":1}}],["21",{"0":{"15":1,"57":3}}],["2038",{"0":{"57":1}}],["2013",{"0":{"57":1}}],["2017",{"0":{"57":1}}],["2012",{"0":{"2":3,"9":1,"32":1,"53":1}}],["20gi",{"0":{"45":2}}],["20",{"0":{"25":1,"27":1,"46":1}}],["2023",{"0":{"68":2}}],["2027",{"0":{"57":1}}],["2025",{"0":{"57":2}}],["202411140044",{"0":{"56":1}}],["202410250048",{"0":{"56":1}}],["2024",{"0":{"25":2,"57":4}}],["2021",{"0":{"15":1,"62":1}}],["200",{"0":{"9":3}}],["232d",{"0":{"56":10}}],["23",{"0":{"5":1,"57":2}}],["295567f106ff46139ad4edf24e52fc31",{"0":{"29":1}}],["29",{"0":{"4":1,"24":1}}],["244",{"0":{"63":1}}],["24",{"0":{"2":1,"57":2,"68":1}}],["2",{"0":{"2":4,"3":1,"5":1,"11":4,"15":2,"25":2,"26":3,"35":2,"40":1,"42":1,"47":4,"54":1,"56":4,"57":3,"64":2,"67":1,"68":4}}],["y2cc+i3f8bwbykrr92fmb8a=",{"0":{"57":1}}],["yarnpkg",{"0":{"50":1}}],["yarn",{"0":{"50":3}}],["yaml",{"0":{"2":4,"3":5,"6":2,"8":2,"14":5,"15":1,"16":2,"17":1,"25":1,"28":1,"33":2,"45":2,"54":1,"56":3,"62":5,"65":2,"67":4}}],["y",{"0":{"32":1,"46":2}}],["yml",{"0":{"25":1,"42":2,"55":1}}],["yes",{"0":{"4":1}}],["yet",{"0":{"2":1,"3":2,"56":1}}],["year",{"0":{"2":1}}],["yourself",{"0":{"56":1}}],["youremail",{"0":{"41":1}}],["your",{"0":{"0":2,"2":14,"3":4,"4":11,"5":2,"6":1,"7":1,"9":2,"14":2,"15":2,"16":1,"17":1,"20":1,"26":2,"31":1,"32":3,"33":1,"34":10,"39":1,"40":2,"41":1,"44":1,"47":4,"53":4,"56":22,"57":5,"59":1,"66":2}}],["you",{"0":{"0":2,"2":10,"3":5,"4":4,"5":2,"6":1,"7":5,"12":4,"15":3,"18":1,"20":3,"21":1,"22":18,"23":4,"25":1,"27":1,"29":2,"30":3,"32":9,"33":1,"34":31,"39":3,"40":8,"41":2,"44":1,"45":1,"46":4,"47":7,"49":2,"54":1,"55":2,"56":50,"57":24,"59":5,"61":1,"62":1,"66":2}}],["ignore=ignore",{"0":{"63":1}}],["ignore",{"0":{"63":1}}],["ips",{"0":{"24":1,"68":2}}],["ip",{"0":{"12":1,"24":2,"27":1,"56":5,"68":4}}],["io",{"0":{"11":5,"13":1,"14":4,"26":1,"37":1,"46":3,"67":1}}],["iac",{"0":{"2":1,"17":1}}],["iam",{"0":{"2":10,"4":1,"7":2,"14":3,"23":2,"26":7,"53":1}}],["i",{"0":{"2":2,"4":1,"16":1,"18":1,"22":2,"25":3,"26":1,"29":2,"30":2,"34":1,"44":2,"56":6,"57":4,"59":1,"61":1}}],["ids",{"0":{"45":1}}],["idempotent",{"0":{"61":1}}],["idea",{"0":{"34":1,"56":1}}],["identity",{"0":{"2":1}}],["id",{"0":{"2":8,"3":2,"4":1,"16":2,"22":3,"26":3,"29":1,"45":7,"52":10,"63":1,"65":1,"68":2}}],["iris",{"0":{"2":2}}],["immutable",{"0":{"61":1}}],["impl",{"0":{"63":4}}],["implemented",{"0":{"61":1}}],["implementation",{"0":{"61":3}}],["implies",{"0":{"61":1}}],["impacts",{"0":{"61":1}}],["imposed",{"0":{"2":1}}],["imports",{"0":{"25":1}}],["import",{"0":{"2":1,"12":2,"30":1,"41":1,"54":2,"57":4,"64":1}}],["important",{"0":{"2":2,"12":1,"34":1,"47":1,"61":1}}],["image",{"0":{"21":2,"28":1,"31":1,"34":2,"40":8,"56":13,"58":2,"59":4,"62":1,"65":1}}],["image=f",{"0":{"21":1}}],["images",{"0":{"2":1,"7":2,"8":2,"15":7,"25":13,"28":4,"34":3,"39":1,"40":6,"56":1,"58":1,"59":1,"65":2}}],["iterator",{"0":{"63":1}}],["iter",{"0":{"63":1}}],["items",{"0":{"5":1,"25":1,"44":1,"47":1}}],["item",{"0":{"5":1,"7":1}}],["itsus",{"0":{"5":1,"7":1}}],["its",{"0":{"4":4,"7":10,"15":2,"41":2,"56":1,"59":1,"61":1}}],["it",{"0":{"2":10,"3":2,"5":2,"6":1,"7":2,"8":1,"12":3,"15":4,"16":1,"18":2,"22":6,"23":1,"25":4,"29":1,"30":2,"34":13,"41":1,"44":2,"45":2,"47":2,"48":1,"49":1,"51":2,"52":1,"53":1,"54":1,"56":18,"57":7,"59":1,"61":8,"64":2,"66":1,"68":1}}],["itx",{"0":{"2":10,"3":1,"4":4,"6":1,"15":1,"17":1,"26":2}}],["inotify",{"0":{"46":5}}],["ingester",{"0":{"43":2}}],["ingress",{"0":{"2":3,"6":2,"15":2,"17":1,"57":1}}],["inc",{"0":{"52":1,"57":9}}],["incompatible",{"0":{"28":1}}],["increased",{"0":{"24":1}}],["increasing",{"0":{"23":1}}],["including",{"0":{"22":1,"28":1,"57":1}}],["included",{"0":{"22":2}}],["include",{"0":{"12":1,"22":3,"52":1,"61":1,"62":1}}],["includes",{"0":{"4":1}}],["input",{"0":{"23":1}}],["invocation",{"0":{"64":1}}],["invocations",{"0":{"30":1}}],["involving",{"0":{"61":1}}],["involved",{"0":{"47":1}}],["invoke",{"0":{"23":1,"30":1,"45":1,"64":1}}],["invoice",{"0":{"22":4}}],["invoices",{"0":{"22":1}}],["inviting",{"0":{"22":1}}],["index",{"0":{"14":1,"32":3,"42":1}}],["indication",{"0":{"3":1}}],["infinity",{"0":{"65":1}}],["influence",{"0":{"61":1}}],["info",{"0":{"43":1,"45":1,"63":1,"67":1,"68":2}}],["inform",{"0":{"7":1,"23":1}}],["information",{"0":{"2":1,"3":1,"22":3,"23":1,"35":1,"57":1}}],["infra",{"0":{"12":1}}],["init",{"0":{"3":1,"6":1,"17":1}}],["initially",{"0":{"3":1,"56":1}}],["initialize",{"0":{"3":1,"30":1}}],["initial",{"0":{"2":1,"3":1,"15":1}}],["inside",{"0":{"15":1}}],["inspect",{"0":{"8":1,"14":1,"22":1,"23":2,"30":1,"39":1}}],["insert",{"0":{"2":1}}],["instead",{"0":{"57":2}}],["instruction",{"0":{"46":1}}],["instructions",{"0":{"1":1,"2":2,"4":1,"12":1,"22":2,"56":3,"57":2}}],["instances",{"0":{"7":1,"9":1,"24":4,"46":1,"63":2}}],["instance",{"0":{"2":4,"4":5,"9":1,"24":6,"47":1,"49":1,"63":2}}],["installation",{"0":{"2":1,"6":3,"10":2,"14":1,"17":4,"39":1,"41":2,"46":2,"56":9,"61":1}}],["install",{"0":{"2":1,"4":2,"6":5,"10":1,"11":3,"12":3,"13":2,"14":6,"17":6,"25":1,"26":1,"35":1,"37":2,"39":1,"46":6,"50":3,"54":1,"56":4,"58":1,"59":1,"61":3}}],["installed",{"0":{"2":3,"3":2,"4":1,"12":1,"15":1,"34":1,"40":1,"56":2,"61":2,"63":1}}],["installer",{"0":{"2":1,"39":2}}],["int",{"0":{"54":1}}],["introduce",{"0":{"28":1}}],["introduction",{"0":{"22":1}}],["intel",{"0":{"68":2}}],["intelligence",{"0":{"42":2}}],["intend",{"0":{"34":1}}],["integrates",{"0":{"22":1}}],["integration",{"0":{"22":2,"42":1}}],["integrationscluster",{"0":{"2":1}}],["interruption",{"0":{"64":1}}],["interrupted",{"0":{"61":1}}],["intermitent",{"0":{"61":1}}],["intermediate",{"0":{"57":1}}],["internally",{"0":{"47":1}}],["internal",{"0":{"18":1,"27":1,"68":2}}],["intervention",{"0":{"2":1,"28":1,"61":1}}],["interactive",{"0":{"2":1}}],["into",{"0":{"2":1,"6":3,"17":2,"34":2,"37":1,"39":2,"47":1,"56":18,"57":2,"61":1}}],["in",{"0":{"0":1,"1":1,"2":6,"3":10,"4":4,"5":3,"6":3,"7":5,"8":5,"10":2,"11":1,"12":2,"14":1,"15":8,"16":6,"17":4,"18":1,"20":1,"22":14,"23":1,"25":3,"28":2,"29":2,"30":3,"32":4,"33":2,"34":5,"35":1,"37":1,"38":1,"39":1,"40":7,"41":1,"44":2,"45":7,"46":4,"47":7,"49":4,"51":1,"52":5,"53":5,"55":1,"56":25,"57":36,"58":1,"59":1,"61":18,"62":1,"63":31,"64":1,"65":2,"66":2,"67":1}}],["isolate",{"0":{"61":1}}],["isn",{"0":{"3":1,"44":1,"57":1}}],["issuer=c=us",{"0":{"57":1}}],["issuer",{"0":{"35":1,"57":9}}],["issue",{"0":{"5":1,"15":1,"34":1,"35":2,"59":1,"68":1}}],["issues",{"0":{"2":1,"5":3,"57":1,"61":1,"62":2}}],["issued",{"0":{"0":1,"2":1}}],["is",{"0":{"0":2,"1":4,"2":11,"3":3,"4":1,"5":2,"6":1,"7":3,"8":1,"10":1,"12":3,"15":8,"17":2,"22":3,"23":2,"25":8,"28":3,"29":1,"32":1,"33":1,"34":11,"39":1,"40":1,"41":3,"42":1,"44":2,"45":4,"47":5,"49":1,"52":2,"56":37,"57":13,"61":20,"67":1,"68":3}}],["if",{"0":{"0":1,"2":5,"3":3,"8":2,"12":1,"15":1,"18":1,"22":3,"29":2,"34":6,"39":6,"44":1,"45":2,"47":3,"49":1,"52":1,"56":11,"57":11,"61":5,"66":1,"67":1}}],["human",{"0":{"28":1}}],["hubsextension",{"0":{"56":1}}],["hub",{"0":{"14":1}}],["h",{"0":{"18":3}}],["http",{"0":{"14":2,"54":1,"68":3}}],["https",{"0":{"1":4,"2":9,"3":2,"4":1,"6":1,"7":3,"11":3,"12":1,"13":1,"14":12,"15":2,"17":1,"20":1,"21":1,"22":1,"23":2,"25":13,"26":3,"29":2,"30":1,"35":1,"37":1,"44":1,"46":5,"47":1,"50":1,"52":1,"56":13,"57":4,"67":1}}],["html",{"0":{"13":1,"14":4,"21":1,"42":1}}],["held",{"0":{"61":1}}],["help",{"0":{"34":1,"40":1,"47":1,"52":1,"56":1}}],["helpful",{"0":{"23":1}}],["helmrelease",{"0":{"61":7}}],["helm",{"0":{"11":20,"46":6,"56":2,"61":20,"67":13}}],["headers",{"0":{"57":1}}],["headers=",{"0":{"54":1}}],["header",{"0":{"12":1,"22":1,"57":5}}],["health",{"0":{"7":1}}],["here",{"0":{"2":1,"3":2,"15":2,"18":1,"22":1,"23":1,"33":1,"34":1,"40":1,"56":8,"57":2}}],["hmac",{"0":{"4":2}}],["hmd",{"0":{"2":1,"7":2}}],["hit",{"0":{"40":1,"41":1,"57":1}}],["hide",{"0":{"3":1,"15":1,"16":1,"39":1}}],["higher",{"0":{"15":1,"24":1}}],["high",{"0":{"0":1,"34":1}}],["happened",{"0":{"68":1}}],["happen",{"0":{"61":1}}],["happens",{"0":{"2":1,"61":1,"68":1}}],["having",{"0":{"57":1,"61":1,"68":1}}],["haven",{"0":{"56":1,"61":1}}],["have",{"0":{"2":2,"3":1,"7":1,"8":1,"12":1,"15":1,"22":3,"24":3,"29":2,"32":3,"34":2,"41":2,"46":1,"47":5,"56":10,"57":10,"61":2,"62":1,"68":1}}],["had",{"0":{"56":1,"61":1}}],["hang",{"0":{"56":1}}],["handling",{"0":{"57":1,"64":1}}],["handle",{"0":{"3":1,"16":1}}],["hand",{"0":{"47":1}}],["hard",{"0":{"44":1}}],["hash",{"0":{"8":1,"56":1,"61":1}}],["has",{"0":{"2":2,"3":1,"15":1,"17":1,"20":1,"21":1,"22":1,"23":1,"25":1,"31":1,"32":2,"41":1,"45":1,"57":1,"61":3,"68":1}}],["hood",{"0":{"47":1}}],["hook",{"0":{"5":3,"42":1}}],["hooks",{"0":{"5":1,"42":1}}],["hot",{"0":{"34":2}}],["hotfix",{"0":{"34":10}}],["holds",{"0":{"15":2,"62":1}}],["home",{"0":{"4":4,"5":1,"46":1,"56":6,"63":29}}],["hosted",{"0":{"22":1,"46":1}}],["hosts",{"0":{"12":1}}],["host",{"0":{"3":2,"4":2,"16":2,"57":6,"68":1}}],["hostname",{"0":{"2":3,"54":1,"57":1}}],["how",{"0":{"2":1,"5":1,"6":1,"7":1,"8":1,"11":1,"12":1,"13":1,"15":2,"22":3,"26":1,"28":1,"29":3,"30":1,"33":1,"34":2,"35":1,"40":1,"41":1,"44":1,"45":1,"48":1,"50":1,"51":1,"52":1,"54":1,"55":1,"56":3,"58":1,"61":1,"62":3,"67":1}}],["however",{"0":{"0":1,"2":1,"34":1,"41":1,"56":3,"57":2}}],["hours",{"0":{"2":1}}],["wrapper",{"0":{"63":4}}],["writing",{"0":{"61":1}}],["written",{"0":{"61":1}}],["writes",{"0":{"61":1}}],["write",{"0":{"2":1,"7":1,"32":1}}],["wrong",{"0":{"56":1,"57":1,"68":1}}],["www",{"0":{"56":1}}],["wget",{"0":{"11":1,"37":1}}],["warning",{"0":{"57":1}}],["warehouse",{"0":{"2":3,"5":1}}],["waiting",{"0":{"61":1}}],["wait",{"0":{"49":1,"56":1}}],["watching",{"0":{"61":1}}],["watches",{"0":{"46":1,"61":2}}],["watch",{"0":{"15":2,"56":1}}],["want",{"0":{"12":1,"23":1,"29":1,"34":4,"40":1,"56":1,"57":1,"67":1}}],["way",{"0":{"2":1,"41":1,"56":2,"61":1,"65":1}}],["was",{"0":{"2":2,"4":1,"8":1,"22":1,"34":1,"42":1,"52":1,"59":1,"61":2,"66":1,"68":1}}],["wcr",{"0":{"2":2,"4":4,"26":1}}],["won",{"0":{"56":1,"61":2}}],["word",{"0":{"56":1,"57":1}}],["worspace",{"0":{"1":1}}],["workaround",{"0":{"44":1}}],["workbench",{"0":{"4":1,"27":1,"40":1,"56":4}}],["works",{"0":{"57":1,"68":1}}],["workstation",{"0":{"4":1,"6":1,"17":1}}],["workspaces",{"0":{"23":1}}],["workspace",{"0":{"1":1,"4":1,"7":1,"23":1,"61":13}}],["workflow",{"0":{"3":1,"22":1,"28":1}}],["work",{"0":{"2":1,"34":2,"44":2,"56":6,"59":1}}],["working",{"0":{"2":1,"4":2,"56":2}}],["worker",{"0":{"0":1,"2":1,"9":1,"29":1,"43":3,"56":6}}],["workers=enabled",{"0":{"20":3}}],["workerslarge",{"0":{"20":1}}],["workers",{"0":{"0":4,"2":1,"9":3,"20":1,"29":2}}],["workload",{"0":{"0":1}}],["would",{"0":{"0":1,"34":1,"47":1,"61":1}}],["wiki",{"0":{"35":1}}],["wire",{"0":{"22":1}}],["wizard",{"0":{"22":3}}],["windows",{"0":{"4":2,"33":1}}],["will",{"0":{"1":1,"2":8,"3":4,"6":2,"12":3,"15":6,"18":1,"20":1,"22":6,"25":1,"27":1,"29":2,"34":7,"39":1,"40":2,"44":1,"45":1,"47":3,"49":4,"56":16,"57":12,"58":1,"59":3,"61":4,"68":2}}],["wildcard",{"0":{"0":2,"2":1,"35":3,"57":31}}],["withpass",{"0":{"57":4}}],["within",{"0":{"3":1,"44":1,"57":1,"61":1}}],["without",{"0":{"2":1,"41":1,"44":1,"56":1,"57":2,"59":1,"61":2}}],["with",{"0":{"1":2,"2":15,"5":1,"6":1,"7":1,"8":1,"9":1,"12":4,"14":2,"15":6,"16":2,"18":1,"22":4,"24":1,"25":1,"27":1,"28":1,"29":4,"32":1,"34":3,"39":1,"40":1,"43":2,"44":2,"45":1,"49":2,"53":3,"56":10,"57":4,"59":1,"61":1,"62":1,"63":1,"64":1}}],["wish",{"0":{"0":1,"34":2,"49":1,"56":1}}],["whole",{"0":{"61":3,"67":1}}],["while",{"0":{"22":1}}],["which",{"0":{"3":1,"4":2,"20":1,"24":1,"34":2,"47":3,"56":4,"61":4}}],["what",{"0":{"3":4,"34":1,"39":1,"56":2,"57":1,"61":1}}],["whatever",{"0":{"0":1,"27":1,"56":1,"57":1}}],["whenever",{"0":{"61":1}}],["when",{"0":{"2":1,"3":1,"5":1,"22":3,"28":4,"30":1,"45":1,"46":1,"47":1,"49":1,"56":3,"57":1,"61":7,"68":2}}],["where",{"0":{"0":1,"2":3,"3":1,"6":1,"15":1,"22":1,"25":1,"28":1,"40":1,"53":1,"56":1}}],["well",{"0":{"15":1,"57":2,"61":1}}],["weaveworks",{"0":{"14":2}}],["weekly",{"0":{"4":1}}],["were",{"0":{"3":1,"5":1,"22":1,"61":1,"63":1}}],["webserver",{"0":{"56":2,"63":1}}],["webhooks",{"0":{"22":2}}],["webhook",{"0":{"5":2}}],["web",{"0":{"2":1,"4":1,"15":1,"56":1}}],["we",{"0":{"0":3,"2":4,"3":2,"4":1,"8":1,"11":1,"15":3,"22":2,"23":1,"28":6,"29":2,"32":4,"44":2,"45":1,"47":3,"52":1,"53":1,"56":9,"57":6,"61":13,"65":1,"67":2}}],["khtml",{"0":{"68":2}}],["kwds",{"0":{"63":2}}],["kwargs",{"0":{"63":5}}],["konnectivity",{"0":{"56":2}}],["kafka",{"0":{"49":4}}],["kim",{"0":{"56":1}}],["killed",{"0":{"43":1}}],["kinds",{"0":{"61":1}}],["kind=general",{"0":{"20":2}}],["kind",{"0":{"0":2,"2":1,"9":2,"26":1,"27":1,"45":2,"46":4,"62":3}}],["knowledge",{"0":{"61":1}}],["know",{"0":{"30":1,"34":1}}],["known",{"0":{"5":1,"15":1}}],["k4p5w",{"0":{"27":1}}],["k",{"0":{"14":1,"61":1}}],["kustomization",{"0":{"14":1}}],["kustomize",{"0":{"14":2,"62":1}}],["kubebuilder",{"0":{"61":2}}],["kuberlr",{"0":{"26":7}}],["kubernates",{"0":{"8":1}}],["kubernetes",{"0":{"0":3,"2":2,"4":1,"6":1,"8":3,"14":8,"15":4,"21":1,"26":4,"45":4,"46":5,"56":11,"61":6,"62":1}}],["kube",{"0":{"4":2,"8":2,"12":3,"14":4,"26":3,"27":3,"43":6,"56":6}}],["kubelogin",{"0":{"4":3,"12":1}}],["kubectl",{"0":{"2":3,"6":3,"8":10,"12":7,"14":13,"15":9,"17":1,"20":3,"26":3,"27":3,"38":2,"39":1,"45":10,"46":1,"49":6,"51":2,"54":1,"56":25,"57":2,"61":1}}],["kcc",{"0":{"23":1}}],["kctx=$",{"0":{"15":1}}],["kc",{"0":{"4":3}}],["ktlo",{"0":{"2":1}}],["kept",{"0":{"3":1,"10":1,"17":1}}],["keeps",{"0":{"61":1}}],["keep",{"0":{"2":1,"18":1,"40":2,"47":1,"52":1,"56":2,"62":1}}],["kenvue",{"0":{"1":3,"4":1,"7":2,"8":1,"11":6,"26":2,"48":1,"55":2,"56":6}}],["key2=value2",{"0":{"67":2}}],["keychain",{"0":{"57":1}}],["key=fill",{"0":{"53":2}}],["key1=value1",{"0":{"67":2}}],["key1",{"0":{"32":1}}],["keys",{"0":{"23":1,"41":3,"44":1,"57":3}}],["keyrings",{"0":{"41":1}}],["keyring",{"0":{"3":1,"15":2,"16":1}}],["key",{"0":{"0":3,"2":6,"5":1,"9":3,"14":1,"32":1,"33":3,"35":7,"41":8,"42":2,"46":5,"53":1,"57":24}}],["k8sadmin",{"0":{"4":1}}],["k8smonitor",{"0":{"4":1}}],["k8soperator",{"0":{"4":1}}],["k8s",{"0":{"0":3,"2":2,"9":3,"14":1,"20":5,"21":3,"26":1,"46":1,"61":3}}],["rsa",{"0":{"57":13}}],["rsync",{"0":{"39":2}}],["risk",{"0":{"56":1}}],["right",{"0":{"2":1,"22":2,"30":1,"40":1,"56":3}}],["rc",{"0":{"56":1}}],["rm",{"0":{"41":1,"65":1}}],["ruler",{"0":{"54":1}}],["rule",{"0":{"32":8}}],["rules",{"0":{"15":1,"32":2,"54":1}}],["ruff",{"0":{"25":2}}],["runtime",{"0":{"61":1}}],["runnner",{"0":{"46":1}}],["runner",{"0":{"46":2,"61":1}}],["running",{"0":{"10":1,"15":6,"28":1,"38":1,"43":1,"61":2,"63":1,"65":1}}],["runs",{"0":{"4":1,"42":1,"56":1,"58":1,"61":4}}],["run",{"0":{"2":1,"3":2,"4":1,"6":1,"14":1,"15":3,"20":2,"22":4,"23":3,"25":1,"30":5,"35":1,"37":1,"39":1,"42":1,"46":2,"50":3,"55":2,"56":6,"57":3,"58":1,"61":3,"63":12,"65":2}}],["rabbitmq",{"0":{"57":1}}],["rate",{"0":{"43":1}}],["rainbow",{"0":{"25":1}}],["raw",{"0":{"14":1,"63":2}}],["ran",{"0":{"4":1}}],["random",{"0":{"2":1}}],["ryan",{"0":{"7":1}}],["r5",{"0":{"2":1,"9":1}}],["rds",{"0":{"2":1,"9":1}}],["row",{"0":{"63":6}}],["rows",{"0":{"63":5}}],["rotation",{"0":{"33":1}}],["rotate",{"0":{"32":2}}],["rollout",{"0":{"27":1}}],["role=datacoves",{"0":{"18":1}}],["role",{"0":{"2":5,"4":2,"7":1,"26":1}}],["roles",{"0":{"2":2,"4":1,"7":5}}],["robertostermann",{"0":{"25":1}}],["route53",{"0":{"2":1}}],["root",{"0":{"0":3,"2":1,"9":3,"35":3,"56":1,"57":35,"61":1}}],["r",{"0":{"2":1,"3":1,"4":1,"6":1,"7":1,"17":1,"26":1,"30":3,"37":1,"50":1,"56":3}}],["rnd",{"0":{"1":1}}],["reinstalled",{"0":{"68":1}}],["returning",{"0":{"68":1}}],["returned",{"0":{"61":1,"64":1}}],["return",{"0":{"57":2,"63":15}}],["retain",{"0":{"45":5}}],["retried",{"0":{"61":2}}],["retrieve",{"0":{"6":1}}],["retrying",{"0":{"61":2}}],["retry",{"0":{"29":1,"56":2,"61":1}}],["regenerate",{"0":{"68":1}}],["regex",{"0":{"43":1}}],["regarding",{"0":{"57":2}}],["region=fill",{"0":{"53":1}}],["region",{"0":{"32":1,"33":1}}],["registrations",{"0":{"52":1}}],["registry",{"0":{"8":1,"11":1,"14":1,"15":1}}],["register",{"0":{"22":1,"52":2}}],["reusing",{"0":{"61":1}}],["reuse",{"0":{"41":1,"59":1}}],["reusable",{"0":{"40":1}}],["remember",{"0":{"29":1}}],["removing",{"0":{"22":1}}],["removed",{"0":{"57":1}}],["remove",{"0":{"3":1,"22":1,"23":3,"45":1,"56":1,"57":2,"63":1,"66":1,"68":1}}],["remote",{"0":{"1":1,"4":2}}],["referer",{"0":{"68":1}}],["references",{"0":{"55":1,"61":2}}],["reference",{"0":{"7":1,"22":1,"28":1,"30":1,"39":1,"42":1,"61":1,"62":1}}],["refs",{"0":{"34":2,"56":1}}],["refreshing",{"0":{"40":1}}],["refresh",{"0":{"23":1}}],["reflected",{"0":{"22":1}}],["re",{"0":{"22":2,"34":2,"49":2,"52":1,"56":3,"57":1,"61":1}}],["rename",{"0":{"12":2}}],["reached",{"0":{"54":1}}],["react",{"0":{"50":1}}],["real",{"0":{"25":1,"63":1,"68":1}}],["reassign",{"0":{"18":2}}],["reason=",{"0":{"43":1}}],["reason",{"0":{"7":3,"43":1,"49":1}}],["readable",{"0":{"61":1}}],["readinessprobe",{"0":{"56":2}}],["readme",{"0":{"56":1}}],["readonly",{"0":{"45":1}}],["readwriteonce",{"0":{"45":2}}],["ready",{"0":{"15":2,"34":1,"40":1}}],["readthedocs",{"0":{"13":1}}],["read",{"0":{"1":1,"2":3,"9":1,"28":1,"32":1,"52":1,"56":1,"57":1,"64":2}}],["related",{"0":{"58":1}}],["relatively",{"0":{"56":2}}],["relationship",{"0":{"13":1}}],["releasing",{"0":{"56":2}}],["release=",{"0":{"39":1}}],["release",{"0":{"11":1,"28":2,"34":13,"39":10,"40":2,"45":3,"47":14,"56":16,"59":7,"61":1,"67":5}}],["released",{"0":{"6":1}}],["releases",{"0":{"2":1,"14":1,"26":2,"28":3,"34":2,"47":9,"56":1,"59":3,"61":1,"67":1}}],["releae",{"0":{"56":1}}],["relevant",{"0":{"47":1}}],["reload",{"0":{"40":2}}],["rely",{"0":{"3":1}}],["revert",{"0":{"56":1}}],["reveal",{"0":{"3":1,"4":1,"6":2,"15":1,"16":2,"17":2,"39":1,"56":2,"57":2}}],["review",{"0":{"3":1,"16":1,"29":1,"32":1,"34":1,"39":1,"40":1,"61":1,"65":2}}],["revoke",{"0":{"2":1}}],["response",{"0":{"68":3}}],["responsible",{"0":{"33":1,"61":1}}],["responding",{"0":{"64":1}}],["respond",{"0":{"64":1}}],["resemble",{"0":{"57":1}}],["reset",{"0":{"49":2,"66":2}}],["resetting",{"0":{"49":1}}],["result=none",{"0":{"64":1}}],["result",{"0":{"30":1,"63":7}}],["results",{"0":{"30":1,"49":1,"61":1}}],["res",{"0":{"30":2}}],["resolution",{"0":{"68":1}}],["resolutions",{"0":{"62":1,"67":1}}],["resolve",{"0":{"27":1}}],["resouorce",{"0":{"45":1}}],["resourceversion",{"0":{"61":1}}],["resourcetype",{"0":{"56":1}}],["resourcegroups",{"0":{"12":1,"45":3}}],["resources",{"0":{"6":1,"10":1,"15":2,"17":2,"45":6,"61":12}}],["resource",{"0":{"2":4,"9":2,"12":2,"20":5,"23":2,"32":4,"45":9,"53":2,"61":9}}],["restarted",{"0":{"61":2}}],["restarting",{"0":{"56":1}}],["restart",{"0":{"27":1}}],["restarts",{"0":{"15":2,"27":1}}],["rest",{"0":{"18":1}}],["restore",{"0":{"18":2,"56":3}}],["restrictions",{"0":{"2":1}}],["resides",{"0":{"2":1}}],["repeatedly",{"0":{"61":1}}],["repeate",{"0":{"18":1}}],["replace",{"0":{"49":1,"56":1,"57":1}}],["replacing",{"0":{"2":2,"53":1}}],["replicated",{"0":{"8":2}}],["repositories",{"0":{"2":2,"8":2,"17":1,"44":2,"46":1,"56":1}}],["repository",{"0":{"2":3,"3":3,"5":2,"6":1,"7":1,"10":3,"11":1,"15":2,"17":5,"28":1,"33":1,"34":1,"39":1,"44":4,"46":2,"56":9,"57":1,"67":2}}],["repos",{"0":{"1":1,"2":8,"3":2,"15":1,"41":1}}],["repo",{"0":{"1":3,"2":9,"3":3,"4":1,"5":1,"15":5,"16":1,"21":1,"28":1,"37":1,"39":2,"42":3,"44":1,"55":1,"67":2}}],["redis",{"0":{"56":4}}],["redirects",{"0":{"22":1}}],["redirect",{"0":{"2":1,"52":1}}],["redhat",{"0":{"25":1}}],["redundancy",{"0":{"0":1}}],["recent",{"0":{"63":1}}],["recently",{"0":{"22":1}}],["received",{"0":{"23":1,"57":2}}],["receives",{"0":{"22":2,"61":1}}],["receive",{"0":{"2":2,"5":2,"56":1,"57":1,"64":1}}],["recomputes",{"0":{"61":1}}],["recommend",{"0":{"0":2,"2":1}}],["recommended",{"0":{"0":2,"8":1,"14":1,"56":1,"57":2}}],["recover",{"0":{"45":1}}],["reconciliaton",{"0":{"61":1}}],["reconciliation",{"0":{"61":8}}],["reconciler",{"0":{"61":1}}],["reconcilers",{"0":{"61":2}}],["reconcile",{"0":{"61":6}}],["reconfiguration",{"0":{"28":3}}],["reconnect",{"0":{"4":1}}],["record",{"0":{"2":1,"22":1,"62":1}}],["requests",{"0":{"45":1,"54":2,"56":1,"61":2}}],["requesting",{"0":{"4":1,"7":3}}],["requested",{"0":{"2":1}}],["request",{"0":{"1":3,"2":5,"4":4,"5":2,"7":11,"45":1,"48":1,"58":1,"68":4}}],["requirement",{"0":{"39":1}}],["requirements",{"0":{"0":2,"1":3,"2":4,"3":1,"4":1,"6":1,"9":3,"10":1,"16":1,"17":1,"37":2,"40":5,"50":1,"56":2,"58":1}}],["require",{"0":{"2":1,"4":1,"15":1,"24":1,"27":2,"28":4,"52":1,"57":1}}],["requires",{"0":{"2":4,"12":1,"28":1,"56":1}}],["required",{"0":{"0":1,"2":3,"3":2,"6":3,"10":1,"15":2,"16":1,"17":1,"22":1,"39":4,"61":2}}],["441",{"0":{"63":1}}],["443",{"0":{"11":1,"68":1}}],["46",{"0":{"63":1}}],["474a",{"0":{"68":2}}],["472",{"0":{"63":1}}],["47",{"0":{"57":1}}],["47h",{"0":{"15":1}}],["48",{"0":{"57":2}}],["4c",{"0":{"57":1}}],["4f",{"0":{"57":1}}],["4ff4",{"0":{"56":1}}],["4fbd",{"0":{"45":1}}],["49c64bd87cbf",{"0":{"45":1}}],["400",{"0":{"63":2}}],["408",{"0":{"63":1}}],["40e9",{"0":{"45":1}}],["409d",{"0":{"45":2}}],["417e",{"0":{"26":1}}],["41cca1b4822b",{"0":{"20":1,"45":1}}],["4b21",{"0":{"26":2}}],["4xlarge",{"0":{"24":1}}],["42c9",{"0":{"20":1,"45":1}}],["4ed9",{"0":{"12":2}}],["4",{"0":{"0":3,"2":1,"11":1,"18":1,"26":3,"40":1,"42":1,"56":2,"67":1}}],["num",{"0":{"63":1}}],["number",{"0":{"23":1,"24":3,"56":1,"57":4,"59":1}}],["null",{"0":{"56":2}}],["nitro",{"0":{"24":2}}],["ns=",{"0":{"38":1}}],["nslookup",{"0":{"27":2}}],["ns",{"0":{"4":1,"12":1,"14":1}}],["navigating",{"0":{"23":1}}],["navigate",{"0":{"5":1,"7":1,"14":1,"22":1,"40":2,"48":1,"52":3}}],["na",{"0":{"4":1}}],["naming",{"0":{"2":1}}],["name=fill",{"0":{"53":1}}],["name=yarn",{"0":{"50":1}}],["name=",{"0":{"21":2}}],["name=emeadev",{"0":{"15":1}}],["named",{"0":{"12":1,"15":2,"55":1,"57":1}}],["names",{"0":{"3":2,"42":1,"45":1,"61":1}}],["namespace=",{"0":{"14":1,"29":4,"43":2}}],["namespaces",{"0":{"8":2,"14":2}}],["namespace",{"0":{"2":1,"8":3,"15":5,"27":1,"29":4,"38":1,"43":1,"45":5,"54":1,"56":3,"61":1,"67":3}}],["name",{"0":{"2":13,"4":3,"7":1,"9":2,"12":2,"14":1,"15":9,"18":1,"20":6,"22":1,"23":1,"26":6,"27":4,"29":2,"30":1,"32":9,"33":4,"34":5,"39":1,"43":1,"45":14,"52":1,"53":5,"56":5,"57":3,"59":1,"62":1,"67":1}}],["nginx",{"0":{"2":2,"6":2,"15":1,"17":1}}],["n",{"0":{"2":1,"8":8,"14":4,"15":6,"27":3,"38":2,"45":5,"49":6,"54":1,"56":41,"67":6}}],["nocerts",{"0":{"57":2}}],["nocrl",{"0":{"57":2}}],["nokeys",{"0":{"57":4}}],["nov",{"0":{"57":2}}],["noout",{"0":{"35":1,"57":8}}],["normal",{"0":{"34":2}}],["none",{"0":{"34":2}}],["non",{"0":{"25":1,"57":1}}],["nonce",{"0":{"13":1}}],["no",{"0":{"2":1,"4":2,"18":1,"25":1,"34":1,"37":1,"49":1,"56":4,"57":1,"67":2}}],["notion",{"0":{"56":1}}],["notifications",{"0":{"23":1}}],["notification",{"0":{"22":1,"23":1}}],["nothing",{"0":{"56":1,"61":1}}],["notes",{"0":{"33":1,"39":1,"47":13,"57":2}}],["note",{"0":{"4":1,"7":1,"8":1,"12":2,"22":3,"29":1,"52":1,"56":6,"57":6}}],["not",{"0":{"2":4,"3":3,"6":1,"8":1,"12":2,"22":3,"25":2,"28":2,"29":1,"33":1,"34":2,"39":1,"40":1,"41":1,"42":1,"46":1,"47":3,"49":1,"55":1,"56":7,"57":15,"61":3,"62":2,"64":2,"67":1,"68":2}}],["now",{"0":{"1":1,"12":1,"20":1,"22":2,"34":2,"54":1,"56":4,"57":3}}],["noel",{"0":{"1":3}}],["noderesourcegroup",{"0":{"45":1}}],["node=",{"0":{"43":1}}],["nodepool",{"0":{"20":6}}],["nodepools",{"0":{"20":3}}],["nodegroup",{"0":{"0":2,"9":2,"20":2}}],["nodes",{"0":{"0":6,"9":6,"26":1,"43":2,"57":2}}],["node",{"0":{"0":2,"20":1,"24":2,"30":1,"43":14,"45":7,"54":1,"56":1}}],["never",{"0":{"56":1}}],["necessarily",{"0":{"44":1}}],["necessary",{"0":{"7":1,"12":1,"15":1,"45":2,"47":1,"56":3,"57":1}}],["net",{"0":{"33":1}}],["network",{"0":{"12":1,"24":1,"64":1}}],["networking",{"0":{"2":1}}],["near",{"0":{"25":1}}],["negotiated",{"0":{"22":1}}],["next",{"0":{"22":1,"56":1,"57":1}}],["needs",{"0":{"14":1,"22":1,"27":1,"56":1}}],["needed",{"0":{"2":1,"3":1,"6":1,"7":1,"17":1,"34":1,"39":1,"56":3,"57":2}}],["need",{"0":{"0":1,"2":9,"3":3,"4":2,"5":2,"7":2,"8":1,"15":2,"16":1,"18":2,"20":1,"22":5,"28":1,"29":1,"32":1,"34":2,"40":2,"44":2,"45":1,"49":1,"52":1,"56":11,"57":3,"59":3,"65":1,"66":2,"67":1}}],["newer",{"0":{"56":1,"63":1}}],["new",{"0":{"0":1,"1":1,"2":2,"4":1,"5":2,"7":1,"8":2,"9":1,"11":4,"12":4,"15":5,"18":1,"20":4,"22":3,"27":1,"28":2,"32":5,"34":2,"39":4,"40":3,"41":1,"42":3,"45":6,"46":1,"47":2,"52":3,"54":1,"56":7,"57":2,"58":3,"59":3,"61":2,"67":1}}],["640",{"0":{"63":1}}],["64",{"0":{"57":1}}],["62",{"0":{"57":1}}],["65",{"0":{"57":1}}],["68",{"0":{"57":1,"63":3}}],["68132",{"0":{"23":2}}],["67cf0ab43499",{"0":{"45":1}}],["6rvhd",{"0":{"15":1}}],["600",{"0":{"4":1}}],["6",{"0":{"0":1,"37":1,"42":1}}],["eg",{"0":{"64":1}}],["egg",{"0":{"56":1}}],["e5",{"0":{"57":1}}],["e6",{"0":{"57":1}}],["e8",{"0":{"57":1}}],["e3",{"0":{"57":1}}],["equivalent",{"0":{"61":1}}],["equals",{"0":{"56":1}}],["eqqmwqdt11mtlj3feojglrymfo6tit",{"0":{"57":1}}],["edeploying",{"0":{"56":1}}],["editor",{"0":{"56":3}}],["edited",{"0":{"40":1}}],["editing",{"0":{"16":1,"25":1,"40":1}}],["edit",{"0":{"0":1,"1":1,"2":1,"3":1,"4":1,"5":1,"6":1,"7":1,"8":1,"9":1,"10":1,"11":1,"12":1,"13":1,"14":1,"15":1,"16":1,"17":1,"18":1,"19":1,"20":1,"21":1,"22":2,"23":1,"24":1,"25":2,"26":1,"27":1,"28":1,"29":1,"30":1,"31":2,"32":1,"33":1,"34":1,"35":1,"36":1,"37":1,"38":1,"39":1,"40":2,"41":1,"42":1,"43":1,"44":1,"45":6,"46":1,"47":2,"48":1,"49":1,"50":1,"51":1,"52":1,"53":1,"54":1,"55":1,"56":3,"57":2,"58":1,"59":1,"60":1,"61":1,"62":1,"63":1,"64":1,"65":1,"66":1,"67":3,"68":1,"69":1}}],["eval",{"0":{"56":2}}],["ever",{"0":{"56":1}}],["everything",{"0":{"2":1,"28":1,"57":1}}],["every",{"0":{"2":2,"15":2,"23":1,"24":1,"28":1,"40":1,"42":1,"62":1}}],["even",{"0":{"40":1}}],["eventhandler",{"0":{"54":1}}],["event",{"0":{"49":1}}],["eventually",{"0":{"28":1}}],["events",{"0":{"22":1,"23":1,"46":1}}],["eugine",{"0":{"56":1}}],["eye",{"0":{"56":1}}],["errorcode",{"0":{"30":1}}],["errors",{"0":{"30":1,"61":1}}],["error",{"0":{"29":1,"56":2,"57":3,"61":1,"64":2,"68":2}}],["erd",{"0":{"13":1}}],["eof",{"0":{"26":2}}],["estimating",{"0":{"24":1}}],["echo",{"0":{"8":2,"46":2}}],["ec2",{"0":{"4":1,"24":3}}],["elasticsearch",{"0":{"49":3}}],["elasticache",{"0":{"49":1}}],["elsewhere",{"0":{"44":1,"57":1}}],["else",{"0":{"28":1}}],["elb",{"0":{"6":1,"24":1}}],["elt",{"0":{"4":1}}],["electronic",{"0":{"2":1}}],["ep",{"0":{"4":1}}],["easiest",{"0":{"56":1}}],["easy",{"0":{"41":1,"56":1}}],["eastus",{"0":{"45":1}}],["east",{"0":{"2":3,"18":2,"20":6,"26":1,"33":1,"45":1,"56":2}}],["earlier",{"0":{"22":1}}],["eat",{"0":{"7":1,"8":1}}],["each",{"0":{"1":1,"2":2,"3":2,"8":1,"9":2,"10":1,"15":1,"17":1,"24":1,"25":1,"56":3,"61":6}}],["ef50501e3a41",{"0":{"12":2}}],["effective",{"0":{"61":1}}],["effect",{"0":{"2":5,"9":2,"32":2,"53":2}}],["efs",{"0":{"2":6,"3":2,"9":1,"16":1}}],["empty",{"0":{"7":1}}],["email=",{"0":{"14":1}}],["email",{"0":{"2":2,"14":1,"22":3,"35":1,"41":1}}],["emeaelmdmprdtidmdmextractionve",{"0":{"29":1}}],["emea",{"0":{"2":2,"7":1}}],["etc",{"0":{"2":4,"4":2,"6":1,"7":1,"12":1,"17":1,"27":1,"46":2,"53":1,"64":1,"65":1}}],["e",{"0":{"2":5,"4":1,"6":2,"15":1,"18":1,"28":2,"29":2,"30":2,"32":2,"44":1,"45":11,"46":1,"57":3,"59":1}}],["export",{"0":{"41":1}}],["expire",{"0":{"32":2,"57":2}}],["explore",{"0":{"29":3}}],["extract",{"0":{"57":1}}],["extraction",{"0":{"29":1}}],["extra",{"0":{"34":1}}],["extension",{"0":{"25":3}}],["extensions",{"0":{"13":1,"25":2,"40":2,"56":3}}],["external",{"0":{"0":1,"2":3,"3":1,"6":2,"9":1,"15":1,"18":3,"44":1,"68":1}}],["exception=oserror",{"0":{"64":1}}],["exceeding",{"0":{"43":1}}],["excel",{"0":{"25":1}}],["excerpt",{"0":{"23":1}}],["exact",{"0":{"40":1}}],["exactly",{"0":{"22":1}}],["examples",{"0":{"29":2,"49":1}}],["example",{"0":{"2":2,"4":1,"7":1,"9":3,"11":1,"15":1,"21":1,"22":1,"24":1,"32":1,"33":1,"54":1,"56":1,"57":2,"62":1}}],["exist",{"0":{"49":1,"56":2}}],["existing",{"0":{"10":1,"15":1,"17":1,"20":2,"34":1,"56":1}}],["exists",{"0":{"8":1,"45":1,"67":1}}],["exit",{"0":{"4":1,"56":1,"61":3,"63":1,"64":1}}],["execution",{"0":{"61":1}}],["execute",{"0":{"56":2,"63":3}}],["executed",{"0":{"23":1}}],["executor",{"0":{"21":1}}],["executors",{"0":{"7":1}}],["exec",{"0":{"15":1,"23":1,"26":1,"27":1,"38":1,"54":1,"56":1,"65":1}}],["exe",{"0":{"4":1}}],["eksctl",{"0":{"14":9}}],["eks",{"0":{"2":8,"4":1,"9":1,"10":2,"14":11,"15":3,"17":2,"26":2,"62":3}}],["engaging",{"0":{"56":1}}],["engineers",{"0":{"2":1}}],["engine",{"0":{"2":1,"9":1,"63":6}}],["enjoy",{"0":{"40":1}}],["enhancements",{"0":{"28":1,"47":2}}],["enis",{"0":{"24":1}}],["eni",{"0":{"24":1}}],["endpoint",{"0":{"33":1}}],["endpoints",{"0":{"2":1}}],["ends",{"0":{"22":1}}],["end",{"0":{"22":2,"40":1,"56":2,"57":4}}],["encrypt",{"0":{"15":1,"35":1,"57":1}}],["encrypted",{"0":{"3":1,"15":1,"16":1}}],["en",{"0":{"13":1}}],["entire",{"0":{"57":1}}],["entity",{"0":{"13":1}}],["entry",{"0":{"56":1,"61":1,"66":1}}],["entra",{"0":{"52":1}}],["entries",{"0":{"0":1,"23":1}}],["enters",{"0":{"22":1}}],["enter",{"0":{"7":1,"15":1,"56":1,"57":1}}],["envoy",{"0":{"68":1}}],["environmet",{"0":{"29":1}}],["environments",{"0":{"3":1,"16":2,"23":1,"30":1,"40":1,"56":1,"57":1,"62":2,"68":1}}],["environment",{"0":{"2":3,"7":1,"9":2,"14":1,"21":1,"22":1,"26":1,"29":3,"33":1,"40":2,"49":3,"53":1,"56":3,"58":1,"61":1,"62":3,"64":1}}],["enviornment",{"0":{"26":1}}],["envs",{"0":{"25":2}}],["env=",{"0":{"21":1}}],["env",{"0":{"3":2,"16":2,"42":1,"52":1,"53":1,"57":3}}],["enable",{"0":{"2":2,"5":2,"9":1,"20":1,"42":1,"49":1,"56":1}}],["enabled",{"0":{"0":2,"2":1,"9":1,"22":1,"24":1,"29":1,"56":1}}],["ens2",{"0":{"29":1}}],["ensure",{"0":{"2":1,"4":1,"5":2,"12":1,"61":1}}],["ensembledev",{"0":{"15":3,"24":1}}],["ensembletest",{"0":{"2":1,"6":1,"53":1}}],["ensemble",{"0":{"1":2,"2":8,"6":1,"7":2,"9":2,"15":1,"17":2,"53":1}}],["either",{"0":{"0":1,"2":2,"34":1}}],["1816",{"0":{"63":1}}],["187",{"0":{"4":1}}],["13",{"0":{"68":2}}],["1335",{"0":{"63":1}}],["138",{"0":{"4":2,"56":1}}],["1d",{"0":{"57":1}}],["1b5ea827f87280f98620dccc1600727c",{"0":{"56":1}}],["1x",{"0":{"56":2}}],["1e9",{"0":{"54":1}}],["1636",{"0":{"63":1}}],["16384",{"0":{"46":1}}],["160",{"0":{"27":1}}],["16",{"0":{"25":1,"67":1}}],["15000",{"0":{"68":1}}],["150",{"0":{"27":1}}],["15",{"0":{"25":1,"57":1,"68":2}}],["157",{"0":{"4":2,"56":1}}],["115",{"0":{"68":2}}],["1151",{"0":{"63":1}}],["11",{"0":{"42":1,"57":1}}],["110",{"0":{"24":2}}],["1134",{"0":{"23":1}}],["113",{"0":{"23":1}}],["1pswd",{"0":{"12":1}}],["1password",{"0":{"11":1,"12":2,"35":1,"55":1}}],["127",{"0":{"68":1}}],["120",{"0":{"63":1}}],["1243",{"0":{"63":1}}],["1211",{"0":{"23":2}}],["12",{"0":{"11":3,"57":5}}],["123",{"0":{"4":1}}],["125ea29c302df7dbb900ed84aa85f0bb",{"0":{"2":3}}],["128",{"0":{"0":4}}],["17t13",{"0":{"68":2}}],["172",{"0":{"27":2}}],["17",{"0":{"2":3,"9":1,"32":1,"53":1}}],["1024",{"0":{"46":1}}],["100",{"0":{"8":1,"43":1}}],["100gb",{"0":{"2":1,"9":1}}],["10",{"0":{"2":3,"4":3,"9":1,"12":26,"20":1,"25":3,"27":1,"32":1,"42":1,"45":4,"53":1,"56":2,"68":9}}],["1tb",{"0":{"2":1,"9":1}}],["1960d75beac7",{"0":{"56":1}}],["19",{"0":{"2":1,"3":1}}],["1",{"0":{"0":4,"1":1,"2":4,"3":1,"5":1,"9":3,"11":1,"12":2,"15":3,"20":1,"22":3,"23":8,"24":5,"25":7,"26":1,"28":4,"33":1,"40":2,"42":1,"46":1,"54":1,"56":43,"57":4,"65":1,"67":1,"68":1}}],["14",{"0":{"0":1,"2":1,"9":1,"25":1}}],["b1",{"0":{"57":1}}],["bb",{"0":{"57":1}}],["b0",{"0":{"57":1}}],["b9c5",{"0":{"56":1}}],["b631",{"0":{"45":1}}],["b302",{"0":{"45":1}}],["b56a",{"0":{"45":1}}],["blocked",{"0":{"66":1}}],["block",{"0":{"57":2}}],["blocks",{"0":{"56":2,"57":2}}],["blobs",{"0":{"32":1}}],["blob",{"0":{"32":4,"33":3,"56":2}}],["blank",{"0":{"44":1}}],["blue",{"0":{"42":1}}],["b",{"0":{"34":1}}],["b818",{"0":{"26":2}}],["bd0f1a58014fcf446b668a876ee7df2a",{"0":{"26":1}}],["bit",{"0":{"56":1}}],["bitbucket",{"0":{"2":1,"5":6,"7":1,"42":1}}],["bi",{"0":{"42":2}}],["bin",{"0":{"26":4,"38":1,"46":4,"56":3,"63":1,"65":1}}],["billing",{"0":{"22":8,"23":4}}],["bottom",{"0":{"56":1}}],["both",{"0":{"2":1,"40":1,"52":2,"56":1,"57":1}}],["book",{"0":{"61":1}}],["boost",{"0":{"46":1}}],["bootstrap",{"0":{"37":1}}],["body",{"0":{"45":1}}],["bore",{"0":{"25":1}}],["box",{"0":{"7":1,"56":1}}],["bunch",{"0":{"57":1}}],["business",{"0":{"42":2,"56":1}}],["built",{"0":{"34":1,"47":1}}],["build",{"0":{"5":1,"11":2,"12":1,"15":1,"34":5,"40":3,"42":1,"59":3,"61":1}}],["bug",{"0":{"28":1,"47":1}}],["bump",{"0":{"28":2}}],["buckets",{"0":{"9":1}}],["bucket",{"0":{"2":21,"9":5,"32":3,"33":2,"53":10}}],["button",{"0":{"7":1,"12":1,"22":1,"25":1,"40":1,"52":1}}],["but",{"0":{"0":2,"3":1,"25":2,"28":2,"39":1,"47":2,"49":1,"56":5,"61":3,"64":1,"67":1,"68":1}}],["bad",{"0":{"61":1,"68":1}}],["bag",{"0":{"57":1}}],["bat",{"0":{"56":1}}],["baltocdn",{"0":{"46":2}}],["balboa",{"0":{"42":1}}],["balancer",{"0":{"2":2}}],["back",{"0":{"49":2,"56":7,"57":2}}],["background",{"0":{"40":1,"61":2}}],["backend",{"0":{"23":1}}],["bar",{"0":{"25":1}}],["bank",{"0":{"22":4}}],["basic",{"0":{"58":1,"65":1}}],["basically",{"0":{"56":1}}],["basis",{"0":{"21":1}}],["bash",{"0":{"15":1,"23":1,"38":2,"54":1,"56":2,"65":3}}],["bashrc",{"0":{"4":1,"56":2}}],["bastions",{"0":{"56":1}}],["bastion",{"0":{"1":2,"4":1,"26":2,"37":1,"55":1,"56":8}}],["base64",{"0":{"8":2,"41":3}}],["based",{"0":{"2":1,"10":1,"15":1,"17":1,"40":1,"52":1,"56":1}}],["base",{"0":{"0":2,"6":2,"17":2,"21":1,"35":4,"56":1,"57":5,"62":2,"63":1}}],["bhe",{"0":{"2":1}}],["breaking",{"0":{"47":2}}],["brew",{"0":{"14":2,"56":5}}],["bru",{"0":{"45":4}}],["bring",{"0":{"34":1}}],["brand",{"0":{"22":1}}],["branch",{"0":{"2":1,"5":1,"34":9,"42":5,"47":1,"55":2,"56":2,"59":1}}],["broker",{"0":{"49":1}}],["broad",{"0":{"15":1}}],["browseresource",{"0":{"56":1}}],["browse",{"0":{"1":1,"2":8,"3":1}}],["bytes",{"0":{"43":4}}],["by",{"0":{"1":2,"2":4,"3":1,"7":2,"8":1,"15":1,"18":2,"22":7,"23":2,"24":1,"28":2,"29":2,"40":2,"43":10,"47":1,"49":1,"52":1,"56":2,"61":6,"62":2,"63":1}}],["best",{"0":{"61":1}}],["beat",{"0":{"56":4}}],["bec",{"0":{"43":1}}],["because",{"0":{"2":1,"24":1,"56":1,"57":1}}],["behave",{"0":{"49":1}}],["behaviors",{"0":{"5":1}}],["behind",{"0":{"34":1}}],["better",{"0":{"25":1}}],["between",{"0":{"22":2,"56":2,"57":1,"61":2}}],["being",{"0":{"16":1,"25":1,"28":1,"40":1,"53":1}}],["begins",{"0":{"22":2}}],["begin",{"0":{"10":1,"47":1,"57":6}}],["been",{"0":{"2":1,"8":1,"17":1,"23":1,"25":1}}],["before",{"0":{"2":1,"7":1,"10":1,"15":1,"47":1,"56":3,"57":4,"59":1,"61":2}}],["below",{"0":{"2":3,"3":1,"29":1,"53":1,"57":1}}],["be",{"0":{"0":4,"1":1,"2":8,"3":4,"4":1,"5":3,"9":1,"10":2,"12":2,"15":5,"16":2,"17":1,"18":3,"20":1,"22":3,"23":1,"24":3,"28":1,"32":3,"33":1,"34":1,"39":1,"40":2,"45":3,"47":2,"49":1,"52":1,"53":1,"56":10,"57":16,"59":1,"61":11,"62":1,"67":1,"68":1}}],["md5",{"0":{"57":4}}],["md",{"0":{"47":1,"56":2,"62":1}}],["mdm",{"0":{"29":1}}],["mc",{"0":{"45":1}}],["mv",{"0":{"26":1}}],["mkdir",{"0":{"26":2,"39":1,"56":1}}],["ms",{"0":{"25":1}}],["m5",{"0":{"9":1,"24":3}}],["m",{"0":{"3":1,"4":2,"16":1,"56":1,"57":1}}],["mozilla",{"0":{"68":2}}],["moveresources",{"0":{"45":1}}],["move",{"0":{"41":1,"45":4}}],["mountpath",{"0":{"27":1}}],["mount",{"0":{"24":1,"56":1}}],["mounted",{"0":{"3":1,"10":1,"17":1}}],["monthly",{"0":{"22":1}}],["monitoring",{"0":{"23":2,"49":1}}],["monitor",{"0":{"12":1}}],["more",{"0":{"9":1,"13":1,"22":1,"23":1,"25":1,"30":1,"34":1,"40":1,"41":1,"45":1,"50":1,"57":1,"61":2,"67":2}}],["module",{"0":{"61":3,"63":1}}],["modulus",{"0":{"57":6}}],["modified",{"0":{"22":1,"39":3,"45":1,"57":1}}],["modifies",{"0":{"22":1}}],["modify",{"0":{"5":1,"20":1,"22":3}}],["modern",{"0":{"56":2}}],["mode",{"0":{"20":1,"51":3}}],["models",{"0":{"13":2,"63":3}}],["model",{"0":{"7":1,"22":1,"25":1,"61":1,"68":2}}],["most",{"0":{"3":1,"24":1,"56":1,"57":1,"61":3,"62":1,"63":1}}],["moments",{"0":{"56":1}}],["moment",{"0":{"2":1}}],["much",{"0":{"22":1,"56":1}}],["multiple",{"0":{"57":2,"61":1}}],["multibranch",{"0":{"5":1}}],["multi",{"0":{"2":2,"9":1}}],["must",{"0":{"0":4,"7":3,"10":2,"15":2,"17":1,"24":2,"32":6,"33":2,"34":2,"45":1,"46":1,"56":4,"57":1,"61":3}}],["miievaibadanbgkqhkig9w0baqefaascbkywggsiageaaoibaqclf9q17cqlowdb",{"0":{"57":1}}],["miiejtccavwgawibagiqq71eg0d4110tqpc8i8ur",{"0":{"57":1}}],["mirror",{"0":{"39":1}}],["mirrors",{"0":{"39":1}}],["missing",{"0":{"23":1}}],["migrating",{"0":{"18":1}}],["migrations",{"0":{"15":1,"56":4}}],["migrated",{"0":{"18":1}}],["migrate",{"0":{"15":1,"56":1}}],["might",{"0":{"2":2,"40":2,"59":1,"66":1}}],["mixin",{"0":{"13":1}}],["mimimize",{"0":{"2":1}}],["microsoft",{"0":{"1":1,"4":1,"12":1,"20":2,"45":1,"52":1,"56":1}}],["minavailable",{"0":{"56":16}}],["minute",{"0":{"40":1,"57":1}}],["minio",{"0":{"33":4}}],["minimum",{"0":{"0":1,"2":1,"9":1,"14":1,"34":1}}],["minor",{"0":{"28":6}}],["mind",{"0":{"2":1,"18":1}}],["min",{"0":{"0":3,"9":3,"20":1,"56":2}}],["message",{"0":{"68":2}}],["mechanism",{"0":{"61":1}}],["mechatroner",{"0":{"25":1}}],["meat",{"0":{"61":1}}],["measued",{"0":{"43":1}}],["means",{"0":{"22":1,"45":1,"61":2}}],["memberships",{"0":{"52":1}}],["memtotal",{"0":{"43":1}}],["memavailable",{"0":{"43":1}}],["memory",{"0":{"43":6,"61":3}}],["memorypressure",{"0":{"43":1}}],["menu",{"0":{"25":1}}],["method",{"0":{"61":1,"68":1}}],["methods",{"0":{"61":1}}],["metadata",{"0":{"22":1,"27":1,"45":2,"49":2}}],["metered",{"0":{"22":2}}],["metrics",{"0":{"11":5,"14":3,"56":2}}],["medical",{"0":{"7":1}}],["merge",{"0":{"5":1,"34":1,"58":1}}],["me",{"0":{"1":1}}],["my",{"0":{"4":2,"11":3,"22":1,"29":5,"56":1}}],["myworkspaces",{"0":{"1":1}}],["mycompany",{"0":{"0":3}}],["matter",{"0":{"56":1}}],["mate",{"0":{"40":1}}],["matched",{"0":{"57":1}}],["matches",{"0":{"57":1}}],["match",{"0":{"22":1,"47":1,"57":3}}],["making",{"0":{"34":1,"56":1}}],["makes",{"0":{"56":1}}],["make",{"0":{"2":1,"3":2,"7":1,"15":1,"25":1,"28":2,"34":4,"39":2,"47":3,"49":1,"52":1,"56":8,"57":6,"59":1,"61":2,"63":2}}],["major",{"0":{"28":7}}],["mapping",{"0":{"57":1}}],["map",{"0":{"27":1}}],["macintosh",{"0":{"68":2}}],["macs",{"0":{"56":1}}],["mac",{"0":{"14":1,"68":2}}],["machine",{"0":{"3":1,"6":1,"15":1,"39":2,"41":2,"46":2,"56":1}}],["martin",{"0":{"7":1}}],["marked",{"0":{"16":1}}],["marks",{"0":{"3":1,"22":2}}],["mark",{"0":{"3":1}}],["may",{"0":{"3":1,"56":1,"57":2}}],["maintenance",{"0":{"51":3}}],["maintain",{"0":{"4":1,"12":1}}],["main",{"0":{"3":1,"7":1,"16":1,"25":1,"30":1,"34":2,"39":1,"46":2,"56":8,"57":1,"61":1,"63":3,"64":1}}],["mail",{"0":{"2":1}}],["master",{"0":{"2":4,"9":2,"18":1,"49":2}}],["manifest",{"0":{"34":1}}],["managing",{"0":{"2":1,"3":1,"14":2,"40":1,"61":1}}],["manages",{"0":{"15":1}}],["manage",{"0":{"5":1,"7":1,"13":1,"15":2,"20":1,"22":1,"47":1,"49":1,"54":1,"56":2,"62":1}}],["management",{"0":{"2":1,"32":2}}],["managed",{"0":{"2":4,"3":1,"8":1,"22":2}}],["manager",{"0":{"0":1,"2":3,"15":4,"54":1}}],["manually",{"0":{"2":2,"22":3,"42":2,"55":1,"56":1,"57":1}}],["manual",{"0":{"2":2,"61":1}}],["maxconcurrentreconciles",{"0":{"61":1}}],["maximum",{"0":{"2":1,"9":1,"24":2,"29":1}}],["max",{"0":{"0":3,"9":3,"20":1,"24":3,"46":3,"54":1,"56":2}}],["vulnerabilities",{"0":{"50":1}}],["v",{"0":{"29":1}}],["v0",{"0":{"26":2,"46":1}}],["vsc",{"0":{"25":1}}],["vscode",{"0":{"25":5}}],["vsix",{"0":{"25":13,"40":1}}],["vs",{"0":{"25":16,"56":1}}],["vm",{"0":{"20":1,"56":2}}],["vma",{"0":{"7":2}}],["v2",{"0":{"14":1,"49":1}}],["vpcxeksrole",{"0":{"26":1}}],["vpcx",{"0":{"5":2}}],["vpn",{"0":{"1":1,"12":5,"56":4,"57":1}}],["venv",{"0":{"56":3,"57":1}}],["vendor",{"0":{"4":1}}],["ve",{"0":{"16":1}}],["verified",{"0":{"57":1}}],["verification",{"0":{"57":1}}],["verifying",{"0":{"56":1}}],["verify",{"0":{"2":1,"12":1,"49":1,"56":2,"57":7}}],["very",{"0":{"12":1,"34":2,"49":1,"56":2,"67":1}}],["versioning",{"0":{"28":3}}],["versions",{"0":{"2":1,"18":1,"25":1,"32":2,"56":3,"65":2}}],["version",{"0":{"0":2,"2":6,"9":2,"11":1,"15":2,"24":4,"28":4,"32":1,"34":1,"39":1,"53":1,"56":14,"57":5,"58":2,"61":2,"63":1,"64":1,"65":1,"67":2}}],["vary",{"0":{"57":1}}],["varies",{"0":{"56":2}}],["variant",{"0":{"22":2}}],["variables",{"0":{"21":1,"52":1,"53":1}}],["variable",{"0":{"3":1,"64":1}}],["vars",{"0":{"25":1}}],["validating",{"0":{"44":1}}],["validations",{"0":{"25":1}}],["validated",{"0":{"44":1}}],["validate",{"0":{"42":1}}],["validity",{"0":{"22":1,"57":4}}],["valid",{"0":{"7":2,"8":1}}],["valueerror",{"0":{"63":1}}],["value=",{"0":{"21":1}}],["values",{"0":{"3":3,"16":2,"32":3,"46":2,"54":1,"61":3,"67":8,"68":2}}],["value",{"0":{"0":3,"3":1,"8":3,"9":3,"27":2,"52":1,"61":2,"63":1,"67":1}}],["v1beta1",{"0":{"26":1}}],["v1envvar",{"0":{"21":1}}],["v1container",{"0":{"21":1}}],["v1podspec",{"0":{"21":1}}],["v1pod",{"0":{"21":1}}],["v1",{"0":{"2":1,"3":1,"14":3,"26":1,"27":1,"45":2,"54":1,"56":1,"61":1}}],["virtual",{"0":{"46":1}}],["virtualnetworkgateways",{"0":{"12":1}}],["visibility",{"0":{"18":1}}],["vi",{"0":{"4":1,"57":2,"67":1}}],["view",{"0":{"56":1}}],["viewers",{"0":{"7":1}}],["viewer",{"0":{"2":1}}],["viewpage",{"0":{"1":1}}],["via",{"0":{"0":1,"2":1,"12":1,"22":1,"56":1}}],["volumename",{"0":{"45":1}}],["volumehandle",{"0":{"45":1}}],["volumemounts",{"0":{"27":1}}],["volumes",{"0":{"23":1,"24":3,"27":1,"45":4,"49":1}}],["volume",{"0":{"0":3,"3":1,"9":3,"16":1,"24":3,"45":11,"49":1,"56":1}}],["volumed",{"0":{"0":4,"2":1,"9":3}}],["v3",{"0":{"0":2,"20":1,"56":2}}],["v5",{"0":{"0":2}}],["d2",{"0":{"57":1}}],["dh",{"0":{"49":1}}],["drop",{"0":{"49":1}}],["draft",{"0":{"34":1}}],["driver",{"0":{"2":4,"3":2,"45":1}}],["dnr240",{"0":{"29":2}}],["dns",{"0":{"0":4,"2":4,"6":1,"9":1,"12":2,"15":2,"27":3,"35":4,"68":1}}],["django",{"0":{"13":1,"18":1,"22":2,"31":1,"44":1,"50":1}}],["dcmaster",{"0":{"18":2}}],["dcp",{"0":{"15":2}}],["dco",{"0":{"11":5}}],["dcw",{"0":{"2":1,"29":7,"38":1,"43":1,"45":5,"49":6,"56":4,"67":3}}],["duration",{"0":{"68":1}}],["during",{"0":{"3":1,"22":2}}],["duplicate",{"0":{"25":1}}],["dump",{"0":{"18":4}}],["due",{"0":{"2":1,"5":1,"61":2,"64":1}}],["dir",{"0":{"64":1}}],["directions",{"0":{"57":1}}],["directly",{"0":{"22":1,"34":3,"61":1}}],["directory",{"0":{"2":1,"4":2,"6":1,"10":1,"14":1,"15":3,"17":2,"34":1,"47":4,"52":1,"56":6,"57":6,"62":3}}],["dill",{"0":{"63":6}}],["did",{"0":{"62":1}}],["digicert",{"0":{"57":10}}],["dict",{"0":{"22":1}}],["diagram",{"0":{"13":1}}],["diffing",{"0":{"61":1}}],["difference",{"0":{"56":1}}],["different",{"0":{"7":1,"22":1,"33":1,"56":1,"57":1,"61":2,"68":1}}],["diff",{"0":{"3":1,"16":1,"34":1,"39":3}}],["disruption",{"0":{"56":2}}],["disruptions",{"0":{"2":1,"56":4}}],["distinction",{"0":{"61":1}}],["distinguish",{"0":{"22":1}}],["distributed",{"0":{"54":1}}],["disable",{"0":{"49":1,"56":3}}],["discounted",{"0":{"22":1}}],["displayed",{"0":{"15":1}}],["disks",{"0":{"45":1}}],["diskpressure",{"0":{"43":1}}],["disk",{"0":{"0":1,"45":10,"56":1}}],["division",{"0":{"2":5}}],["dbshell",{"0":{"49":1}}],["db",{"0":{"2":4,"3":5,"9":2,"16":4,"18":5,"57":1,"68":1}}],["dbtrunnerresult",{"0":{"30":2,"64":1}}],["dbtrunner",{"0":{"30":2,"64":3}}],["dbt",{"0":{"2":6,"3":1,"9":2,"12":2,"25":2,"28":4,"30":7,"42":7,"53":3,"56":3,"58":3,"64":10,"65":3}}],["dagrun",{"0":{"63":3}}],["dag",{"0":{"58":1,"63":7}}],["dags",{"0":{"2":6,"9":1,"56":1,"63":1}}],["days",{"0":{"32":5}}],["dashboard",{"0":{"5":1,"14":10}}],["dates",{"0":{"22":1}}],["date",{"0":{"3":1,"22":1,"29":1,"44":1,"57":3}}],["datasource",{"0":{"29":2}}],["datahubproject",{"0":{"67":1}}],["datahub",{"0":{"22":1,"28":3,"49":11,"67":8}}],["data",{"0":{"2":2,"3":1,"5":1,"8":1,"16":1,"18":2,"22":1,"26":1,"27":1,"32":1,"42":1,"49":2,"54":1,"56":1,"57":4}}],["datacove",{"0":{"2":2,"4":3,"26":1}}],["datacoveslocal",{"0":{"22":2,"33":1,"59":3,"62":2}}],["datacovesgateway",{"0":{"12":1}}],["datacoves",{"0":{"0":8,"1":1,"2":31,"3":9,"4":8,"6":6,"7":17,"9":3,"10":3,"12":22,"15":15,"16":3,"17":7,"18":14,"20":11,"22":16,"23":1,"25":17,"28":4,"37":1,"39":9,"42":1,"45":1,"46":2,"47":2,"51":1,"52":4,"53":2,"56":32,"57":16,"59":1,"61":1,"62":1,"65":3,"68":2}}],["databases",{"0":{"2":2,"18":1}}],["database",{"0":{"0":2,"2":10,"9":1,"15":1,"18":7,"61":1}}],["d",{"0":{"2":1,"7":1,"8":2,"18":3,"35":2,"41":2,"46":2,"50":1,"65":1}}],["dorzey",{"0":{"25":1}}],["doing",{"0":{"22":1,"34":1}}],["doesn",{"0":{"23":1,"57":2}}],["does",{"0":{"22":2,"56":2,"61":1,"64":1,"67":1,"68":1}}],["doubt",{"0":{"3":1}}],["downgrade",{"0":{"56":1}}],["down",{"0":{"22":1,"56":2}}],["downtimes",{"0":{"2":1}}],["downloaded",{"0":{"12":1,"47":2}}],["download",{"0":{"1":1,"2":1,"8":1,"11":3,"12":2,"14":1,"20":1,"26":2,"34":3,"47":1,"56":1}}],["done",{"0":{"2":1,"7":1,"17":1,"34":1,"38":1,"49":1,"52":1,"56":5,"57":2}}],["don",{"0":{"2":1,"5":1,"22":2,"34":4,"40":1,"44":1,"57":1}}],["do",{"0":{"2":3,"3":2,"7":1,"22":2,"23":1,"28":3,"29":1,"34":6,"38":1,"44":1,"46":1,"47":1,"52":1,"56":9,"57":2,"59":1,"62":2,"63":2,"67":2}}],["doc",{"0":{"46":1}}],["document",{"0":{"15":2,"22":1,"47":1,"56":3,"57":1}}],["documented",{"0":{"3":2,"8":1,"56":2}}],["documentation",{"0":{"2":1,"27":1,"39":1,"56":3,"61":1,"62":1}}],["dockerconfigjson",{"0":{"8":1}}],["docker",{"0":{"2":2,"4":2,"5":1,"7":1,"8":11,"14":8,"15":3,"28":1,"34":1,"46":3,"57":1,"58":2,"62":1,"65":6,"66":6}}],["docs",{"0":{"1":1,"2":3,"3":1,"12":2,"14":4,"21":1,"24":1,"30":1,"42":6,"56":9,"57":2,"62":3}}],["domain=ensembletest",{"0":{"17":1,"39":1}}],["domain=ensembledev",{"0":{"15":1}}],["domain=fill",{"0":{"6":1}}],["domain",{"0":{"0":6,"2":3,"3":3,"6":3,"10":1,"12":1,"15":6,"17":4,"23":2,"29":2,"33":1,"35":8,"39":2,"51":2,"52":1,"57":1,"62":1}}],["derived",{"0":{"61":2}}],["de",{"0":{"57":1}}],["delivered",{"0":{"47":1}}],["deliver",{"0":{"47":1}}],["delegated",{"0":{"52":1}}],["delegation",{"0":{"24":1}}],["deleted",{"0":{"25":1,"45":2,"49":2}}],["delete",{"0":{"8":2,"23":1,"32":3,"45":7,"47":2,"49":6,"56":1,"57":1,"63":1}}],["deleteobjectversion",{"0":{"2":2,"9":1,"32":1,"53":1}}],["deleteobject",{"0":{"2":2,"9":1,"32":1,"53":1}}],["deal",{"0":{"44":1}}],["debian",{"0":{"46":2}}],["deb",{"0":{"46":2}}],["debt",{"0":{"22":1}}],["debugging",{"0":{"64":1,"65":1}}],["debug",{"0":{"21":1,"29":1,"30":2,"35":2,"65":1}}],["detached",{"0":{"46":1}}],["detailed",{"0":{"23":1}}],["detail",{"0":{"4":1}}],["details",{"0":{"2":2,"4":1,"9":1,"32":1,"56":3,"68":1}}],["determine",{"0":{"52":1,"61":1}}],["determined",{"0":{"22":1}}],["detected",{"0":{"61":2}}],["detection",{"0":{"61":2}}],["detects",{"0":{"61":1}}],["detect",{"0":{"15":1}}],["decisions",{"0":{"63":2}}],["decrypt",{"0":{"15":2}}],["declined",{"0":{"5":1}}],["defaulted",{"0":{"32":3}}],["defaults",{"0":{"21":1}}],["default",{"0":{"8":6,"25":1,"40":1,"45":2,"56":3}}],["defined",{"0":{"34":1,"61":1}}],["define",{"0":{"2":2}}],["definitions",{"0":{"62":1}}],["definition",{"0":{"2":1}}],["deprecated",{"0":{"28":1}}],["deprecate",{"0":{"28":1}}],["dependency",{"0":{"34":1}}],["dependencies",{"0":{"5":1,"6":2,"17":2,"28":1,"34":1,"46":1}}],["depending",{"0":{"22":1,"28":1,"29":1,"42":1}}],["depend",{"0":{"12":1}}],["deps",{"0":{"12":1,"30":2}}],["deployed",{"0":{"15":2}}],["deploying",{"0":{"6":1,"15":4,"56":1}}],["deploy",{"0":{"3":2,"6":1,"14":1,"15":3,"39":1,"42":1,"56":2,"58":1}}],["deploys",{"0":{"2":1}}],["deployments",{"0":{"12":1,"15":2,"56":2,"61":1}}],["deployment",{"0":{"1":1,"3":5,"4":2,"6":2,"10":3,"12":1,"15":6,"17":3,"27":4,"37":1,"39":7,"56":4,"61":1}}],["destination",{"0":{"45":3}}],["desired",{"0":{"30":1,"47":1,"61":1}}],["describing",{"0":{"61":1}}],["describes",{"0":{"15":1,"45":1}}],["describe",{"0":{"4":1,"14":2,"47":1,"56":1,"57":1}}],["described",{"0":{"2":1,"22":1,"56":1}}],["descriptions",{"0":{"22":1}}],["description",{"0":{"2":2}}],["desktop",{"0":{"1":1,"4":2}}],["devs",{"0":{"59":1}}],["dev001",{"0":{"38":1}}],["devops",{"0":{"20":1,"62":1}}],["devices",{"0":{"7":1}}],["devusr",{"0":{"5":1,"7":1}}],["dev123",{"0":{"2":2,"3":1,"12":9,"16":2,"18":1,"45":5,"49":1,"56":8,"67":6,"68":2}}],["developers",{"0":{"22":1}}],["developer",{"0":{"2":1,"7":9,"22":3,"62":1}}],["development",{"0":{"2":3,"26":1,"33":2,"39":1,"56":2}}],["dev",{"0":{"1":2,"2":2,"5":1,"7":9,"13":1,"56":1,"62":1}}],["d16s",{"0":{"0":1}}],["d4s",{"0":{"0":2,"20":1}}],["d4ds",{"0":{"0":1}}],["g2",{"0":{"57":5}}],["gmt",{"0":{"57":10}}],["g1",{"0":{"57":6}}],["gain",{"0":{"56":1}}],["gateway",{"0":{"54":1}}],["gave",{"0":{"46":1}}],["gnupg2",{"0":{"46":1}}],["gz",{"0":{"26":3}}],["gpg",{"0":{"41":6,"46":1}}],["gpt",{"0":{"25":1}}],["gp2",{"0":{"2":1,"9":1}}],["guitar",{"0":{"22":1}}],["guide",{"0":{"2":1,"3":1,"23":1,"45":1,"52":1}}],["global",{"0":{"5":1,"7":1,"57":5,"62":1}}],["gt",{"0":{"4":4,"5":7,"8":7,"11":2,"12":4,"14":3,"18":2,"22":1,"26":1,"28":1,"29":2,"30":1,"32":2,"33":5,"35":9,"41":1,"43":4,"45":14,"51":4,"56":1,"57":3,"59":1,"63":3,"64":9,"65":1,"67":12}}],["goroutines",{"0":{"61":3}}],["gone",{"0":{"49":1}}],["golang",{"0":{"46":1}}],["google",{"0":{"46":1}}],["good",{"0":{"34":2,"56":1}}],["got",{"0":{"22":1,"25":1,"30":1,"46":1}}],["going",{"0":{"11":1,"34":1,"40":1,"52":1,"56":2,"57":1}}],["go",{"0":{"4":1,"5":1,"7":2,"22":2,"27":1,"29":5,"31":2,"44":1,"46":3,"49":2,"52":1,"55":1,"56":4,"57":4,"61":12,"66":1}}],["g",{"0":{"2":2,"6":2,"13":1,"15":1,"28":2,"29":4,"32":2,"45":11,"46":1}}],["great",{"0":{"56":1}}],["green",{"0":{"42":1}}],["grep",{"0":{"3":1,"6":1,"14":2,"38":2,"45":1,"49":5}}],["gr7",{"0":{"26":1}}],["grid",{"0":{"25":1}}],["grace",{"0":{"61":1}}],["graph",{"0":{"13":2,"52":1}}],["graphviz",{"0":{"13":1}}],["grafana",{"0":{"2":2,"9":1,"12":1,"29":4,"31":3,"32":1,"33":4,"54":1}}],["granted",{"0":{"22":1,"52":1}}],["grants",{"0":{"2":1}}],["grant",{"0":{"2":5,"3":1,"4":1,"7":1,"18":2,"31":1}}],["group",{"0":{"2":2,"3":1,"4":1,"7":1,"12":1,"20":6,"31":2,"32":1,"45":8,"52":1,"54":1}}],["groups",{"0":{"0":1,"2":7,"4":2,"7":3,"9":1,"23":1,"31":1,"45":1,"52":4,"56":1}}],["growth",{"0":{"0":1,"22":3}}],["gives",{"0":{"64":1}}],["give",{"0":{"3":1}}],["gitsecret",{"0":{"57":1}}],["gitlab",{"0":{"42":1}}],["gitignore",{"0":{"42":1}}],["git",{"0":{"2":3,"3":16,"4":2,"6":5,"9":1,"10":1,"15":10,"16":11,"17":5,"28":1,"29":7,"34":6,"39":20,"41":1,"42":3,"44":1,"56":4,"57":10,"62":1}}],["githubusercontent",{"0":{"14":1}}],["github",{"0":{"0":1,"1":1,"2":1,"3":1,"4":1,"5":1,"6":1,"7":1,"8":1,"9":1,"10":1,"11":1,"12":1,"13":1,"14":2,"15":1,"16":1,"17":1,"18":1,"19":1,"20":1,"21":1,"22":1,"23":1,"24":1,"25":2,"26":3,"27":1,"28":2,"29":1,"30":1,"31":1,"32":1,"33":1,"34":3,"35":2,"36":1,"37":1,"38":1,"39":2,"40":1,"41":1,"42":2,"43":1,"44":1,"45":1,"46":5,"47":3,"48":1,"49":1,"50":1,"51":1,"52":1,"53":1,"54":1,"55":1,"56":11,"57":3,"58":1,"59":2,"60":1,"61":1,"62":1,"63":1,"64":1,"65":1,"66":1,"67":1,"68":1,"69":1}}],["gig",{"0":{"0":1}}],["gib",{"0":{"0":1}}],["gecko",{"0":{"68":2}}],["gentle",{"0":{"61":1}}],["generation",{"0":{"61":1}}],["generating",{"0":{"22":1,"47":1,"57":1}}],["generates",{"0":{"14":1}}],["generate",{"0":{"2":1,"14":1,"28":2,"34":1,"39":2,"42":1,"52":2,"57":1,"59":2,"67":1}}],["generated",{"0":{"2":1,"28":1,"39":1,"47":4}}],["generally",{"0":{"56":1}}],["generallarge",{"0":{"20":1}}],["general",{"0":{"0":5,"2":1,"9":3,"15":1,"32":1}}],["gets",{"0":{"49":1,"61":1}}],["getdbt",{"0":{"30":1}}],["getbucketlocation",{"0":{"2":2,"9":1,"53":1}}],["getobjectversion",{"0":{"2":2,"9":1,"32":1,"53":1}}],["getobject",{"0":{"2":2,"9":1,"32":1,"53":1}}],["get",{"0":{"2":1,"4":2,"6":1,"7":1,"8":5,"12":5,"14":4,"15":4,"20":2,"26":1,"29":2,"32":1,"34":1,"35":1,"37":2,"38":1,"40":1,"45":17,"46":4,"49":3,"52":1,"56":18,"57":5,"58":1,"63":2,"65":1,"66":1,"67":3,"68":1}}],["getting",{"0":{"2":1,"35":1,"47":1}}],["geo",{"0":{"0":1}}],["ctrl",{"0":{"64":1}}],["c5",{"0":{"57":1}}],["c1",{"0":{"57":1}}],["c=us",{"0":{"57":7}}],["cb",{"0":{"57":1}}],["cbi",{"0":{"7":2}}],["cwwouzl4+aalfwj2pr+otupnjhci8stdedvmy5jtxskdal+5pgnu7zjbkfhbodgt",{"0":{"57":1}}],["c2lau2viyxn0awfucy1nywncb29rlvbyby5sb2nhbcaou2viyxn0awfuifnhc3np",{"0":{"57":1}}],["ccs",{"0":{"56":4}}],["ccc",{"0":{"2":1}}],["cfg",{"0":{"57":1}}],["cf",{"0":{"35":4,"57":1}}],["cycle",{"0":{"32":1}}],["cycles",{"0":{"2":1}}],["c6599969b",{"0":{"27":1}}],["csp",{"0":{"61":1}}],["csr",{"0":{"57":2}}],["cssperfusion",{"0":{"56":1}}],["csv",{"0":{"25":4}}],["csi",{"0":{"2":3,"3":1,"45":2}}],["cn",{"0":{"57":1}}],["cn=digicert",{"0":{"57":4}}],["cn=datacoves",{"0":{"57":1}}],["cn=",{"0":{"57":2}}],["cn=thawte",{"0":{"57":6}}],["cni",{"0":{"24":5}}],["cname",{"0":{"2":1}}],["cer",{"0":{"35":4,"57":31}}],["certfile",{"0":{"57":2}}],["certs",{"0":{"57":4}}],["certain",{"0":{"15":1,"22":1,"34":1,"38":1,"61":3,"62":1}}],["cert",{"0":{"0":1,"2":2,"15":1,"35":1,"57":2}}],["certificates",{"0":{"0":3,"2":2,"15":1,"52":1,"57":5}}],["certificate",{"0":{"0":2,"2":4,"26":1,"35":4,"37":1,"57":32}}],["ceftificates",{"0":{"35":1}}],["celery",{"0":{"23":5}}],["center",{"0":{"22":1}}],["crl2pkcs7",{"0":{"57":2}}],["criteria",{"0":{"28":2}}],["crds",{"0":{"15":1}}],["credits",{"0":{"22":2}}],["credit",{"0":{"22":7}}],["credential",{"0":{"5":2}}],["credentials",{"0":{"1":1,"3":2,"4":1,"5":2,"8":2,"11":1,"12":4,"14":2,"15":1,"18":1,"20":1,"44":1}}],["creating",{"0":{"14":2,"22":1,"56":1}}],["creation",{"0":{"2":1,"32":2}}],["creates",{"0":{"15":2,"22":5}}],["createrole",{"0":{"2":1,"18":1}}],["createdb",{"0":{"2":1,"18":1}}],["created",{"0":{"2":4,"3":1,"5":1,"10":2,"17":3,"22":2,"39":1,"40":1,"45":1,"46":1,"47":1,"52":1}}],["create",{"0":{"0":1,"2":21,"3":1,"4":3,"5":2,"6":1,"8":4,"13":1,"14":7,"15":5,"18":3,"20":1,"22":3,"30":1,"32":9,"34":3,"35":1,"40":6,"42":3,"45":4,"46":2,"53":5,"54":1,"56":2,"59":2,"62":2,"65":1}}],["c319",{"0":{"12":2}}],["ci",{"0":{"7":1,"15":1,"28":1,"42":5,"58":3,"65":1}}],["cpu",{"0":{"43":2}}],["cp",{"0":{"4":1,"56":1}}],["cd7203755386",{"0":{"68":2}}],["cd",{"0":{"3":2,"6":2,"15":1,"17":2,"26":5,"37":1,"39":4,"56":2,"57":1}}],["c",{"0":{"2":1,"4":1,"15":1,"38":1,"49":1,"61":1,"64":1,"65":2}}],["cursor",{"0":{"63":1}}],["curl",{"0":{"26":2,"46":2,"55":2,"57":2}}],["currently",{"0":{"2":1,"3":1,"24":1,"44":1,"47":1,"56":1,"61":1}}],["current",{"0":{"2":1,"15":2,"25":1,"26":1,"32":2,"46":1,"47":1,"58":1,"61":1}}],["cuts",{"0":{"25":1}}],["cust",{"0":{"7":2}}],["custom",{"0":{"5":1,"27":8,"57":1,"61":1}}],["customers",{"0":{"22":12,"27":1,"47":2,"57":1}}],["customer",{"0":{"1":1,"22":19,"28":1,"39":1,"47":4,"56":6,"57":2}}],["clcerts",{"0":{"57":2}}],["cluser",{"0":{"45":1}}],["clusters",{"0":{"1":2,"2":3,"4":1,"7":2,"14":2,"17":1,"23":1,"26":1,"55":1,"56":3}}],["cluster",{"0":{"0":2,"2":35,"3":8,"6":9,"8":2,"9":3,"10":6,"12":3,"14":8,"15":15,"17":9,"20":6,"22":1,"23":2,"26":5,"33":3,"39":4,"41":1,"45":11,"47":1,"51":3,"52":1,"53":1,"54":2,"55":2,"56":9,"57":2,"59":2,"62":12,"68":5}}],["claims",{"0":{"49":1}}],["claim",{"0":{"45":2,"49":1,"52":2}}],["class",{"0":{"2":1,"9":1}}],["cleanly",{"0":{"61":1}}],["cleaning",{"0":{"61":1}}],["cleanup",{"0":{"23":1}}],["clear",{"0":{"23":1}}],["cliv2",{"0":{"14":1}}],["cli",{"0":{"6":2,"12":2,"13":1,"14":3,"15":5,"17":2,"20":1,"22":2,"23":1,"28":1,"30":4,"34":3,"39":5,"45":2,"47":4,"49":1,"51":2,"54":1,"56":1,"57":2,"59":4,"62":1,"63":5,"64":3}}],["clicking",{"0":{"22":2,"23":1,"40":1,"52":1}}],["click",{"0":{"4":1,"7":2,"12":1,"22":2,"32":7,"40":2,"52":2,"56":4,"66":1}}],["clients",{"0":{"62":1}}],["client",{"0":{"1":1,"2":3,"3":2,"12":4,"15":1,"16":2,"26":2,"52":7,"56":6,"57":1,"62":1}}],["close",{"0":{"46":1}}],["cloud",{"0":{"46":1,"48":1,"56":2}}],["cloudflare",{"0":{"35":1}}],["cloudx",{"0":{"2":7,"3":4,"17":1,"55":2}}],["clonning",{"0":{"4":1}}],["cloned",{"0":{"3":1,"37":1}}],["clone",{"0":{"3":3,"4":2,"6":2,"15":2,"17":2,"39":2,"56":1,"57":2}}],["chrome",{"0":{"68":2}}],["chicken",{"0":{"56":1}}],["chunks",{"0":{"63":1}}],["chunk",{"0":{"43":2}}],["cherrypick",{"0":{"34":1}}],["cherry",{"0":{"34":2}}],["checked",{"0":{"40":1,"47":1,"56":4,"57":1}}],["checker",{"0":{"25":1}}],["checkboxes",{"0":{"40":1}}],["checking",{"0":{"25":1,"61":2}}],["checks",{"0":{"5":1,"22":1}}],["checkout",{"0":{"3":1,"16":1,"34":1,"39":1,"56":1,"57":1}}],["check",{"0":{"1":2,"2":1,"8":3,"9":1,"11":2,"12":1,"15":2,"22":5,"23":1,"25":1,"32":2,"37":1,"39":3,"45":1,"50":1,"56":13,"57":5}}],["chown",{"0":{"56":1}}],["choosing",{"0":{"24":1,"52":1}}],["choose",{"0":{"2":2,"4":1,"31":1,"32":1,"40":2,"53":1,"56":1}}],["chosen",{"0":{"22":1}}],["chmod",{"0":{"4":1,"26":1}}],["channels",{"0":{"61":2}}],["channel",{"0":{"56":1,"61":1}}],["changing",{"0":{"34":1,"55":1}}],["changelog",{"0":{"47":1}}],["changed",{"0":{"18":1,"34":1,"55":1,"57":2,"59":1,"61":1,"68":1}}],["change",{"0":{"4":1,"6":1,"15":1,"27":1,"34":3,"39":2,"40":1,"42":1,"47":1,"59":1,"61":5,"67":1}}],["changes",{"0":{"2":1,"3":4,"6":1,"15":2,"16":3,"22":1,"28":6,"32":2,"39":3,"47":3,"56":1,"59":1,"61":6,"64":1}}],["chatgpt",{"0":{"25":1}}],["chat",{"0":{"25":1}}],["charliermarsh",{"0":{"25":1}}],["charts",{"0":{"11":1,"56":1,"61":1}}],["chart",{"0":{"11":10,"56":1,"61":2,"67":5}}],["characters",{"0":{"2":2}}],["chain",{"0":{"2":1,"57":1}}],["chapsbx",{"0":{"2":1}}],["chap",{"0":{"1":2,"2":3,"4":2}}],["cacerts",{"0":{"57":2}}],["cached",{"0":{"39":3}}],["ca",{"0":{"57":10}}],["callbacks=false",{"0":{"63":1}}],["callback",{"0":{"63":2}}],["calling",{"0":{"61":1}}],["called",{"0":{"61":1,"62":1}}],["call",{"0":{"57":1,"61":2,"63":1,"64":1}}],["calico",{"0":{"56":4}}],["calculator",{"0":{"24":3}}],["calculate",{"0":{"22":1}}],["capacity",{"0":{"45":1}}],["capped",{"0":{"24":1}}],["carry",{"0":{"61":1}}],["careful",{"0":{"57":1,"62":1}}],["carefully",{"0":{"34":1}}],["card",{"0":{"22":5}}],["cat",{"0":{"26":1,"41":1,"46":3,"57":2}}],["cautious",{"0":{"22":1}}],["caused",{"0":{"61":1}}],["cause",{"0":{"2":1,"56":1,"57":1,"68":1}}],["cases",{"0":{"22":1}}],["case",{"0":{"0":1,"11":1,"25":1,"56":1,"57":1,"61":3,"66":1}}],["cannot",{"0":{"57":1}}],["cant",{"0":{"25":1}}],["can",{"0":{"0":1,"2":4,"3":2,"5":1,"8":1,"9":1,"10":1,"12":3,"15":1,"20":1,"22":2,"23":4,"24":2,"27":1,"32":3,"34":5,"44":1,"47":3,"49":1,"54":1,"55":1,"56":16,"57":6,"59":2,"61":3,"62":1,"66":1}}],["coves",{"0":{"56":1,"58":2}}],["cover",{"0":{"34":1,"56":1,"57":1}}],["cooked",{"0":{"40":1}}],["copied",{"0":{"35":2,"68":1}}],["copy",{"0":{"4":1,"8":1,"22":1,"29":1,"34":1,"35":1,"40":1,"41":2,"56":4}}],["co",{"0":{"25":1}}],["cold",{"0":{"25":1}}],["column",{"0":{"25":1}}],["colors",{"0":{"25":1}}],["costs",{"0":{"22":1}}],["count",{"0":{"20":2,"43":1}}],["could",{"0":{"2":1,"3":1,"5":1,"18":1,"22":1,"28":1,"47":1,"52":1,"53":1,"61":2,"68":1}}],["cortex",{"0":{"56":2}}],["corner",{"0":{"40":1,"56":2}}],["correct",{"0":{"34":1,"44":1,"45":1,"56":3,"57":4}}],["correctly",{"0":{"5":1,"56":2}}],["corresponding",{"0":{"18":1,"40":1,"61":1}}],["coredns",{"0":{"27":9,"56":2}}],["core",{"0":{"3":3,"13":1,"15":8,"16":2,"21":1,"23":1,"27":1,"28":1,"33":1,"53":1,"54":2,"56":28,"57":2,"59":1,"61":3,"62":2,"64":3,"65":1}}],["codeserver",{"0":{"44":1}}],["codeservers",{"0":{"23":1}}],["code",{"0":{"5":1,"24":1,"25":17,"34":4,"38":2,"40":6,"44":2,"45":5,"56":5,"58":3,"61":6,"62":1,"64":1,"68":2}}],["concurrently",{"0":{"61":2}}],["concurrency",{"0":{"61":2}}],["conceptualy",{"0":{"61":1}}],["concepts",{"0":{"21":1,"61":1}}],["convert",{"0":{"57":1}}],["converting",{"0":{"57":1}}],["convenient",{"0":{"41":1}}],["conventions",{"0":{"61":1}}],["convention",{"0":{"2":3,"22":1}}],["constructs",{"0":{"61":1}}],["constraints",{"0":{"24":1}}],["consent",{"0":{"52":1}}],["console",{"0":{"30":1,"53":1,"56":1}}],["consumer",{"0":{"7":1}}],["consideration",{"0":{"2":1}}],["condition=",{"0":{"43":3}}],["condition",{"0":{"2":1,"43":3}}],["connected",{"0":{"22":2}}],["connectivity",{"0":{"12":1}}],["connection",{"0":{"2":2,"12":2,"16":1,"18":1,"22":2,"42":3,"46":1}}],["connect",{"0":{"2":4,"5":2,"12":2,"18":2,"26":2,"57":1,"63":1}}],["continuing",{"0":{"47":1}}],["continuous",{"0":{"42":1}}],["continue",{"0":{"40":1}}],["control",{"0":{"15":1,"41":1,"47":1,"56":1,"61":1,"68":1}}],["controllers",{"0":{"61":2}}],["controller",{"0":{"2":1,"15":2,"61":5}}],["content",{"0":{"54":1,"69":1}}],["contenttype",{"0":{"13":1}}],["contents",{"0":{"4":1,"61":2}}],["context=itx",{"0":{"17":1}}],["context=fill",{"0":{"6":1}}],["context",{"0":{"4":1,"6":3,"8":3,"12":6,"15":2,"17":2,"20":1,"25":1,"26":3,"51":2,"56":3,"57":3,"61":1,"64":1}}],["contexts",{"0":{"4":1,"8":1,"12":1,"26":1,"56":2,"57":1}}],["contains",{"0":{"10":1,"12":1,"17":1,"29":1,"39":1}}],["contain",{"0":{"3":1,"57":2}}],["container=",{"0":{"29":2}}],["containerservice",{"0":{"56":1}}],["containers",{"0":{"27":1,"29":1,"32":1,"43":1,"56":1}}],["containers=",{"0":{"21":1}}],["container",{"0":{"2":1,"3":1,"29":2,"32":4,"33":2,"43":3,"65":1}}],["containing",{"0":{"2":1,"56":1}}],["conflunce",{"0":{"8":1}}],["confluence",{"0":{"1":1,"2":1,"15":1,"42":1}}],["confirm",{"0":{"34":1}}],["confirmed",{"0":{"2":1}}],["configmaps",{"0":{"61":1}}],["configmap",{"0":{"27":2,"61":2}}],["config2",{"0":{"26":1}}],["configs",{"0":{"2":1,"3":1,"56":1}}],["config",{"0":{"2":2,"3":5,"4":4,"6":3,"8":2,"10":2,"12":4,"14":1,"15":6,"17":4,"26":3,"27":1,"39":4,"41":1,"45":4,"47":1,"56":8,"57":7,"62":2,"66":2}}],["configuring",{"0":{"3":1,"16":1,"56":1}}],["configure",{"0":{"2":2,"3":1,"4":3,"5":4,"8":1,"14":3,"15":2,"18":1,"20":1,"22":1,"26":1,"32":2,"33":3,"35":1,"40":1,"42":1,"52":4,"53":1}}],["configured",{"0":{"0":1,"2":1,"5":1,"12":1,"15":1,"33":1,"40":1,"53":1}}],["configurations",{"0":{"1":2,"62":1}}],["configuration",{"0":{"0":3,"2":6,"3":11,"5":2,"9":2,"10":4,"15":8,"16":1,"17":3,"32":4,"33":3,"39":4,"42":1,"47":1,"52":1,"56":4,"57":1,"61":3,"62":7}}],["comes",{"0":{"56":1}}],["come",{"0":{"47":1,"49":1,"56":1}}],["combined",{"0":{"39":1,"47":3}}],["comx",{"0":{"7":3}}],["commentary",{"0":{"56":1}}],["comments",{"0":{"3":2}}],["common",{"0":{"40":1,"56":1,"62":1}}],["commited",{"0":{"39":1}}],["committed",{"0":{"34":1}}],["commits",{"0":{"34":1}}],["commiting",{"0":{"15":1}}],["commit",{"0":{"3":2,"6":1,"16":2,"28":1,"34":1,"39":5,"42":1,"57":1}}],["commands",{"0":{"20":1,"34":1,"56":2,"57":3,"62":1,"63":2,"64":2,"65":1}}],["command",{"0":{"2":1,"4":1,"6":1,"18":1,"26":1,"28":1,"30":5,"34":2,"55":1,"56":4,"57":2,"59":1,"63":3,"65":1}}],["component",{"0":{"67":1}}],["components",{"0":{"4":1,"14":1,"15":1,"62":2}}],["compose",{"0":{"61":1,"65":4}}],["complex",{"0":{"56":1}}],["complexity",{"0":{"56":1}}],["completed",{"0":{"64":1}}],["completes",{"0":{"22":1}}],["completely",{"0":{"22":1}}],["complete",{"0":{"2":1,"22":1,"32":4,"48":1,"52":1,"57":3}}],["compllicated",{"0":{"56":1}}],["complain",{"0":{"56":1}}],["comparison",{"0":{"56":1}}],["compare",{"0":{"34":1,"56":2,"57":1}}],["compatible",{"0":{"28":4}}],["compile",{"0":{"30":1}}],["comprehensive",{"0":{"22":1}}],["compute",{"0":{"45":1,"61":1}}],["computed",{"0":{"15":1}}],["compute+storage",{"0":{"0":1}}],["com",{"0":{"0":9,"1":4,"2":12,"3":2,"4":3,"6":2,"7":4,"8":1,"9":3,"11":1,"12":17,"14":6,"15":9,"17":2,"18":2,"20":6,"22":2,"24":1,"25":13,"26":4,"27":4,"30":1,"33":1,"35":1,"39":2,"41":1,"45":1,"46":3,"47":1,"50":1,"51":1,"56":12,"57":24,"59":3,"62":2,"65":1,"66":2,"68":2}}],["pkcs12",{"0":{"57":6}}],["pkcs7",{"0":{"57":2}}],["pfx",{"0":{"57":12}}],["pzeembwga1uechmvbwtjzxj0igrldmvsb3btzw50ienbmt4wpaydvqqlddvzc2fz",{"0":{"57":1}}],["physical",{"0":{"56":1}}],["pdbs",{"0":{"56":7}}],["pdb",{"0":{"56":29}}],["pc",{"0":{"48":1,"56":1}}],["pce",{"0":{"2":1,"7":3}}],["pvc",{"0":{"45":25,"49":4}}],["pv",{"0":{"45":18,"49":2}}],["p",{"0":{"39":1,"45":1,"56":18}}],["ps",{"0":{"38":1,"65":1}}],["psql",{"0":{"18":1}}],["p=",{"0":{"27":1}}],["pg",{"0":{"18":2}}],["png",{"0":{"13":1}}],["pypi",{"0":{"58":1}}],["pypa",{"0":{"37":1}}],["pygraphviz",{"0":{"13":1}}],["py",{"0":{"6":2,"13":2,"15":7,"17":2,"22":2,"25":1,"28":1,"34":3,"37":2,"39":5,"42":1,"47":4,"49":2,"51":2,"54":2,"56":2,"57":2,"59":4,"62":1,"63":28}}],["python3",{"0":{"15":1,"37":1,"46":2,"56":3,"63":28}}],["python",{"0":{"2":1,"6":1,"12":1,"17":1,"25":4,"28":1,"30":4,"37":1,"38":2,"40":3,"46":2,"56":3,"62":1,"64":2}}],["plus",{"0":{"54":1}}],["plugins",{"0":{"5":2}}],["place",{"0":{"56":1}}],["placed",{"0":{"42":1}}],["places",{"0":{"3":1,"25":1}}],["plane",{"0":{"56":1,"68":1}}],["plane=controller",{"0":{"15":1}}],["plan",{"0":{"22":5,"56":2}}],["platform",{"0":{"7":1}}],["please",{"0":{"2":9,"3":1,"7":1,"22":2,"57":1}}],["pem",{"0":{"57":5}}],["pending",{"0":{"3":1,"61":3}}],["people",{"0":{"3":1,"57":1}}],["persistentvolumeclaim",{"0":{"45":1}}],["persistentvolume",{"0":{"45":1}}],["persistentvolumereclaimpolicy",{"0":{"45":2}}],["persistent",{"0":{"45":11,"49":2}}],["personal",{"0":{"1":2}}],["period",{"0":{"22":3,"61":1}}],["permission",{"0":{"9":1,"32":1,"56":1}}],["permissions",{"0":{"4":2,"9":3,"15":1,"20":1,"22":2,"31":2,"52":8}}],["perform",{"0":{"4":1,"29":4,"56":1}}],["performance",{"0":{"0":1,"28":1,"46":2}}],["per",{"0":{"2":2,"7":1,"21":1,"22":1,"24":3,"47":1,"62":1}}],["pidpressure",{"0":{"43":1}}],["pii",{"0":{"42":2}}],["pickle",{"0":{"63":2}}],["pick",{"0":{"34":3,"44":2,"56":6}}],["pilot",{"0":{"25":1}}],["pip",{"0":{"37":4,"46":1,"50":3,"65":1}}],["pipx",{"0":{"12":1}}],["pip3",{"0":{"6":1,"13":1,"17":1,"56":1}}],["pipeline",{"0":{"5":1,"55":1}}],["pipelines",{"0":{"2":2,"3":1,"17":1,"55":1,"65":1}}],["pingid",{"0":{"2":1}}],["ping",{"0":{"2":3,"3":3,"16":2}}],["put",{"0":{"44":1,"56":1}}],["putobject",{"0":{"2":2,"9":1,"32":1,"53":1}}],["pushing",{"0":{"59":1}}],["pushed",{"0":{"28":2,"59":2}}],["push",{"0":{"3":2,"11":1,"15":1,"16":1,"34":2,"39":3,"54":1,"57":3,"59":2}}],["public",{"0":{"2":1,"12":1}}],["pulling",{"0":{"44":1}}],["pull",{"0":{"2":1,"3":1,"6":1,"8":1,"15":2,"16":2,"39":2,"56":1,"58":1}}],["purpose",{"0":{"0":1,"47":2,"56":2,"57":2}}],["panic",{"0":{"61":1}}],["panel",{"0":{"22":3,"44":1,"56":4}}],["package",{"0":{"46":1,"50":1,"61":1}}],["packages",{"0":{"46":2,"56":1,"63":28}}],["paste",{"0":{"41":2,"56":2}}],["past",{"0":{"22":1,"56":1}}],["pass",{"0":{"3":1,"16":1,"61":1}}],["password=",{"0":{"8":1,"14":1}}],["passwords",{"0":{"2":1,"57":1}}],["password",{"0":{"0":1,"1":1,"2":7,"3":1,"4":1,"8":2,"9":2,"11":2,"12":1,"14":1,"16":1,"18":2,"33":1,"57":7,"66":1}}],["pay",{"0":{"22":1}}],["payment",{"0":{"22":2}}],["payload",{"0":{"22":1,"54":1}}],["parser",{"0":{"63":1}}],["parameters",{"0":{"29":1}}],["params",{"0":{"3":3,"6":2,"33":2,"56":1,"62":5}}],["particularly",{"0":{"57":1}}],["partial",{"0":{"13":1}}],["parts",{"0":{"57":1,"61":1}}],["party",{"0":{"7":1}}],["part",{"0":{"2":1,"25":1,"47":1}}],["patch",{"0":{"27":2,"28":6,"45":1,"56":20,"67":3}}],["paths",{"0":{"57":1}}],["path",{"0":{"3":1,"27":2,"45":2,"59":1,"68":1}}],["pattern",{"0":{"2":1,"27":1}}],["page",{"0":{"2":3,"5":1,"15":1,"22":2,"40":5,"52":2}}],["pages",{"0":{"1":1}}],["port",{"0":{"68":1}}],["portal",{"0":{"12":1,"52":1,"56":1}}],["potential",{"0":{"61":1}}],["pomerium",{"0":{"57":2,"68":6}}],["populated",{"0":{"40":1}}],["popup",{"0":{"7":1}}],["power",{"0":{"25":2}}],["possibility",{"0":{"64":1}}],["possible",{"0":{"22":1,"61":1}}],["post",{"0":{"5":2,"46":1,"54":1,"56":1}}],["postgres",{"0":{"2":6,"3":2,"9":2,"18":7}}],["postgresql",{"0":{"0":1,"49":2}}],["policies",{"0":{"14":1}}],["policy",{"0":{"2":7,"45":4,"53":1}}],["pointers",{"0":{"61":1}}],["points",{"0":{"24":1,"61":1}}],["point",{"0":{"22":1,"34":1,"39":1,"56":1}}],["pointtositeconfiguration",{"0":{"12":1}}],["pointing",{"0":{"2":1,"4":1}}],["pod=",{"0":{"29":1}}],["pod=~",{"0":{"29":1,"43":1}}],["pods=$",{"0":{"38":1}}],["pods",{"0":{"8":1,"15":4,"20":1,"24":6,"38":2,"43":3,"49":2,"56":4}}],["pod",{"0":{"8":1,"13":1,"15":3,"21":2,"23":2,"24":4,"29":4,"38":1,"43":7,"49":1,"54":1,"56":4,"63":1}}],["pools",{"0":{"0":1,"56":1}}],["p10",{"0":{"0":1}}],["practice",{"0":{"34":1}}],["prd001",{"0":{"29":3,"43":1}}],["prd",{"0":{"4":3,"9":2}}],["primitive",{"0":{"61":1}}],["prices",{"0":{"22":1}}],["pricing",{"0":{"22":1}}],["priority",{"0":{"34":1}}],["prior",{"0":{"15":1,"17":1,"56":2}}],["privileges",{"0":{"14":1,"46":1}}],["privilege",{"0":{"4":1}}],["private",{"0":{"2":1,"8":1,"35":2,"41":2,"42":2,"57":4}}],["principal",{"0":{"2":1}}],["print",{"0":{"2":1,"14":2,"30":1,"38":1,"56":1,"57":2}}],["pr",{"0":{"2":1,"3":1,"42":1,"59":1}}],["precise",{"0":{"61":1}}],["preconfigured",{"0":{"40":1}}],["prevent",{"0":{"56":2}}],["previous",{"0":{"46":1,"56":1}}],["preview",{"0":{"25":1}}],["pressure",{"0":{"43":1}}],["present",{"0":{"34":2}}],["prefixing",{"0":{"59":1}}],["prefix",{"0":{"24":1,"32":3,"43":1}}],["prefer",{"0":{"56":1}}],["preferring",{"0":{"22":1}}],["preferences",{"0":{"26":1}}],["preference",{"0":{"0":1}}],["prepare",{"0":{"40":1,"56":1,"57":1}}],["prepay",{"0":{"22":1}}],["prepaid",{"0":{"22":1}}],["prerequisites",{"0":{"6":1,"15":2,"17":1}}],["pre",{"0":{"5":1,"42":1,"59":4}}],["premium",{"0":{"0":1}}],["proto",{"0":{"68":1}}],["protocol",{"0":{"11":1,"63":1}}],["problems",{"0":{"57":1,"61":1}}],["problem",{"0":{"56":1,"68":1}}],["probably",{"0":{"2":1,"12":1,"56":1,"57":1}}],["procedure",{"0":{"56":1}}],["procedures",{"0":{"56":1}}],["processes",{"0":{"38":1,"65":1}}],["process",{"0":{"8":1,"22":3,"34":1,"40":1,"47":1,"56":3,"57":1,"61":3,"63":1,"67":1}}],["proc",{"0":{"46":3}}],["prometheus",{"0":{"43":1,"54":1,"56":2}}],["produced",{"0":{"47":1}}],["product",{"0":{"22":2}}],["products",{"0":{"22":7}}],["production",{"0":{"0":1,"1":1,"2":4,"4":1,"30":1,"42":1,"59":1}}],["prod",{"0":{"42":2}}],["properly",{"0":{"39":1}}],["properties",{"0":{"32":1}}],["program",{"0":{"61":1}}],["programmatic",{"0":{"30":1}}],["programatically",{"0":{"30":1}}],["proxy",{"0":{"14":4}}],["pro001",{"0":{"2":1}}],["profiles",{"0":{"12":1,"40":3,"42":1}}],["profile",{"0":{"2":1,"12":2,"40":12,"56":15}}],["project",{"0":{"2":2,"3":1,"5":6,"15":13,"23":1,"42":1,"62":1,"64":1}}],["projects",{"0":{"1":1,"2":8,"3":1,"15":2,"23":5,"56":1}}],["providing",{"0":{"52":1}}],["provides",{"0":{"22":1}}],["providers",{"0":{"12":1,"32":1,"33":1,"45":1}}],["provider",{"0":{"2":1,"26":1,"33":5,"56":1}}],["provided",{"0":{"0":1,"1":2,"47":1,"59":1}}],["provisioner",{"0":{"3":2,"18":1}}],["provisioned",{"0":{"2":1}}],["tls",{"0":{"57":6}}],["tld",{"0":{"0":1}}],["tmux",{"0":{"46":2}}],["tsv",{"0":{"45":1}}],["turning",{"0":{"51":2}}],["turn",{"0":{"23":1,"49":2}}],["tutorial",{"0":{"14":1}}],["traceback",{"0":{"63":1}}],["track",{"0":{"61":1}}],["transport",{"0":{"46":1}}],["transition",{"0":{"22":1}}],["transfers",{"0":{"22":1}}],["transfer",{"0":{"22":2,"41":1}}],["transform",{"0":{"12":2,"56":1}}],["treat",{"0":{"61":1}}],["tree",{"0":{"34":1,"56":3,"57":1}}],["try",{"0":{"56":2,"57":2,"59":1}}],["triggerer",{"0":{"63":1}}],["triggered",{"0":{"40":1}}],["trigger",{"0":{"55":1}}],["triggering",{"0":{"28":1}}],["trial",{"0":{"22":11}}],["true",{"0":{"8":1,"18":3,"22":2,"43":3}}],["trusted",{"0":{"2":1}}],["tgz",{"0":{"11":2}}],["two",{"0":{"8":1,"15":1,"32":3,"56":2,"57":2,"61":1}}],["txt",{"0":{"6":1,"17":1,"37":1,"50":1,"56":1,"58":1}}],["t",{"0":{"2":1,"3":1,"5":1,"22":2,"23":1,"34":4,"40":1,"44":2,"56":4,"57":4,"61":3}}],["targetresourcegroup",{"0":{"45":1}}],["targeted",{"0":{"39":1,"58":1}}],["targetted",{"0":{"34":1}}],["tar",{"0":{"26":4}}],["taxes",{"0":{"22":1}}],["tally",{"0":{"22":2,"23":2}}],["tag",{"0":{"21":1,"34":6,"56":3}}],["tags",{"0":{"14":1,"28":2,"34":3,"56":1}}],["tasks",{"0":{"23":15}}],["task",{"0":{"21":1,"23":3,"29":2,"61":1,"63":5}}],["tap",{"0":{"14":3}}],["tail",{"0":{"8":1}}],["table",{"0":{"49":1}}],["tab",{"0":{"2":1,"5":3,"32":3}}],["taqy",{"0":{"2":2,"5":1,"7":5,"8":5,"65":1,"66":2}}],["take",{"0":{"2":1,"34":1,"40":1,"49":1,"56":5,"57":1,"61":1}}],["typha",{"0":{"56":2}}],["typically",{"0":{"2":1,"5":1,"22":1,"40":1,"57":1,"68":1}}],["type=",{"0":{"27":1}}],["types",{"0":{"22":2,"61":2}}],["type",{"0":{"0":2,"2":2,"4":1,"7":2,"9":1,"22":2,"24":5,"54":1}}],["tee",{"0":{"46":2}}],["text",{"0":{"35":1,"40":1,"57":3}}],["tenant",{"0":{"26":1,"52":5,"56":2}}],["temporary",{"0":{"57":1}}],["temporarily",{"0":{"56":1}}],["temporal",{"0":{"18":2}}],["template",{"0":{"4":2,"7":1,"27":2,"42":1,"56":4}}],["templates",{"0":{"2":2}}],["termination",{"0":{"61":1}}],["terminated",{"0":{"43":1}}],["terminal",{"0":{"4":2,"56":1,"65":1}}],["terraform",{"0":{"2":3,"17":1}}],["team",{"0":{"2":6,"7":2,"51":1,"53":1,"62":1}}],["tests",{"0":{"67":1}}],["test",{"0":{"1":1,"2":4,"6":1,"17":1,"22":6,"27":1,"54":3,"59":1}}],["tis",{"0":{"63":4}}],["ti",{"0":{"27":1,"38":1,"65":1}}],["timkmecl",{"0":{"25":1}}],["timeout",{"0":{"68":2}}],["timestamp",{"0":{"28":1}}],["times",{"0":{"23":1,"29":1}}],["time",{"0":{"2":1,"6":1,"25":1,"28":1,"29":1,"44":1,"54":3,"56":2,"61":1,"68":2}}],["ticket",{"0":{"2":1,"59":1}}],["title=how+to+request+access+to+bitbucket+",{"0":{"1":1}}],["tier",{"0":{"0":1}}],["thusly",{"0":{"47":1,"56":2,"57":4}}],["than",{"0":{"24":1,"34":1,"61":1,"63":1}}],["that",{"0":{"0":1,"2":8,"3":2,"4":2,"5":2,"6":1,"8":1,"10":1,"12":2,"14":1,"15":3,"16":1,"18":3,"21":1,"22":4,"24":1,"25":2,"28":6,"29":1,"31":1,"34":8,"39":2,"40":3,"41":3,"42":3,"44":2,"45":3,"46":1,"47":4,"55":1,"56":19,"57":9,"58":1,"59":1,"61":14,"62":1,"64":1,"65":1,"66":1,"67":1,"68":1}}],["thinking",{"0":{"41":1}}],["think",{"0":{"25":1,"57":1,"61":1}}],["thing",{"0":{"23":1,"62":1}}],["things",{"0":{"12":1,"25":1,"49":1,"56":1,"62":1}}],["third",{"0":{"7":1}}],["this",{"0":{"2":18,"3":3,"4":1,"8":1,"10":1,"11":1,"12":2,"15":5,"17":3,"18":1,"20":1,"22":2,"25":12,"27":2,"34":3,"44":1,"45":3,"47":3,"48":1,"49":2,"52":5,"56":24,"57":15,"58":1,"59":2,"61":10,"62":2,"67":2,"68":3}}],["those",{"0":{"5":1,"8":1,"16":1,"22":1,"47":1,"56":1,"57":2,"62":1,"68":2}}],["three",{"0":{"22":1,"29":1,"33":1,"49":1}}],["threshold",{"0":{"2":1,"9":1}}],["through",{"0":{"2":1,"15":1,"17":1,"22":2,"34":1,"56":1,"57":1}}],["them",{"0":{"15":3,"22":4,"28":1,"40":1,"47":1,"49":1,"50":1,"56":3,"57":2,"59":1,"61":5}}],["their",{"0":{"8":1,"22":1,"44":1,"47":1,"61":4,"62":1,"68":1}}],["then",{"0":{"6":1,"10":1,"20":1,"22":3,"27":2,"32":2,"34":2,"35":3,"37":1,"39":1,"47":3,"52":2,"56":5,"57":4,"63":1,"66":1}}],["they",{"0":{"2":1,"3":1,"10":2,"17":2,"22":3,"28":2,"40":1,"49":1,"52":1,"56":2,"57":2,"61":2}}],["these",{"0":{"2":2,"6":1,"7":1,"17":2,"22":3,"25":1,"44":1,"49":4,"56":1,"57":6,"61":2,"68":1}}],["therefore",{"0":{"29":1,"45":1}}],["there",{"0":{"1":1,"2":2,"3":1,"7":1,"15":1,"22":1,"23":1,"29":1,"30":1,"33":1,"34":1,"39":2,"47":1,"56":5,"57":3,"61":2}}],["the",{"0":{"0":4,"1":1,"2":62,"3":25,"4":14,"5":7,"6":12,"7":12,"8":6,"9":1,"10":6,"11":9,"12":13,"14":7,"15":41,"16":5,"17":14,"18":7,"20":4,"22":78,"23":10,"24":6,"25":5,"27":7,"28":7,"29":14,"30":6,"31":1,"32":8,"33":5,"34":16,"35":3,"37":1,"39":17,"40":29,"41":6,"44":5,"45":40,"46":11,"47":21,"49":8,"51":1,"52":10,"53":7,"54":2,"55":4,"56":124,"57":83,"58":2,"59":7,"61":60,"62":6,"63":1,"64":3,"65":1,"66":2,"67":9,"68":5}}],["touching",{"0":{"61":1}}],["too",{"0":{"61":1}}],["tool",{"0":{"7":1,"42":2}}],["tools",{"0":{"5":1,"7":1,"56":2,"58":1}}],["tos",{"0":{"56":1,"62":1}}],["today",{"0":{"51":1}}],["todo",{"0":{"3":4,"16":1,"57":1}}],["topic",{"0":{"34":1}}],["top",{"0":{"23":1,"40":1}}],["total",{"0":{"22":1,"23":1,"43":3}}],["tokens",{"0":{"23":1,"52":1}}],["tokenproxy",{"0":{"13":1}}],["token",{"0":{"2":1,"14":2,"26":1,"35":1,"52":3}}],["to",{"0":{"0":4,"1":4,"2":51,"3":16,"4":15,"5":9,"6":6,"7":15,"8":6,"10":2,"11":2,"12":7,"13":1,"14":4,"15":23,"16":3,"17":2,"18":7,"20":8,"22":33,"23":4,"25":4,"26":4,"27":5,"28":7,"29":13,"30":3,"31":3,"32":8,"33":5,"34":24,"35":3,"39":10,"40":15,"41":6,"42":4,"44":3,"45":12,"46":1,"47":12,"48":3,"49":4,"50":1,"51":1,"52":10,"53":1,"54":1,"55":2,"56":69,"57":22,"58":1,"59":8,"61":32,"62":8,"63":3,"64":5,"65":2,"66":3,"67":7,"68":2}}],["src",{"0":{"58":1,"59":1,"61":1,"62":2}}],["skip",{"0":{"57":1,"67":1}}],["snf1w==",{"0":{"57":1}}],["snowflake",{"0":{"2":1,"7":2,"25":4,"42":4,"65":4}}],["switch",{"0":{"56":1}}],["sftp",{"0":{"27":4}}],["sleep",{"0":{"65":1}}],["sleistner",{"0":{"25":1}}],["slightly",{"0":{"57":1}}],["slack",{"0":{"23":1,"56":1}}],["slug",{"0":{"2":2,"22":2,"29":4,"49":1}}],["sqltypes",{"0":{"63":1}}],["sqltools",{"0":{"25":1}}],["sqlalchemy",{"0":{"63":9}}],["sqlfluff",{"0":{"25":2}}],["sql",{"0":{"18":2,"25":1,"42":1,"63":1}}],["symbol",{"0":{"57":1}}],["symbolic",{"0":{"46":1}}],["synchronization",{"0":{"29":1}}],["sync",{"0":{"8":2,"9":2,"22":2,"23":2,"29":13,"49":3,"61":1}}],["sys",{"0":{"46":3,"63":1}}],["sysadmin",{"0":{"7":3}}],["systems",{"0":{"15":1}}],["system32",{"0":{"4":1}}],["system",{"0":{"2":2,"5":2,"8":2,"14":4,"15":4,"21":1,"22":2,"27":3,"46":2,"56":11,"64":1}}],["scaffolded",{"0":{"61":1}}],["scope",{"0":{"56":2}}],["screen",{"0":{"56":1}}],["scritps",{"0":{"46":1}}],["scripts",{"0":{"6":2,"10":2,"12":1,"15":1,"17":2,"39":2,"54":1,"56":1,"62":2}}],["script",{"0":{"4":1,"15":1,"58":1}}],["scroll",{"0":{"22":1}}],["schedulable",{"0":{"63":1}}],["scheduling",{"0":{"63":4}}],["schedule",{"0":{"63":2}}],["schedules",{"0":{"61":1}}],["scheduler",{"0":{"29":2,"56":2,"61":1,"63":12}}],["scheduled",{"0":{"2":1}}],["schema",{"0":{"18":1}}],["scm",{"0":{"2":7,"3":1,"4":1,"5":2,"6":1,"15":1,"17":1}}],["s3",{"0":{"2":30,"9":13,"25":13,"29":6,"32":9,"33":2,"53":15}}],["shutting",{"0":{"56":2}}],["shellenv",{"0":{"56":2}}],["shell",{"0":{"54":1,"56":1}}],["sh",{"0":{"13":1,"24":3,"35":4,"46":1,"49":1,"54":1}}],["sha",{"0":{"28":1}}],["sharing",{"0":{"23":1}}],["share",{"0":{"2":5,"53":1}}],["sharepoint",{"0":{"1":1}}],["sha256withrsaencryption",{"0":{"57":4}}],["sha2",{"0":{"4":2}}],["shown",{"0":{"23":1}}],["show",{"0":{"11":1,"25":1,"34":1,"45":1,"49":2,"56":1,"57":1,"65":1}}],["should",{"0":{"2":1,"3":2,"4":1,"15":2,"16":1,"22":2,"28":1,"30":1,"34":1,"41":1,"45":1,"49":2,"56":5,"57":16,"61":5}}],["shortcut",{"0":{"4":2}}],["short",{"0":{"2":1,"25":1,"61":1}}],["safari",{"0":{"68":2}}],["safe",{"0":{"52":1,"57":1}}],["say",{"0":{"34":1,"57":1}}],["save",{"0":{"22":1,"31":1,"32":2,"40":1,"44":1,"49":1,"56":2,"68":2}}],["sales",{"0":{"22":1,"29":1}}],["sa",{"0":{"5":1,"7":1,"42":2}}],["sample",{"0":{"42":1}}],["samuelcolvin",{"0":{"25":1}}],["saml",{"0":{"2":1}}],["same",{"0":{"0":1,"1":1,"2":4,"3":1,"22":1,"28":1,"40":1,"45":1,"47":1,"56":4}}],["sandbox",{"0":{"2":1}}],["svc",{"0":{"2":1,"6":1,"18":1,"42":6,"46":1,"54":1}}],["side",{"0":{"64":1}}],["site",{"0":{"63":28}}],["simplify",{"0":{"61":1}}],["simply",{"0":{"56":1,"61":1}}],["simpler",{"0":{"65":1}}],["simple",{"0":{"44":2,"56":1}}],["similar",{"0":{"2":1,"56":2}}],["silent",{"0":{"30":1}}],["sigkilled",{"0":{"61":1}}],["sigterm",{"0":{"61":1}}],["sigs",{"0":{"14":1,"26":1,"46":1}}],["signal",{"0":{"61":1}}],["signaled",{"0":{"61":1}}],["signature",{"0":{"57":4}}],["signatures",{"0":{"2":1}}],["signing",{"0":{"46":1}}],["signup",{"0":{"22":1}}],["sign",{"0":{"2":1}}],["singleton",{"0":{"61":1}}],["single",{"0":{"2":2,"41":1,"52":1,"57":6,"61":1}}],["since",{"0":{"2":1,"25":1}}],["size",{"0":{"0":5,"9":3,"20":1,"43":1,"54":1,"68":1}}],["solution",{"0":{"63":1,"64":1,"68":1}}],["solve",{"0":{"62":1}}],["sort",{"0":{"23":1}}],["soon",{"0":{"22":1,"57":1}}],["source",{"0":{"24":2,"47":1,"56":1,"61":1,"62":1}}],["sources",{"0":{"5":1,"39":1,"46":2}}],["sourcecode",{"0":{"1":1,"2":8,"3":2,"4":1,"6":1,"15":2,"17":1,"39":1}}],["software",{"0":{"4":1,"46":1}}],["something",{"0":{"62":1}}],["sometimes",{"0":{"22":1,"30":1,"44":1,"56":1,"59":1,"65":1,"66":1,"67":1}}],["somewhat",{"0":{"61":1}}],["someone",{"0":{"15":1}}],["some",{"0":{"2":4,"24":1,"27":1,"29":1,"52":1,"54":1,"56":2,"57":1,"61":1,"67":1}}],["so",{"0":{"2":7,"3":3,"5":1,"7":1,"20":1,"23":1,"44":1,"52":1,"56":7,"57":1,"59":2,"61":3,"67":1}}],["s",{"0":{"2":6,"3":3,"6":1,"7":1,"8":1,"12":2,"15":3,"16":1,"20":1,"21":1,"22":4,"23":3,"26":1,"28":1,"34":1,"35":1,"38":1,"39":3,"40":1,"46":3,"47":1,"53":1,"56":18,"57":6,"61":8,"62":1,"64":2}}],["serialized",{"0":{"63":1}}],["serially",{"0":{"61":1}}],["serial",{"0":{"57":4}}],["series",{"0":{"47":2}}],["service=datacoves",{"0":{"14":1}}],["serviceaccount",{"0":{"2":1}}],["service",{"0":{"1":1,"2":9,"3":2,"5":2,"7":1,"8":6,"14":1,"18":1,"22":1,"26":1,"42":2,"45":1,"46":1,"59":1,"68":1}}],["services",{"0":{"0":1,"2":4,"3":2,"14":2,"15":2,"18":1,"22":5,"32":1,"61":1,"65":1}}],["servers",{"0":{"27":1,"28":1,"44":1}}],["server=",{"0":{"14":1}}],["server=jnj",{"0":{"8":1}}],["server",{"0":{"0":1,"4":1,"5":2,"11":5,"14":3,"15":3,"18":1,"22":1,"24":1,"26":1,"27":2,"35":3,"38":2,"40":6,"41":1,"44":1,"45":5,"56":8,"57":3,"58":3,"61":1}}],["self",{"0":{"46":1,"63":12}}],["selecting",{"0":{"52":1}}],["selects",{"0":{"22":1}}],["selected",{"0":{"5":1,"52":1}}],["select",{"0":{"5":1,"8":1,"22":2,"29":3,"32":8,"34":2,"52":2,"56":1,"59":2}}],["several",{"0":{"28":1,"57":2}}],["semantic",{"0":{"28":1}}],["seat",{"0":{"22":1}}],["seats",{"0":{"22":2}}],["search",{"0":{"7":2,"23":1,"29":1,"31":1}}],["sessions",{"0":{"68":1}}],["session=session",{"0":{"63":2}}],["session",{"0":{"13":1,"46":1,"63":6,"68":2}}],["sets",{"0":{"22":3,"40":2,"56":3}}],["setting",{"0":{"6":1,"22":1,"56":1,"57":1,"61":1}}],["settings",{"0":{"5":2,"44":1}}],["set",{"0":{"5":1,"6":1,"11":1,"12":2,"14":1,"17":1,"18":2,"20":1,"22":6,"26":1,"32":1,"39":1,"40":9,"42":1,"51":3,"52":1,"53":1,"56":16,"57":1,"59":2,"61":4,"67":2}}],["setupwithmanager",{"0":{"61":1}}],["setup",{"0":{"2":1,"3":2,"6":2,"12":1,"15":7,"17":1,"22":8,"56":2,"57":2,"62":1}}],["sentry",{"0":{"56":3}}],["sensitive",{"0":{"3":1,"16":1,"57":1}}],["sends",{"0":{"61":1}}],["send",{"0":{"2":1,"23":1,"61":1}}],["separate",{"0":{"3":1,"10":1,"17":1,"61":1,"62":1}}],["secert",{"0":{"57":1}}],["secure",{"0":{"42":1}}],["security",{"0":{"2":4,"50":1,"52":1}}],["seconds",{"0":{"23":2,"43":1}}],["section",{"0":{"12":1,"56":2,"57":4}}],["sections",{"0":{"2":1}}],["secret=",{"0":{"8":1}}],["secret",{"0":{"2":1,"3":7,"6":1,"8":9,"14":6,"15":6,"16":6,"17":1,"33":4,"39":2,"41":5,"52":4,"53":1,"57":16,"61":3,"62":4}}],["secrets",{"0":{"2":2,"3":3,"4":1,"5":1,"6":1,"8":4,"15":2,"16":4,"17":1,"39":3,"49":1,"52":1,"57":9,"61":1,"62":4}}],["seen",{"0":{"61":1}}],["seems",{"0":{"57":1}}],["seem",{"0":{"24":1}}],["see",{"0":{"1":1,"3":2,"10":3,"11":1,"15":1,"21":1,"22":2,"23":3,"30":1,"31":1,"54":1,"56":6,"57":6}}],["spawns",{"0":{"61":1}}],["space",{"0":{"56":1}}],["spacekey=ahrx",{"0":{"1":1}}],["spell",{"0":{"25":2}}],["spelufo",{"0":{"15":1}}],["spec",{"0":{"27":4,"45":3,"56":18,"61":1}}],["spec=k8s",{"0":{"21":1}}],["specifies",{"0":{"61":1}}],["specified",{"0":{"20":1,"22":1,"46":2}}],["specific",{"0":{"1":1,"7":1,"28":1,"40":1,"62":1}}],["specifying",{"0":{"22":1}}],["specify",{"0":{"2":1,"3":1,"59":1,"61":1}}],["special",{"0":{"2":2,"56":4,"57":1}}],["sudirectories",{"0":{"62":1}}],["sudo",{"0":{"4":1,"46":12,"56":5}}],["suspect",{"0":{"57":1}}],["suffix",{"0":{"33":1}}],["success=false",{"0":{"64":1}}],["successful",{"0":{"29":1}}],["succeed",{"0":{"61":1}}],["such",{"0":{"0":2,"2":2,"3":1,"12":1,"22":1,"28":1,"40":1,"49":2,"57":2,"61":1,"67":1}}],["sum",{"0":{"24":1,"43":7}}],["summary",{"0":{"0":1,"2":1,"9":1}}],["sure",{"0":{"5":1,"7":1,"15":1,"34":4,"47":2,"49":1,"52":1,"56":9,"57":6}}],["su",{"0":{"4":1,"56":3}}],["supbrocesses",{"0":{"61":1}}],["superset",{"0":{"3":2,"12":1,"18":1,"22":2,"28":3}}],["supported",{"0":{"56":1}}],["supports",{"0":{"2":1}}],["support",{"0":{"0":1,"4":2,"7":3,"12":3,"25":2,"51":2,"62":1,"66":1}}],["suggested",{"0":{"40":1}}],["suggest",{"0":{"2":1,"53":1}}],["subtle",{"0":{"61":1}}],["subprocesses",{"0":{"61":3}}],["subjectaltname",{"0":{"57":1}}],["subject=c=us",{"0":{"57":1}}],["subject",{"0":{"57":9}}],["subcription",{"0":{"45":3}}],["subdirectories",{"0":{"62":1}}],["subdirectory",{"0":{"15":1,"62":1}}],["subdomains",{"0":{"12":1}}],["subdomain",{"0":{"2":2}}],["subsection",{"0":{"57":1}}],["subsequent",{"0":{"57":1}}],["subscribing",{"0":{"22":1}}],["subscription",{"0":{"12":2,"20":1,"22":14,"32":1,"56":2}}],["subscriptions",{"0":{"12":1,"22":1,"45":3}}],["substitute",{"0":{"2":1}}],["submodule",{"0":{"3":4,"6":2,"10":1,"17":2,"39":2}}],["submodules",{"0":{"3":2,"10":1,"17":1,"62":1}}],["submit",{"0":{"2":2,"3":1,"4":1}}],["sub",{"0":{"2":1,"57":1}}],["ssassi",{"0":{"4":1}}],["ssh",{"0":{"4":6,"6":1,"26":2,"39":2,"44":1,"46":1,"56":3}}],["sso",{"0":{"2":2}}],["ssl",{"0":{"0":1,"2":2,"15":1,"35":1,"57":2}}],["ssd",{"0":{"0":1}}],["stuck",{"0":{"56":1}}],["structs",{"0":{"61":1}}],["struct",{"0":{"61":1}}],["structure",{"0":{"39":1,"42":1}}],["str",{"0":{"54":1}}],["strangely",{"0":{"49":1}}],["stream",{"0":{"54":1}}],["streams",{"0":{"54":1}}],["streamsets`",{"0":{"29":1}}],["streetsidesoftware",{"0":{"25":1}}],["strip",{"0":{"57":1}}],["stripe",{"0":{"22":29}}],["strings",{"0":{"30":1}}],["stringlike",{"0":{"2":1}}],["still",{"0":{"2":1,"61":1}}],["sts",{"0":{"2":2}}],["stephen",{"0":{"34":1}}],["steps",{"0":{"11":1,"15":1,"18":1,"45":1,"49":1,"56":5,"57":1}}],["step",{"0":{"2":2,"45":1,"46":3,"56":3,"57":5}}],["stockunpickler",{"0":{"63":1}}],["stop",{"0":{"2":1,"3":1,"23":1,"47":1,"56":1}}],["stored",{"0":{"15":1,"43":2,"61":1}}],["stores",{"0":{"5":1}}],["store",{"0":{"2":2,"24":1,"56":1}}],["storageclassname",{"0":{"45":2}}],["storage",{"0":{"0":3,"2":4,"9":4,"32":8,"33":4,"45":2,"53":1}}],["stable",{"0":{"21":1,"46":2}}],["stack",{"0":{"9":1,"54":2,"57":1}}],["starting",{"0":{"61":1}}],["started",{"0":{"23":1}}],["start",{"0":{"7":1,"22":2,"40":1,"44":2,"56":1,"57":1,"61":1}}],["star",{"0":{"4":1}}],["states",{"0":{"63":1}}],["state=state",{"0":{"63":1}}],["state",{"0":{"61":3,"63":2}}],["stateless",{"0":{"61":1}}],["statement",{"0":{"2":3,"9":1,"32":1,"47":1,"53":1,"56":1,"57":1}}],["stats",{"0":{"23":2}}],["status=",{"0":{"43":3}}],["status",{"0":{"2":1,"15":3,"25":1,"30":1,"34":1,"43":5,"57":2}}],["staff",{"0":{"2":4,"3":1}}],["standard",{"0":{"0":3,"2":1,"9":1,"20":1,"25":2}}],["f1",{"0":{"57":1}}],["f7",{"0":{"57":1}}],["f8",{"0":{"57":2}}],["fddcd2fc",{"0":{"45":1}}],["fact",{"0":{"61":1}}],["faq",{"0":{"61":1}}],["fa",{"0":{"57":2}}],["favorite",{"0":{"56":2}}],["fashion",{"0":{"56":1}}],["faster",{"0":{"2":1}}],["false",{"0":{"45":1}}],["fake",{"0":{"32":3}}],["failing",{"0":{"61":1}}],["failures",{"0":{"61":3}}],["failed",{"0":{"56":1}}],["fails",{"0":{"56":1}}],["fail",{"0":{"29":2,"61":1}}],["fcf8238f2a56",{"0":{"26":1}}],["fc",{"0":{"18":1}}],["flavio",{"0":{"26":1}}],["flag",{"0":{"15":1}}],["flower",{"0":{"23":3}}],["flexible",{"0":{"0":1,"18":1}}],["func",{"0":{"63":5}}],["function",{"0":{"61":1}}],["functionality",{"0":{"29":1,"67":1}}],["functions",{"0":{"4":1,"61":1}}],["future",{"0":{"62":1}}],["fully",{"0":{"61":1}}],["fullchain",{"0":{"35":2}}],["full",{"0":{"9":3,"45":2,"47":1,"57":1}}],["fulfilled",{"0":{"2":1}}],["fs",{"0":{"3":1,"16":1,"46":3}}],["f",{"0":{"3":1,"6":1,"14":5,"15":3,"16":1,"17":1,"22":1,"30":1,"39":1,"45":1,"57":1,"63":1,"67":2}}],["few",{"0":{"25":1,"56":2,"61":1}}],["features",{"0":{"28":3,"34":1,"47":3,"56":1}}],["feature",{"0":{"21":1,"22":1}}],["fetchall",{"0":{"63":4}}],["fetch",{"0":{"3":1,"6":1,"34":1,"56":1,"63":1}}],["federated",{"0":{"2":1}}],["federate",{"0":{"2":1}}],["federation",{"0":{"2":1}}],["fit",{"0":{"44":1}}],["fivetran",{"0":{"42":1}}],["fine",{"0":{"57":1}}],["finished",{"0":{"56":1}}],["final",{"0":{"40":1,"57":1}}],["finally",{"0":{"34":1,"40":1,"52":1,"56":1,"57":2,"66":1}}],["finalizers",{"0":{"61":1}}],["finalize",{"0":{"22":3}}],["find",{"0":{"2":1,"7":1,"18":1,"25":1,"32":1,"55":1,"57":2}}],["field",{"0":{"7":1,"22":5,"40":5,"44":1,"56":1}}],["fields",{"0":{"2":2,"22":1,"40":1,"68":1}}],["first",{"0":{"6":1,"15":1,"16":1,"22":1,"34":3,"39":1,"47":1,"56":3,"57":4,"61":1}}],["fixing",{"0":{"34":1}}],["fixes",{"0":{"28":1,"47":2}}],["fixtures",{"0":{"15":1}}],["fix",{"0":{"4":1,"34":3,"50":4}}],["fill",{"0":{"32":4}}],["filled",{"0":{"3":1,"16":1}}],["filtering",{"0":{"43":1}}],["filters",{"0":{"29":3,"56":1}}],["filter",{"0":{"23":1,"29":1,"32":2}}],["filtered",{"0":{"2":1}}],["fileutils",{"0":{"25":1}}],["file",{"0":{"2":2,"12":1,"15":1,"25":3,"45":1,"47":2,"53":1,"54":1,"56":6,"57":6,"59":2,"63":31,"64":1,"65":1,"67":1}}],["filesystem",{"0":{"2":1}}],["files",{"0":{"2":4,"3":2,"10":1,"12":1,"15":2,"16":1,"17":2,"25":1,"34":1,"39":1,"40":3,"46":2,"47":2,"57":25,"58":1}}],["four",{"0":{"56":1}}],["found",{"0":{"5":1,"57":1,"68":2}}],["folling",{"0":{"28":1}}],["followed",{"0":{"15":1,"56":1}}],["follows",{"0":{"3":1,"17":1,"22":1,"56":1}}],["follow",{"0":{"2":4,"3":1,"4":1,"22":3,"28":1,"39":1,"46":1,"57":1}}],["following",{"0":{"2":1,"4":2,"5":3,"6":1,"7":1,"14":1,"15":1,"20":1,"22":1,"29":3,"34":2,"45":1,"46":2,"47":1,"49":1,"52":1,"53":1,"56":3,"57":7,"62":1}}],["folder",{"0":{"4":2,"37":1,"47":1}}],["format",{"0":{"47":1,"57":6}}],["form",{"0":{"40":1,"48":1,"52":1}}],["forget",{"0":{"22":1,"40":1}}],["forceful",{"0":{"61":1}}],["force",{"0":{"12":1,"54":1}}],["forwarded",{"0":{"68":5}}],["forward",{"0":{"2":1,"27":1}}],["for",{"0":{"0":5,"1":1,"2":19,"3":4,"4":1,"5":6,"6":2,"7":7,"8":2,"9":9,"10":1,"12":1,"15":7,"17":4,"22":6,"23":1,"24":3,"25":2,"30":1,"33":2,"38":1,"39":2,"40":2,"41":1,"42":2,"44":1,"45":1,"46":3,"47":4,"49":2,"53":3,"56":16,"57":8,"58":1,"61":5,"62":6,"63":2,"66":1,"68":2}}],["framework",{"0":{"61":3}}],["frequently",{"0":{"61":1}}],["frequency",{"0":{"4":1}}],["free",{"0":{"0":1,"22":4}}],["from",{"0":{"2":10,"3":3,"5":1,"6":1,"7":1,"8":2,"11":1,"12":3,"15":6,"17":2,"18":1,"22":2,"23":2,"24":2,"25":1,"28":1,"29":1,"30":1,"34":5,"35":1,"39":1,"40":1,"41":2,"44":1,"45":2,"46":2,"47":2,"52":1,"56":9,"57":3,"61":2,"64":1,"67":1}}],["fron",{"0":{"1":1}}],["a0",{"0":{"57":1}}],["a9",{"0":{"57":1}}],["a9d047415b53",{"0":{"45":2}}],["a7",{"0":{"57":1}}],["ae",{"0":{"57":1}}],["aed",{"0":{"2":1,"3":1}}],["ah",{"0":{"57":1}}],["ahrx",{"0":{"1":1,"3":1,"4":1,"6":1,"15":1,"17":1,"39":4}}],["ag",{"0":{"46":1}}],["against",{"0":{"56":1}}],["again",{"0":{"46":1,"57":3,"66":2,"68":1}}],["agent",{"0":{"54":1,"56":2,"68":2}}],["age",{"0":{"15":2,"56":2}}],["a278",{"0":{"26":2}}],["aware",{"0":{"57":1}}],["awhile",{"0":{"49":1,"56":1}}],["awk",{"0":{"14":2,"38":1}}],["awswexnval0001",{"0":{"26":1}}],["awswcrnval001n",{"0":{"4":2}}],["awsaztirll000q",{"0":{"4":1}}],["aws",{"0":{"2":6,"4":1,"7":1,"9":2,"10":1,"14":7,"15":1,"17":2,"24":1,"26":11,"32":2,"33":3,"53":3,"56":1}}],["aio",{"0":{"14":1}}],["airfow",{"0":{"3":1}}],["airflow",{"0":{"2":10,"3":3,"9":1,"12":1,"16":4,"18":1,"21":4,"22":2,"28":3,"29":5,"30":1,"40":4,"42":3,"43":2,"56":5,"58":2,"63":48}}],["airbyte",{"0":{"2":3,"3":2,"12":1,"18":2,"22":2,"28":3,"42":2}}],["abb5",{"0":{"68":2}}],["able",{"0":{"57":1,"67":1}}],["absolute",{"0":{"34":1}}],["abstract",{"0":{"13":1}}],["about",{"0":{"22":1,"27":1,"56":2,"57":2}}],["above",{"0":{"7":1,"56":2,"57":6}}],["aks",{"0":{"12":2,"20":5,"45":1,"56":5}}],["affects",{"0":{"61":1}}],["affecting",{"0":{"59":1}}],["af14",{"0":{"12":2}}],["afterwards",{"0":{"57":1}}],["after",{"0":{"3":1,"4":1,"6":1,"10":1,"15":1,"16":1,"22":1,"32":5,"34":2,"46":1,"55":1,"57":7,"61":1,"66":1}}],["at",{"0":{"2":1,"5":1,"8":1,"13":1,"15":1,"22":1,"23":2,"40":1,"44":1,"51":1,"56":5,"61":2}}],["attempted",{"0":{"56":1}}],["attached",{"0":{"24":1}}],["attachments",{"0":{"24":2}}],["attachment",{"0":{"24":1}}],["attach",{"0":{"2":3}}],["attributes",{"0":{"2":1,"57":2}}],["am",{"0":{"56":1}}],["amd64",{"0":{"26":3}}],["amazon",{"0":{"14":4}}],["amazonaws",{"0":{"2":4,"25":13,"26":1}}],["amorer01",{"0":{"4":2}}],["amp",{"0":{"1":1,"2":1,"4":1,"7":2,"37":2,"44":1,"52":1,"56":6}}],["apr",{"0":{"57":5}}],["apache",{"0":{"21":2}}],["apt",{"0":{"13":1,"46":12}}],["apiserver",{"0":{"26":1}}],["apiversion",{"0":{"26":2,"27":1,"45":2}}],["api",{"0":{"2":3,"3":3,"9":2,"12":1,"14":2,"15":4,"16":3,"23":1,"35":1,"52":1,"53":3,"54":2,"56":13,"57":4,"59":1,"61":5}}],["appear",{"0":{"57":1}}],["appropriate",{"0":{"34":1,"57":1}}],["appropiate",{"0":{"15":1,"40":1}}],["app=core",{"0":{"15":1}}],["applewebkit",{"0":{"68":2}}],["apple",{"0":{"56":1}}],["applies",{"0":{"61":1}}],["applied",{"0":{"45":1,"56":1}}],["application",{"0":{"2":1,"4":5,"15":1,"54":1}}],["applying",{"0":{"61":1}}],["apply",{"0":{"14":5,"39":1,"45":1,"56":2,"61":2}}],["apps",{"0":{"6":1,"15":4,"17":1,"24":1,"39":1,"52":1}}],["app000300001207",{"0":{"4":1}}],["app",{"0":{"2":5,"4":5,"5":1,"7":13,"50":2,"52":4,"56":5}}],["appdevtools",{"0":{"2":1,"7":3,"42":1,"66":1}}],["average",{"0":{"23":1}}],["avoid",{"0":{"2":1,"61":1}}],["available",{"0":{"2":1,"47":1,"56":2,"61":1}}],["availability",{"0":{"0":1}}],["azt",{"0":{"2":1,"15":1}}],["az",{"0":{"2":2,"9":1,"12":5,"20":6,"45":5,"56":2}}],["azuread",{"0":{"52":4}}],["azurepubliccloud",{"0":{"26":1}}],["azure",{"0":{"0":1,"1":1,"12":6,"18":4,"20":1,"26":1,"32":3,"33":3,"45":2,"46":1,"52":3,"56":7}}],["auxwf",{"0":{"38":1}}],["aug",{"0":{"5":1,"57":1}}],["audit",{"0":{"50":5}}],["aud",{"0":{"2":1}}],["auth0",{"0":{"52":1}}],["auth",{"0":{"26":1}}],["authenticator",{"0":{"26":6}}],["authenticate",{"0":{"12":1,"68":3}}],["authentication",{"0":{"2":1,"9":1,"26":1,"44":1,"52":1,"66":1}}],["authority",{"0":{"26":1,"47":1,"68":1}}],["authoritative",{"0":{"3":1,"23":1}}],["authorization",{"0":{"2":1}}],["autogenerated",{"0":{"28":1}}],["autoscaler",{"0":{"20":1}}],["autoscaling",{"0":{"2":1,"9":1}}],["automatic",{"0":{"56":1}}],["automatically",{"0":{"5":1,"22":3,"45":1,"47":1,"49":1,"56":1}}],["automate",{"0":{"2":1,"42":1}}],["auto",{"0":{"0":1,"47":1}}],["aurora",{"0":{"2":2,"9":1}}],["adapter",{"0":{"67":1}}],["adapter=s3",{"0":{"53":1}}],["advantage",{"0":{"61":1}}],["adjustments",{"0":{"29":1,"52":1}}],["adjust",{"0":{"29":1}}],["adjusting",{"0":{"22":2}}],["aduser",{"0":{"26":2}}],["ad",{"0":{"7":1,"52":1}}],["adding",{"0":{"27":1,"40":1,"46":1,"56":1}}],["additional",{"0":{"2":2,"17":1,"34":1,"56":1}}],["adds",{"0":{"25":8}}],["addresses",{"0":{"27":2}}],["address",{"0":{"22":1,"27":1,"56":2,"61":1}}],["added",{"0":{"4":1,"25":1,"56":1}}],["add",{"0":{"2":1,"3":3,"15":2,"16":2,"20":6,"22":5,"27":2,"32":3,"33":1,"39":3,"40":2,"42":3,"46":4,"50":1,"52":4,"54":1,"56":7,"57":6,"67":4}}],["addons",{"0":{"2":1}}],["adw",{"0":{"2":2}}],["administrator",{"0":{"20":2}}],["administrate",{"0":{"20":1,"56":1}}],["admin",{"0":{"0":1,"2":2,"4":1,"5":1,"7":7,"14":3,"20":1,"22":8,"31":1,"40":2,"52":1}}],["around",{"0":{"44":1,"56":1}}],["args=args",{"0":{"63":1}}],["args",{"0":{"26":1,"30":3,"63":7}}],["argocd",{"0":{"2":1,"3":1}}],["architecture",{"0":{"2":1}}],["arn",{"0":{"2":5,"9":2,"26":1,"53":2}}],["articles",{"0":{"44":1}}],["artifacory",{"0":{"11":1}}],["artifacts",{"0":{"53":1}}],["artifactrepo",{"0":{"8":1,"11":1,"65":1,"66":2}}],["artifact",{"0":{"8":1}}],["artifactory",{"0":{"2":1,"7":2,"11":4,"66":2}}],["artemiseks",{"0":{"2":2}}],["artemis",{"0":{"1":2,"2":3,"25":1}}],["aren",{"0":{"56":2}}],["area",{"0":{"5":1,"22":1,"56":1}}],["are",{"0":{"2":2,"3":3,"4":1,"8":1,"10":1,"11":1,"17":4,"22":4,"25":1,"28":4,"29":2,"30":1,"33":1,"34":4,"39":1,"40":6,"44":1,"45":1,"47":6,"49":2,"56":16,"57":6,"61":7,"62":3,"65":1,"68":1}}],["acmesh",{"0":{"35":1}}],["acme",{"0":{"35":4}}],["across",{"0":{"8":1}}],["activate",{"0":{"56":1,"57":1}}],["activity",{"0":{"56":1}}],["active",{"0":{"2":1}}],["actions",{"0":{"32":2,"39":1}}],["action",{"0":{"1":1,"2":5,"9":2,"32":2,"45":2,"53":2}}],["acces",{"0":{"66":1}}],["accessible",{"0":{"56":1}}],["accessing",{"0":{"56":2}}],["accessmodes",{"0":{"45":2}}],["accessed",{"0":{"2":1}}],["access",{"0":{"1":9,"2":13,"3":1,"4":3,"5":2,"6":1,"7":10,"14":1,"15":2,"16":1,"33":1,"39":1,"41":1,"42":1,"48":1,"52":2,"53":4,"56":11,"68":1}}],["accidentally",{"0":{"34":1}}],["according",{"0":{"56":1}}],["accordingly",{"0":{"2":1,"4":1,"17":1,"18":1,"22":2,"40":1,"48":1,"52":1,"53":1,"58":1}}],["accounts",{"0":{"22":5,"42":2,"52":1}}],["account",{"0":{"2":11,"5":2,"8":6,"12":1,"14":1,"18":1,"20":2,"22":15,"23":1,"32":6,"33":2,"42":1,"56":2,"66":3}}],["announcement",{"0":{"47":1}}],["annotate",{"0":{"8":2}}],["annotations",{"0":{"15":1}}],["annotation",{"0":{"0":1}}],["answers",{"0":{"44":1}}],["another",{"0":{"22":1,"34":1,"45":1}}],["analytics",{"0":{"2":4,"4":1,"28":1,"40":1}}],["anything",{"0":{"34":1,"64":1}}],["anymore",{"0":{"12":1}}],["any",{"0":{"2":3,"22":1,"28":2,"34":2,"39":2,"46":1,"49":2,"56":8,"57":3,"61":3,"64":1}}],["ank",{"0":{"2":2,"6":1,"17":2}}],["an",{"0":{"2":16,"3":1,"6":1,"7":1,"10":1,"11":1,"13":1,"15":3,"17":1,"20":3,"22":4,"23":1,"24":2,"30":1,"34":1,"35":1,"41":1,"44":1,"49":1,"52":1,"53":3,"56":7,"57":5,"61":3,"62":2,"64":1,"67":1}}],["and",{"0":{"0":3,"1":2,"2":11,"3":4,"4":4,"5":5,"6":2,"7":6,"8":2,"10":1,"12":4,"14":3,"15":15,"16":4,"17":2,"18":3,"22":25,"23":1,"24":4,"25":5,"26":1,"27":2,"28":3,"29":5,"30":1,"31":1,"32":10,"34":6,"35":7,"37":1,"39":2,"40":12,"41":2,"42":1,"43":1,"44":6,"45":5,"46":7,"47":4,"49":5,"50":1,"52":10,"54":1,"56":42,"57":25,"58":3,"59":5,"61":24,"62":2,"63":1,"64":1,"66":2,"67":2,"68":2}}],["alright",{"0":{"57":1}}],["already",{"0":{"3":1,"8":1,"15":2,"20":1,"29":1,"34":1,"41":1,"45":1,"56":2}}],["algorithm",{"0":{"57":4}}],["alarms",{"0":{"56":1}}],["alert",{"0":{"54":5}}],["alerts",{"0":{"54":2}}],["alphabet",{"0":{"24":1}}],["alpha2",{"0":{"14":2}}],["altered",{"0":{"56":1}}],["alternatively",{"0":{"3":1}}],["alter",{"0":{"2":1,"18":1,"56":1}}],["along",{"0":{"2":1}}],["allrows",{"0":{"63":2}}],["allows",{"0":{"56":1}}],["allow",{"0":{"2":5,"9":2,"22":1,"32":2,"44":1,"53":2,"56":2,"61":1,"68":1}}],["allowed",{"0":{"0":1,"56":3}}],["allocated",{"0":{"2":1,"9":1}}],["all",{"0":{"1":2,"3":1,"7":2,"8":2,"11":1,"15":1,"16":1,"22":2,"28":1,"31":1,"32":3,"34":2,"40":2,"41":2,"46":1,"47":4,"49":1,"56":8,"57":1,"58":1,"61":3,"63":6,"64":1}}],["also",{"0":{"0":1,"2":1,"7":1,"15":1,"18":1,"22":2,"29":1,"35":1,"52":1,"56":3,"57":2,"59":1,"61":1}}],["aspect",{"0":{"49":1}}],["asc",{"0":{"41":3,"46":1}}],["asdf",{"0":{"4":2}}],["assuming",{"0":{"57":1}}],["assume",{"0":{"15":2}}],["assumerolewithwebidentity",{"0":{"2":1}}],["assemble",{"0":{"57":1}}],["assist",{"0":{"56":1}}],["associated",{"0":{"40":1}}],["associate",{"0":{"2":1}}],["asked",{"0":{"57":1}}],["ask",{"0":{"2":1,"3":1,"15":1,"20":2,"34":3,"47":1,"56":1}}],["asx",{"0":{"1":1,"3":1,"4":1,"6":1,"7":2,"15":1,"17":1,"39":4}}],["as",{"0":{"0":1,"1":1,"2":5,"3":6,"5":1,"6":1,"7":3,"10":2,"11":1,"12":2,"15":4,"17":3,"22":8,"28":1,"30":1,"33":1,"34":2,"35":2,"39":1,"40":1,"42":1,"44":1,"46":1,"47":3,"49":1,"52":1,"56":6,"57":9,"61":5,"67":1}}],["a",{"0":{"0":3,"1":1,"2":25,"3":4,"4":3,"5":3,"6":4,"7":1,"9":1,"10":2,"11":1,"12":1,"13":1,"14":2,"15":13,"16":1,"17":1,"20":11,"21":2,"22":21,"23":2,"24":3,"25":5,"26":1,"27":2,"28":2,"29":1,"30":2,"31":1,"32":2,"33":1,"34":16,"39":10,"40":6,"41":4,"42":1,"44":2,"45":3,"46":3,"47":2,"48":1,"52":4,"53":3,"55":2,"56":54,"57":15,"58":1,"59":4,"61":31,"62":1,"63":1,"64":1,"65":2,"66":1,"67":2,"68":2}}],["ok",{"0":{"57":1,"68":1}}],["okay",{"0":{"57":1}}],["o=digicert",{"0":{"57":9}}],["o=jsonpath=",{"0":{"2":1}}],["oomkilled",{"0":{"43":1}}],["ou=www",{"0":{"57":9}}],["ourselves",{"0":{"56":1}}],["our",{"0":{"24":1,"25":2,"28":2,"32":2,"33":1,"47":1,"51":1,"56":2,"57":2,"61":3,"67":1}}],["outside",{"0":{"65":1}}],["output",{"0":{"23":1,"39":1,"41":1,"56":2,"57":2}}],["outlined",{"0":{"15":1}}],["outdated",{"0":{"2":1,"3":1}}],["out",{"0":{"2":3,"3":2,"47":3,"49":1,"56":11,"57":10,"61":2,"66":1}}],["occurs",{"0":{"56":2}}],["occur",{"0":{"22":1}}],["oci",{"0":{"11":4}}],["obj",{"0":{"63":1}}],["objects",{"0":{"18":1,"32":5}}],["object",{"0":{"15":2,"32":2}}],["obsolete",{"0":{"25":1}}],["observability",{"0":{"9":1,"54":1}}],["observavility",{"0":{"9":1}}],["over",{"0":{"61":1}}],["overview",{"0":{"52":1,"56":1,"61":1}}],["override",{"0":{"21":5}}],["overwrite",{"0":{"15":1}}],["overall",{"0":{"2":1}}],["o",{"0":{"8":2,"13":1,"15":1,"45":1}}],["owned",{"0":{"18":1}}],["ownerreferences",{"0":{"61":1}}],["owner",{"0":{"2":1,"18":4,"22":1,"46":1}}],["own",{"0":{"8":1,"44":1,"61":2}}],["oidc",{"0":{"2":4}}],["opusfwngvhomii2aiyeptvnqdslqv59muxpui8r6aw",{"0":{"57":1}}],["op",{"0":{"27":2}}],["operations",{"0":{"61":1}}],["operation",{"0":{"61":2}}],["operator",{"0":{"15":10,"61":10,"67":1}}],["openssl",{"0":{"35":1,"57":22}}],["openssh",{"0":{"4":1}}],["open",{"0":{"4":1,"14":2,"56":1}}],["openid",{"0":{"2":2,"52":1}}],["option",{"0":{"54":2,"57":1,"67":2}}],["optional",{"0":{"0":2,"9":1,"12":1,"14":1,"35":1,"42":2}}],["opt",{"0":{"2":3,"3":1,"22":1}}],["oauth2",{"0":{"52":1}}],["oauth",{"0":{"2":2,"52":1}}],["old",{"0":{"1":1,"8":1,"20":1,"45":4,"47":1,"56":2}}],["other",{"0":{"0":1,"2":2,"5":1,"7":1,"9":1,"10":1,"15":1,"17":2,"32":3,"34":2,"56":3,"57":1,"59":1,"61":1}}],["os",{"0":{"0":1,"68":2}}],["orm",{"0":{"63":2}}],["origien",{"0":{"45":1}}],["origin",{"0":{"45":3}}],["original",{"0":{"34":1,"57":1}}],["orchestrate",{"0":{"42":1,"56":1}}],["orchestration",{"0":{"4":1,"42":1}}],["organized",{"0":{"62":1}}],["organization",{"0":{"56":1}}],["organizational",{"0":{"52":1}}],["org",{"0":{"21":1}}],["ordering",{"0":{"57":2}}],["order",{"0":{"7":3,"8":1,"22":1,"56":1,"57":3}}],["orrumcorp",{"0":{"12":3}}],["orrum",{"0":{"1":1,"12":18,"25":2,"27":5,"56":11,"57":20,"68":2}}],["or",{"0":{"0":3,"2":5,"3":1,"4":1,"6":1,"8":1,"9":1,"11":2,"15":3,"22":6,"23":2,"24":1,"28":3,"29":2,"34":6,"40":3,"44":1,"47":1,"56":3,"57":2,"58":1,"61":5,"63":1,"65":1,"67":1,"68":1}}],["often",{"0":{"56":1,"61":1}}],["offline",{"0":{"49":1}}],["official",{"0":{"35":1}}],["off",{"0":{"23":1,"49":1,"51":2}}],["of",{"0":{"0":1,"2":8,"3":2,"4":3,"5":1,"9":1,"15":5,"18":1,"22":3,"23":1,"24":7,"25":2,"27":1,"28":1,"29":2,"30":1,"32":5,"34":2,"39":2,"40":1,"41":1,"47":6,"49":2,"52":1,"56":20,"57":11,"58":1,"61":16,"63":1,"64":1,"65":2,"68":1}}],["online",{"0":{"56":1}}],["only",{"0":{"2":1,"9":1,"22":2,"34":2,"39":4,"44":2,"45":1,"52":2,"56":1,"57":2,"61":1,"64":2}}],["onto",{"0":{"34":1}}],["onmicrosoft",{"0":{"12":3}}],["once",{"0":{"2":1,"6":2,"17":1,"22":4,"34":2,"40":1,"42":1,"52":1,"56":3,"57":2,"59":1,"61":1}}],["ones",{"0":{"2":1,"22":1,"40":1}}],["one",{"0":{"2":6,"3":1,"4":1,"7":2,"8":1,"9":2,"22":3,"23":2,"34":2,"40":1,"45":1,"53":1,"56":6,"57":4,"62":1}}],["onboarding",{"0":{"1":3}}],["on",{"0":{"0":1,"1":1,"2":14,"3":3,"4":6,"5":1,"6":1,"7":7,"8":1,"9":1,"10":2,"11":2,"12":5,"13":1,"14":3,"15":3,"16":1,"17":2,"18":5,"19":1,"20":1,"21":2,"22":22,"23":2,"24":1,"25":3,"26":1,"27":1,"28":2,"29":2,"30":3,"31":1,"32":6,"33":1,"34":2,"35":1,"36":1,"37":2,"38":2,"39":1,"40":6,"41":1,"42":12,"43":2,"44":1,"45":1,"46":4,"47":1,"48":2,"49":2,"50":1,"51":3,"52":8,"53":2,"54":3,"55":3,"56":13,"57":1,"58":3,"59":3,"60":1,"61":1,"62":1,"63":2,"64":2,"65":1,"66":2,"67":1,"68":4,"69":1}}]],"serializationVersion":2}